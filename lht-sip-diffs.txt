Only in lightning-hydra-template-extended: .envrc
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/.vscode/launch.json synthmatch-image-proc/.vscode/launch.json
*** lightning-hydra-template-extended/.vscode/launch.json	Fri Aug 15 14:00:35 2025
--- synthmatch-image-proc/.vscode/launch.json	Tue Aug 12 16:46:31 2025
***************
*** 2,19 ****
      "version": "0.2.0",
      "configurations": [
          {
!             "name": "avix-cnn",
              "type": "python",
              "request": "launch",
              "program": "${workspaceFolder}/src/train.py",
-             "args": [
-                 "experiment=avix_cnn_16kdss",
-                 "trainer=mps"
-             ],
              "console": "integratedTerminal",
              "cwd": "${workspaceFolder}",
              "env": {},
!             "justMyCode": false
          }
      ]
  }--- 2,89 ----
      "version": "0.2.0",
      "configurations": [
          {
!             "name": "Gen",
              "type": "python",
              "request": "launch",
+             "program": "${workspaceFolder}/generate_vimh.py",
+             "console": "integratedTerminal",
+             "justMyCode": false,
+             "env": {},
+             "python": "${workspaceFolder}/.venv/bin/python",
+             "cwd": "${workspaceFolder}",
+             "args": []
+         },
+         {
+             "name": "Disp",
+             "type": "python",
+             "request": "launch",
+             "program": "${workspaceFolder}/display_vimh.py",
+             "console": "integratedTerminal",
+             "justMyCode": false,
+             "env": {},
+             "python": "${workspaceFolder}/.venv/bin/python",
+             "cwd": "${workspaceFolder}",
+             "args": []
+         },
+         {
+             "name": "Aura",
+             "type": "python",
+             "request": "launch",
              "program": "${workspaceFolder}/src/train.py",
              "console": "integratedTerminal",
+             "justMyCode": false,
+             "env": {},
+             "python": "${workspaceFolder}/.venv/bin/python",
              "cwd": "${workspaceFolder}",
+             "args": ["experiment=vimh_cnn_auraloss"]
+         },
+         {
+             "name": "AuraO",
+             "type": "python",
+             "request": "launch",
+             "program": "${workspaceFolder}/src/train.py",
+             "console": "integratedTerminal",
+             "justMyCode": false,
              "env": {},
!             "python": "${workspaceFolder}/.venv/bin/python",
!             "cwd": "${workspaceFolder}",
!             "args": ["experiment=vimh_cnn_auraloss_ordinal"]
!         },
!         {
!             "name": "AuraR",
!             "type": "python",
!             "request": "launch",
!             "program": "${workspaceFolder}/src/train.py",
!             "console": "integratedTerminal",
!             "justMyCode": false,
!             "env": {},
!             "python": "${workspaceFolder}/.venv/bin/python",
!             "cwd": "${workspaceFolder}",
!             "args": ["experiment=vimh_cnn_auraloss_regression"]
!         },
!         {
!             "name": "SynthMSS",
!             "type": "python",
!             "request": "launch",
!             "program": "${workspaceFolder}/src/train.py",
!             "console": "integratedTerminal",
!             "justMyCode": false,
!             "env": {},
!             "python": "${workspaceFolder}/.venv/bin/python",
!             "cwd": "${workspaceFolder}",
!             "args": ["experiment=vimh_cnn_synthesis_multiscale"]
!         },
!         {
!             "name": "SynthPNP",
!             "type": "python",
!             "request": "launch",
!             "program": "${workspaceFolder}/src/train.py",
!             "console": "integratedTerminal",
!             "justMyCode": false,
!             "env": {},
!             "python": "${workspaceFolder}/.venv/bin/python",
!             "cwd": "${workspaceFolder}",
!             "args": ["experiment=vimh_cnn_perceptual_spectral"]
          }
      ]
  } Only in synthmatch-image-proc/.vscode: tasks.json
Only in synthmatch-image-proc: AURA_LOSS_PLAN.md
Only in lightning-hydra-template-extended: AVIX_CHERRY_PICK_PLAN.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/CLAUDE.md synthmatch-image-proc/CLAUDE.md
*** lightning-hydra-template-extended/CLAUDE.md	Thu Aug 14 16:56:15 2025
--- synthmatch-image-proc/CLAUDE.md	Tue Aug 26 14:29:30 2025
***************
*** 2,11 ****

  This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

! ## Repository Overview

! This is an extended Lightning-Hydra-Template for deep learning projects using PyTorch Lightning and Hydra configuration management. The template provides a clean, organized structure for rapid ML experimentation with minimal boilerplate.  In this template project we've extended the original template as described in ./docs/extensions.md

  ## Core Technologies

  - **PyTorch Lightning**: High-level PyTorch wrapper for organizing model training
--- 2,19 ----

  This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

! ## Repository Overview: SynthMatching via Image Processing

! This is a fork of the Lightning-Hydra-Template-Extended template
! (/l/lht) for deep learning projects using PyTorch Lightning and Hydra
! configuration management. The template provides a number models and
! experiments for MNIST and CIFAR-10/100 image classification.

+ In this project, we want to do image classification on *audio
+ spectrograms*.  Thus, the main new features of this project are novel
+ mappings from audio to spectrogram images and the evaluation of their
+ performance using the various architectures available in src/models/components/.
+
  ## Core Technologies

  - **PyTorch Lightning**: High-level PyTorch wrapper for organizing model training
***************
*** 61,68 ****
  python src/eval.py ckpt_path="/path/to/checkpoint.ckpt"
  ```

! ### Environment Management
  ```bash
  source .venv/bin/activate.csh

  # Alternative activation shortcuts (see Makefile)
--- 69,84 ----
  python src/eval.py ckpt_path="/path/to/checkpoint.ckpt"
  ```

! ### UV Environment Setup
  ```bash
+ # Create environment (Python 3.9 required for auraloss compatibility)
+ uv venv --python 3.9
+
+ # Activate environment
+ source .venv/bin/activate
+
+ # Install dependencies
+ uv pip install -r requirements.txt
  source .venv/bin/activate.csh

  # Alternative activation shortcuts (see Makefile)
***************
*** 159,165 ****
    - `trainer/`: Lightning trainer configurations (CPU, GPU, MPS, DDP)
    - `callbacks/`: Training callbacks
    - `logger/`: Logging configurations
!   - `experiment/`: Complete experiment configurations (50+ experiments)
  - **Config composition**: Uses Hydra's `defaults` list to compose configurations
  - **Override system**: Parameters can be overridden via command line (e.g., `python src/train.py trainer.max_epochs=20`)
  - **Auto-configuration**: VIMH models auto-configure from dataset metadata
--- 175,181 ----
    - `trainer/`: Lightning trainer configurations (CPU, GPU, MPS, DDP)
    - `callbacks/`: Training callbacks
    - `logger/`: Logging configurations
!   - `experiment/`: Complete experiment configurations
  - **Config composition**: Uses Hydra's `defaults` list to compose configurations
  - **Override system**: Parameters can be overridden via command line (e.g., `python src/train.py trainer.max_epochs=20`)
  - **Auto-configuration**: VIMH models auto-configure from dataset metadata
Only in synthmatch-image-proc: CLAUDE.md.orig
Only in synthmatch-image-proc: CONDITIONING_SUPPORT_PLAN.md
Only in synthmatch-image-proc: ClaudeNotes.txt
Only in synthmatch-image-proc: CursorMerge.txt
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/Makefile synthmatch-image-proc/Makefile
*** lightning-hydra-template-extended/Makefile	Tue Aug 26 15:23:02 2025
--- synthmatch-image-proc/Makefile	Tue Aug 26 14:29:30 2025
***************
*** 9,15 ****
--- 9,19 ----
  	find . | grep -E ".ipynb_checkpoints" | xargs rm -rf
  	rm -f .coverage
  	rm -rf ./viz/diagrams
+ 	rm -rf ./outputs/

+ dc dclean: ## Clean data files
+ 	rm -rf data-vimh/*
+
  cl clean-logs: ## Clean logs
  	rm -rf logs/**

***************
*** 20,30 ****
  	git pull
  	git pull origin main

- tb tensorboard: ## Launch TensorBoard on port 6006
- 	@lsof -i :6006 >/dev/null 2>&1 && echo "TensorBoard already running on port 6006" || \
- 		(echo "Starting TensorBoard on port 6006..." && tensorboard --logdir logs/train/runs/ --port 6006 &)
- 	@echo "Open http://localhost:6006/"
-
  a activate: ## Activate the uv environment
  	@echo "Add to ~/.tcshrc: alias a 'echo \"source .venv/bin/activate.csh\" && source .venv/bin/activate.csh'"
  	@echo "Then just type: a"
--- 24,29 ----
***************
*** 42,51 ****
  trc trcnn train-cnn: ## Train with CNN architecture
  	time python src/train.py model=mnist_cnn_small

- tg train-with-gradients: ## Train with gradient statistics tracking enabled
- 	python src/train.py callbacks=grouped_progress_bar_with_gradients logger=tensorboard
- 	@echo "Check gradient stats at http://localhost:6006/#scalars&tagFilter=grad_stats"
-
  trvs train-vit-small: ## Train small ViT (~38K params)
  	time python src/train.py model=mnist_vit_38k

--- 41,46 ----
***************
*** 162,168 ****
  	time python src/train.py experiment=vit_mnist


! ev995 exp-vit-995: ## Run ViT experiment achieving SOTA 99.5% validation accuracy
  	time python src/train.py experiment=mnist_vit_995
  	# == python src/train.py model=mnist_vit_995 data=mnist_vit_995 trainer.max_epochs=200 trainer.min_epochs=10 trainer.gradient_clip_val=1.0 data.batch_size=128 seed=12345 tags="[mnist,vit,995,optimized]"

--- 157,163 ----
  	time python src/train.py experiment=vit_mnist


! evit995 exp-vit-995: ## Run ViT experiment achieving SOTA 99.5% validation accuracy
  	time python src/train.py experiment=mnist_vit_995
  	# == python src/train.py model=mnist_vit_995 data=mnist_vit_995 trainer.max_epochs=200 trainer.min_epochs=10 trainer.gradient_clip_val=1.0 data.batch_size=128 seed=12345 tags="[mnist,vit,995,optimized]"

***************
*** 175,191 ****
  emhcc10 exp-multihead-cnn-cifar10: ## Run MultiHead CNN CIFAR-10 classification experiment
  	time python src/train.py experiment=multihead_cnn_cifar10

! evimh exp-vimh-16kdss: ## Run VIMH CNN training with 16K dataset samples (SimpleSynth)
  	time python src/train.py experiment=vimh_cnn_16kdss # ./configs/experiment/vimh_cnn_16kdss.yaml
! evimho exp-vimh-16kdss-ordinal: ## Run VIMH CNN training with ordinal regression loss (distance-aware)
! 	time python src/train.py experiment=vimh_cnn_16kdss_ordinal # ./configs/experiment/vimh_cnn_16kdss.yaml
! evimhr exp-vimh-16kdss-regression: ## Run VIMH CNN training with pure regression heads (sigmoid + parameter mapping)
  	time python src/train.py experiment=vimh_cnn_16kdss_regression # ./configs/experiment/vimh_cnn_16kdss_regression.yaml

! excn exp-convnext: ## Run ConvNeXt-V2 experiment
  	time python src/train.py experiment=convnext_mnist

! ecnb exp-convnext-benchmark: ## Run official ConvNeXt V2-Tiny benchmark (acid test)
  	time python src/train.py experiment=convnext_v2_official_tiny_benchmark

  # CIFAR BENCHMARKS "cb" - Computer Vision Dataset Experiments
--- 170,206 ----
  emhcc10 exp-multihead-cnn-cifar10: ## Run MultiHead CNN CIFAR-10 classification experiment
  	time python src/train.py experiment=multihead_cnn_cifar10

! # VIMH SIMP EXPERIMENTS:
!
! ev exp-vimh-16kdss: ## Run VIMH CNN training with 16K dataset samples (SimpleSynth)
  	time python src/train.py experiment=vimh_cnn_16kdss # ./configs/experiment/vimh_cnn_16kdss.yaml
! evo exp-vimh-16kdss-ordinal: ## Run VIMH CNN training with ordinal regression loss (distance-aware)
! 	time python src/train.py experiment=vimh_cnn_16kdss_ordinal # ./configs/experiment/vimh_cnn_16kdss_ordinal.yaml
! evr exp-vimh-16kdss-regression: ## Run VIMH CNN training with pure regression heads (sigmoid + parameter mapping)
  	time python src/train.py experiment=vimh_cnn_16kdss_regression # ./configs/experiment/vimh_cnn_16kdss_regression.yaml

! evc exp-vimh-convnext: ## Train ConvNeXt CNN on VIMH dataset - ./configs/experiment/vimh_cnn_64k.yaml
! 	time python src/train.py experiment=vimh_cnn_64k
!
! eva exp-vimh-auraloss: ## Train CNN on VIMH dataset using auraloss
! 	time python src/train.py experiment=vimh_cnn_auraloss # ./configs/experiment/vimh_cnn_auraloss.yaml
!
! evao exp-vimh-auraloss-ordinal: ## Train CNN on VIMH dataset using auraloss with ordinal accuracy
! 	time python src/train.py experiment=vimh_cnn_auraloss_ordinal # ./configs/experiment/vimh_cnn_auraloss_ordinal.yaml
!
! evar exp-vimh-auraloss-regression: ## Train CNN on VIMH dataset using auraloss with regression accuracy
! 	time python src/train.py experiment=vimh_cnn_auraloss_regression # ./configs/experiment/vimh_cnn_auraloss_regression.yaml
!
! evaall: eva evao evar ## Run all VIMH dataset auraloss training examples (eva evao evar)
!
! evall: ev evo evr evc evaall ## Run all VIMH dataset training examples (ev evo evr evc evaall)
!
! # MNIST CONVNEXT EXPERIMENTS:
!
! excn exp-convnext: ## Run baseline ConvNeXt-V2 experiment - ./configs/experiment/convnext_mnist.yaml
  	time python src/train.py experiment=convnext_mnist

! ecnb exp-convnext-benchmark: ## Run official ConvNeXt V2-Tiny benchmark (acid test) - ./configs/experiment/convnext_v2_official_tiny_benchmark.yaml
  	time python src/train.py experiment=convnext_v2_official_tiny_benchmark

  # CIFAR BENCHMARKS "cb" - Computer Vision Dataset Experiments
***************
*** 270,276 ****
  cbq100cc cifar100-quick-coarse: ## Quick CIFAR-100 coarse validation (5 epochs)
  	python src/train.py experiment=cifar100_coarse_cnn trainer.max_epochs=5 trainer.min_epochs=1

! cbqa cifar-quick-all: cbq10c cbq10cn cbq10cn64 cbq10cn128 cbq100sdn cbq100cnn1m cbq100cn10m cbq100c cbq100cc  ## Run all quick CIFAR validations

  # CIFAR BENCHMARK SUITES "cbs" - Systematic Comparisons

--- 285,291 ----
  cbq100cc cifar100-quick-coarse: ## Quick CIFAR-100 coarse validation (5 epochs)
  	python src/train.py experiment=cifar100_coarse_cnn trainer.max_epochs=5 trainer.min_epochs=1

! cbqa cifar-quick-all: cbq10c cbq10cn cbq100c cbq100cc ## Run all quick CIFAR validations

  # CIFAR BENCHMARK SUITES "cbs" - Systematic Comparisons

***************
*** 286,299 ****
  cbsa benchmark-all: cbs10 cbs100 ## Run complete CIFAR benchmark suite
  	@echo "=== Complete CIFAR benchmark suite finished ==="

! allqt all-quick-tests: tqa cbqa ## All quick tests

! # UTILITY TARGETS

! lc list-configs: ## List available model configurations
! 	@echo "Available model configs:"
! 	@find configs/model -name "*.yaml" | sed 's|configs/model/||' | sed 's|\.yaml||' | sort
! 	@echo "\nAvailable data configs:"
! 	@find configs/data -name "*.yaml" | sed 's|configs/data/||' | sed 's|\.yaml||' | sort
! 	@echo "\nAvailable experiment configs:"
! 	@find configs/experiment -name "*.yaml" | sed 's|configs/experiment/||' | sed 's|\.yaml||' | sort
--- 301,323 ----
  cbsa benchmark-all: cbs10 cbs100 ## Run complete CIFAR benchmark suite
  	@echo "=== Complete CIFAR benchmark suite finished ==="

! cbstk cifar-benchmark-stk:
! 	time python src/train.py experiment=cifar100_stk_cnn_1m

! cbstkel cifar-benchmark-stk-efficient-leaf:
! 	time python src/train.py experiment=cifar100_el_stk_cnn_1m

! cbres cifar-benchmark-resonarium:
! 	time python src/train.py experiment=cifar100_res_cnn_1m
!
! cbresel cifar-benchmark-resonarium-efficient-leaf:
! 	time python src/train.py experiment=cifar100_el_res_cnn_1m
! 	# == python src/train.py data=audio_spectrogram_base data.prefix=cifar100-el data.synth_engine=resonarium data.dataset_size=8
!
! cballel: cbstkel cbresel
!
! ss simp-small: ## Generate SimpleSynth dataset
! 	source .venv/bin/activate && time python generate_vimh.py --pickle ++dataset.size=256
!
! sl simp-large: ## Generate large SimpleSynth dataset
! 	source .venv/bin/activate && time python generate_vimh.py --pickle ++dataset.size=16384
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/README-JOS.md synthmatch-image-proc/README-JOS.md
*** lightning-hydra-template-extended/README-JOS.md	Fri Jul  4 21:04:45 2025
--- synthmatch-image-proc/README-JOS.md	Tue Aug 26 15:53:34 2025
***************
*** 1,5 ****
--- 1,11 ----
  # JOS Notes to Self

+ ## MAJOR DIFFERENCES FROM LHT and AVIX
+
+ * _Different VIMH Format_
+   - Each spectrum have a min and max field (float32) allowing it to be normalized
+   - WE WILL USE CONDITIONING INPUTS INSTEAD (/l/lht, /l/av)
+
  ## Bootstrap

  ```tcsh
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/README.md synthmatch-image-proc/README.md
*** lightning-hydra-template-extended/README.md	Tue Jul 15 03:12:31 2025
--- synthmatch-image-proc/README.md	Tue Aug 26 14:29:30 2025
***************
*** 1,5 ****
! # Lightning-Hydra-Template-*Extended*

  ## Extensions

  This [Lightning-Hydra-Template-Extended](https://github.com/josmithiii/lightning-hydra-template-extended.git)
--- 1,13 ----
! # SynthMatching via Image Processing

+ ## 🚀 Quick Start
+
+ See [CLAUDE.md](CLAUDE.md)
+
+ ---
+
+ # BEGIN README.md for Lightning-Hydra-Template-Extended
+
  ## Extensions

  This [Lightning-Hydra-Template-Extended](https://github.com/josmithiii/lightning-hydra-template-extended.git)
***************
*** 29,41 ****

  ```bash
  # Train with VIMH dataset
! python src/train.py experiment=vimh_cnn

  # Run complete training example
  python examples/vimh_training.py

  # Quick demo with visualizations
  python examples/vimh_training.py --demo --save-plots
  ```

  ### 📊 Dataset Format
--- 37,53 ----

  ```bash
  # Train with VIMH dataset
! python src/train.py experiment=vimh_cnn_64k

  # Run complete training example
  python examples/vimh_training.py

  # Quick demo with visualizations
  python examples/vimh_training.py --demo --save-plots
+
+ # Evaluate checkpoint on testset
+ python src/train.py --config-name=eval --data=mnist ckpt_path=path/to/your/checkpoint.ckpt
+ python src/train.py --config-name=eval --data=cifar10 ckpt_path=path/to/your/checkpoint.ckpt
  ```

  ### 📊 Dataset Format
***************
*** 56,62 ****
  num_workers: 4

  # Model auto-configures from dataset
! # configs/experiment/vimh_cnn.yaml
  defaults:
    - override /data: vimh
    - override /model: vimh_cnn_64k
--- 68,74 ----
  num_workers: 4

  # Model auto-configures from dataset
! # configs/experiment/vimh_cnn_64k.yaml
  defaults:
    - override /data: vimh
    - override /model: vimh_cnn_64k
Only in synthmatch-image-proc: SPECTRAL_GRID_GRADIENTS.html
Only in synthmatch-image-proc/archive: CLAUDE.md.orig
Only in lightning-hydra-template-extended/archive: CONVNEXT_10M_PLAN.md
Only in lightning-hydra-template-extended/archive: CONVNEXT_PLAN.md
Only in lightning-hydra-template-extended/archive: CONVNEXT_PLAN_0.md
Only in lightning-hydra-template-extended/archive: MULTIHEAD_PLAN.md
Only in synthmatch-image-proc/archive: Makefile.orig
Only in lightning-hydra-template-extended/archive: README-EXTENSIONS-ORIGINAL.md
Only in lightning-hydra-template-extended/archive: README-PLAN-ORIGINAL.md
Only in synthmatch-image-proc/archive: README_JTFS.md
Only in lightning-hydra-template-extended/archive: REGRESSION_HEAD_PLAN.md
Only in lightning-hydra-template-extended/archive: REGRESSION_HEAD_PLAN.pdf
Only in lightning-hydra-template-extended/archive: SIZE_RENAME_PLAN.md
Only in synthmatch-image-proc/archive: SPECTROGRAM_3x32x32_PLAN.md
Only in lightning-hydra-template-extended/archive: TRAINING_EXAMPLE_PLAN.md
Only in lightning-hydra-template-extended/archive: VIT_PLAN.md
Only in lightning-hydra-template-extended/archive: VIT_PLAN_0.md
Only in synthmatch-image-proc/archive: auraloss-integration-diffs.txt
Only in lightning-hydra-template-extended/archive: cbq-log.md
Only in lightning-hydra-template-extended/archive: cbq.sh
Only in lightning-hydra-template-extended/archive: configs
Only in lightning-hydra-template-extended/archive: docs-TEMP-BACKUP
Only in synthmatch-image-proc/archive: extensions.md.orig
Only in synthmatch-image-proc/archive: generate_dataset.py
Only in synthmatch-image-proc/archive: generate_dataset_helpers.py
Only in synthmatch-image-proc/archive: generate_simple_torchaudio.yaml
Only in synthmatch-image-proc/archive: generate_vimh_out.py
Only in lightning-hydra-template-extended/archive: lht0-diffs.txt
Only in lightning-hydra-template-extended/archive: multihead_data_details.md
Only in synthmatch-image-proc/archive: normalization-diff.txt
Only in lightning-hydra-template-extended/archive: test_loss_comparison.py
Only in synthmatch-image-proc/archive: test_multiscale_loss.py
Only in synthmatch-image-proc/archive: test_multiscale_synthesis.py
Only in lightning-hydra-template-extended/archive: test_param_configs.py
Only in lightning-hydra-template-extended/archive: trnm-log-2025-06-30.txt
Only in lightning-hydra-template-extended/archive: tve-log-2025-07-02.txt
Only in synthmatch-image-proc/archive: update_waterfall_display_GEMINI.py
Only in lightning-hydra-template-extended/archive: verify_convnext_10m.py
Only in lightning-hydra-template-extended/archive: vimh_lightning.md
Only in synthmatch-image-proc: compare_three.sh
Only in lightning-hydra-template-extended/configs/callbacks: gradient_stats.yaml
Only in lightning-hydra-template-extended/configs/callbacks: grouped_progress_bar.yaml
Only in lightning-hydra-template-extended/configs/callbacks: grouped_progress_bar_with_gradients.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/callbacks/rich_progress_bar.yaml synthmatch-image-proc/configs/callbacks/rich_progress_bar.yaml
*** lightning-hydra-template-extended/configs/callbacks/rich_progress_bar.yaml	Tue Aug 26 15:18:52 2025
--- synthmatch-image-proc/configs/callbacks/rich_progress_bar.yaml	Wed Jul  9 23:44:06 2025
***************
*** 1,5 ****
! # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.ProgressBar.html
! # Changed from RichProgressBar to default ProgressBar to fix Rich console issues

  rich_progress_bar:
!   _target_: lightning.pytorch.callbacks.ProgressBar
--- 1,4 ----
! # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.RichProgressBar.html

  rich_progress_bar:
!   _target_: lightning.pytorch.callbacks.RichProgressBar
Only in synthmatch-image-proc/configs/data: audio_spectrogram_base.yaml
Only in synthmatch-image-proc/configs/data: cifar100-32x32-stk.yaml
Only in synthmatch-image-proc/configs/data: cifar100-32x32_8000Hz_0p5s_8dss_resonarium_2p.yaml
Only in synthmatch-image-proc/configs/data: cifar100-32x32_8000Hz_1p0s_16384dss_resonarium_2p.yaml
Only in synthmatch-image-proc/configs/data: cifar100-32x32_8000Hz_1p0s_16384dss_stk_1p.yaml
Only in synthmatch-image-proc/configs/data: cifar100-32x32_8000Hz_1p0s_5dss_stk_1p.yaml
Only in synthmatch-image-proc/configs/data: resonarium_32x32.yaml
Only in synthmatch-image-proc/configs/data: stk_32x32.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/data/vimh.yaml synthmatch-image-proc/configs/data/vimh.yaml
*** lightning-hydra-template-extended/configs/data/vimh.yaml	Tue Jul 15 02:04:45 2025
--- synthmatch-image-proc/configs/data/vimh.yaml	Tue Aug 26 14:29:30 2025
***************
*** 1,6 ****
  _target_: src.data.vimh_datamodule.VIMHDataModule
! data_dir: ${paths.root_dir}/data-vimh/vimh-32x32_8000Hz_1p0s_256dss_resonarium_2p
  batch_size: 128 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
! num_workers: 2
! pin_memory: False
  persistent_workers: True
--- 1,7 ----
  _target_: src.data.vimh_datamodule.VIMHDataModule
! #data_dir: ${paths.root_dir}/data-vimh/vimh-32x32x3_8000Hz_1p0s_256dss_simple_2p
! data_dir: ${paths.root_dir}/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p
  batch_size: 128 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
! num_workers: auto # Automatically detect based on platform/accelerator
! pin_memory: auto  # Automatically detect based on platform/accelerator
  persistent_workers: True
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/data/vimh_16kdss.yaml synthmatch-image-proc/configs/data/vimh_16kdss.yaml
*** lightning-hydra-template-extended/configs/data/vimh_16kdss.yaml	Tue Jul 15 23:50:08 2025
--- synthmatch-image-proc/configs/data/vimh_16kdss.yaml	Fri Jul 18 01:04:08 2025
***************
*** 1,7 ****
--- 1,10 ----
  _target_: src.data.vimh_datamodule.VIMHDataModule

  # Dataset directory
+ #data_dir: ${paths.root_dir}/data-vimh/vimh-32x32x3_8000Hz_1p0s_256dss_simple_2p
+ # ../../data-vimh/vimh-32x32x3_8000Hz_1p0s_256dss_simple_2p
  data_dir: ${paths.root_dir}/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p
+ # ../../data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p

  # Training parameters
  batch_size: 128
Only in synthmatch-image-proc/configs/experiment: cifar100_el_res_cnn_1m.yaml
Only in synthmatch-image-proc/configs/experiment: cifar100_el_stk_cnn_1m.yaml
Only in synthmatch-image-proc/configs/experiment: cifar100_res_cnn_1m.yaml
Only in synthmatch-image-proc/configs/experiment: cifar100_stk_cnn_1m.yaml
Only in lightning-hydra-template-extended/configs/experiment: vimh_cnn.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/experiment/vimh_cnn_16kdss.yaml synthmatch-image-proc/configs/experiment/vimh_cnn_16kdss.yaml
*** lightning-hydra-template-extended/configs/experiment/vimh_cnn_16kdss.yaml	Tue Jul 15 23:50:08 2025
--- synthmatch-image-proc/configs/experiment/vimh_cnn_16kdss.yaml	Tue Aug 26 14:29:30 2025
***************
*** 4,10 ****
  # Optimized for serious training evaluation with larger resonarium dataset

  defaults:
!   - override /data: vimh_16kdss
    - override /model: vimh_cnn_64k
    - override /trainer: default

--- 4,10 ----
  # Optimized for serious training evaluation with larger resonarium dataset

  defaults:
!   - override /data: vimh_16kdss  # ../data/vimh_16kdss.yaml
    - override /model: vimh_cnn_64k
    - override /trainer: default

Only in synthmatch-image-proc/configs/experiment: vimh_cnn_64k.yaml
Only in synthmatch-image-proc/configs/experiment: vimh_cnn_auraloss.yaml
Only in synthmatch-image-proc/configs/experiment: vimh_cnn_auraloss_ordinal.yaml
Only in synthmatch-image-proc/configs/experiment: vimh_cnn_auraloss_regression.yaml
Only in synthmatch-image-proc/configs/experiment: vimh_cnn_synthesis_multiscale.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/extras/default.yaml synthmatch-image-proc/configs/extras/default.yaml
*** lightning-hydra-template-extended/configs/extras/default.yaml	Tue Aug 26 15:18:34 2025
--- synthmatch-image-proc/configs/extras/default.yaml	Sat Jul 12 20:42:35 2025
***************
*** 1,9 ****
  # disable python warnings if they annoy you
  ignore_warnings: False

- # suppress specific DataLoader num_workers warnings (useful for MPS)
- ignore_dataloader_warnings: True
-
  # ask user for tags if none are provided in the config
  enforce_tags: True

--- 1,6 ----
Only in synthmatch-image-proc/configs: generate_simple.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/logger/tensorboard.yaml synthmatch-image-proc/configs/logger/tensorboard.yaml
*** lightning-hydra-template-extended/configs/logger/tensorboard.yaml	Tue Aug 26 15:19:17 2025
--- synthmatch-image-proc/configs/logger/tensorboard.yaml	Wed Jul  9 23:44:06 2025
***************
*** 4,10 ****
    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    save_dir: "${paths.output_dir}/tensorboard/"
    name: null
!   log_graph: True   # Enable computational graph logging
    default_hp_metric: True
    prefix: ""
    # version: ""
--- 4,10 ----
    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    save_dir: "${paths.output_dir}/tensorboard/"
    name: null
!   log_graph: False
    default_hp_metric: True
    prefix: ""
    # version: ""
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/model/cifar100_convnext_10m.yaml synthmatch-image-proc/configs/model/cifar100_convnext_10m.yaml
*** lightning-hydra-template-extended/configs/model/cifar100_convnext_10m.yaml	Thu Jul 10 03:47:48 2025
--- synthmatch-image-proc/configs/model/cifar100_convnext_10m.yaml	Tue Aug 26 14:29:30 2025
***************
*** 22,25 ****
--- 22,29 ----
    _target_: torch.nn.CrossEntropyLoss
    label_smoothing: 0.1     # Advanced training technique

+ <<<<<<< HEAD
  compile: false
+ =======
+ compile: false
+ >>>>>>> template/main
Only in synthmatch-image-proc/configs/model: vimh_cnn_64k_auraloss.yaml
Only in synthmatch-image-proc/configs/model: vimh_cnn_64k_melstft.yaml
Only in synthmatch-image-proc/configs/model: vimh_cnn_64k_stft.yaml
Only in synthmatch-image-proc/configs/model: vimh_cnn_64k_synthesis_auraloss.yaml
Only in synthmatch-image-proc/configs/model: vimh_cnn_64k_synthesis_mse.yaml
Only in synthmatch-image-proc/configs/model: vimh_cnn_64k_synthesis_multiscale.yaml
Only in synthmatch-image-proc/configs/model: vimh_cnn_64k_synthesis_validation.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/trainer/default.yaml synthmatch-image-proc/configs/trainer/default.yaml
*** lightning-hydra-template-extended/configs/trainer/default.yaml	Tue Aug 26 15:18:18 2025
--- synthmatch-image-proc/configs/trainer/default.yaml	Wed Jul  9 23:44:06 2025
***************
*** 14,22 ****
  # perform a validation loop every N training epochs
  check_val_every_n_epoch: 1

- # log metrics every N steps for detailed tracking
- log_every_n_steps: 10
-
  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: False
--- 14,19 ----
Only in synthmatch-image-proc/data: cifar-10-python.tar.gz
Only in lightning-hydra-template-extended/data: cifar-100-mh-example
Only in synthmatch-image-proc/data: cifar-100-python.tar.gz
Only in synthmatch-image-proc/data: cifar100-32x32_8000Hz_1p0s_256dss_resonarium_2p
Only in synthmatch-image-proc/data: cifar100-32x32_8000Hz_1p0s_256dss_stk_1p
Only in synthmatch-image-proc/data: cifar100-el-32x32_8000Hz_1p0s_256dss_resonarium_2p
Only in synthmatch-image-proc/data: cifar100-el-32x32_8000Hz_1p0s_256dss_stk_1p
Only in synthmatch-image-proc/data: cifar100mh-32x32_8000Hz_0p5s_8dss_resonarium_2p
Only in synthmatch-image-proc/data: cifar100mh-32x32_8000Hz_1p0s_256dss_resonarium_2p
Only in synthmatch-image-proc/data: cifar100mh-32x32_8000Hz_1p0s_256dss_stk_1p
Only in synthmatch-image-proc/data: cifar100mh-el-32x32_8000Hz_1p0s_256dss_resonarium_2p
Only in synthmatch-image-proc/data: cifar100mh-el-32x32_8000Hz_1p0s_256dss_stk_1p
Only in lightning-hydra-template-extended/data: resonarium_32x32
Only in lightning-hydra-template-extended/data: resonarium_32x32_binary
Only in lightning-hydra-template-extended/data: stk_32x32
Only in lightning-hydra-template-extended/data: stk_32x32_binary
Only in synthmatch-image-proc/data: test_vimh_format
Only in lightning-hydra-template-extended/data-vimh: avix-vimh-32x32x3_8000Hz_1p0s_72dss_simple_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32_8000Hz_1p0s_256dss_resonarium_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32_8000Hz_1p0s_256dss_stk_1p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32x3_8000Hz_1p0s_16384dss_resonarium_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32x3_8000Hz_1p0s_16384dss_simple_1p
Only in synthmatch-image-proc/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p: test
Binary files lightning-hydra-template-extended/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p/test_batch and synthmatch-image-proc/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p/test_batch differ
Only in synthmatch-image-proc/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p: train
Binary files lightning-hydra-template-extended/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p/train_batch and synthmatch-image-proc/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p/train_batch differ
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p/vimh_dataset_info.json synthmatch-image-proc/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p/vimh_dataset_info.json
*** lightning-hydra-template-extended/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p/vimh_dataset_info.json	Tue Jul 15 09:57:59 2025
--- synthmatch-image-proc/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p/vimh_dataset_info.json	Fri Jul 18 01:43:39 2025
***************
*** 2,24 ****
    "format": "VIMH",
    "version": "1.0",
    "dataset_name": "vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p",
!   "original_dataset_path": "/Users/jos/w/SynthMatchClean/data/dataset_8000Hz_1p0s_16384dss_simple_2p",
!   "output_format": "pickle",
!   "n_samples": 16384,
!   "train_samples": 13108,
!   "test_samples": 3276,
!   "image_size": "32x32x3",
    "height": 32,
    "width": 32,
    "channels": 3,
    "varying_parameters": 2,
    "parameter_names": [
      "note_velocity",
!     "decay_time"
    ],
    "label_encoding": {
!     "format": "[height] [width] [channels] [N] [param1_id] [param1_val] [param2_id] [param2_val] ...",
!     "metadata_bytes": 6,
      "N_range": [
        0,
        255
--- 2,70 ----
      "format": "VIMH",
      "version": "1.0",
      "dataset_name": "vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p",
!     "output_format": "binary",
      "height": 32,
      "width": 32,
      "channels": 3,
      "varying_parameters": 2,
      "parameter_names": [
          "note_velocity",
!         "log10_decay_time"
      ],
+     "train_samples": 13107,
+     "test_samples": 3277,
+     "total_samples": 16384,
+     "n_samples": 16384,
+     "sample_rate": 8000,
+     "duration": 1.0,
+     "synth_type": "simple",
+     "image_size": "32x32x3",
+     "parameter_mappings": {
+         "note_number": {
+             "min": 43.35,
+             "max": 43.35,
+             "description": "simple parameter: note_number"
+         },
+         "note_velocity": {
+             "min": 126.0,
+             "max": 127.0,
+             "description": "simple parameter: note_velocity"
+         },
+         "log10_decay_time": {
+             "min": -2.0,
+             "max": 0.301,
+             "description": "simple parameter: log10_decay_time"
+         }
+     },
+     "fixed_parameters": {
+         "note_number": {
+             "value": 43.35,
+             "description": "MIDI note number (fixed at 100 Hz)"
+         }
+     },
+     "pre_emphasis_coefficient": 0.0,
+     "spectrogram_config": {
+         "sample_rate": 8000,
+         "type": "stft",
+         "n_fft": 80,
+         "n_window": 80,
+         "hop_length": 80,
+         "window_type": "rectangular",
+         "bins_per_harmonic": 1.0,
+         "n_bins": 32,
+         "method": "efficient_leaf"
+     },
+     "mel_config": {
+         "freq_min": 40.0,
+         "freq_max_ratio": 0.9
+     },
      "label_encoding": {
!         "format": "[height] [width] [channels] [spec_min] [spec_max] [N] [param1_id] [param1_val] [param2_id] [param2_val] ...",
!         "metadata_bytes": 14,
!         "scale_factors": {
!             "spec_min": "float32 - minimum dB value before normalization",
!             "spec_max": "float32 - maximum dB value before normalization"
!         },
          "N_range": [
              0,
              255
***************
*** 31,62 ****
        0,
        255
      ]
-   },
-   "spectrogram_config": {
-     "sample_rate": 8000,
-     "freq_min": 40.0,
-     "freq_max_ratio": 0.9,
-     "n_mels": 32,
-     "method": "torchaudio"
-   },
-   "parameter_mappings": {
-     "note_number": {
-       "min": 42.0,
-       "max": 42.0,
-       "scale": "linear",
-       "description": "simple parameter: note_number"
-     },
-     "note_velocity": {
-       "min": 10.0,
-       "max": 127.0,
-       "scale": "linear",
-       "description": "simple parameter: note_velocity"
-     },
-     "decay_time": {
-       "min": 0.01,
-       "max": 2.0,
-       "scale": "log",
-       "description": "simple parameter: decay_time"
-     }
    }
  }
\ No newline at end of file
--- 77,81 ----
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32x3_8000Hz_1p0s_16384dss_stk_1p
Only in synthmatch-image-proc/data-vimh: vimh-32x32x3_8000Hz_1p0s_256dss_simple_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32_8000Hz_1p0s_256dss_resonarium_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32_8000Hz_1p0s_256dss_stk_1p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32x3_8000Hz_1p0s_16384dss_resonarium_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32x3_8000Hz_1p0s_16384dss_simple_1p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32x3_8000Hz_1p0s_16384dss_simple_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32x3_8000Hz_1p0s_16384dss_stk_1p
Only in synthmatch-image-proc: data2vimh.py
Only in synthmatch-image-proc: display_vimh.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/architectures.md synthmatch-image-proc/docs/architectures.md
*** lightning-hydra-template-extended/docs/architectures.md	Tue Aug 26 15:41:31 2025
--- synthmatch-image-proc/docs/architectures.md	Tue Aug 26 14:29:30 2025
***************
*** 9,15 ****
  | Architecture | Parameters | Type | Best For | Config Files |
  |-------------|------------|------|----------|-------------|
  | **SimpleDenseNet** | 8K-68K | Fully-connected | Quick prototyping | `mnist_sdn_*.yaml` |
- | **SimpleMLP** | 8K-68K | Fully-connected (no BN) | Batch size = 1 | `simple_mlp_*.yaml` |
  | **SimpleCNN** | 8K-3.3M | Convolutional | Image classification | `mnist_cnn_*.yaml` |
  | **ConvNeXt-V2** | 18K-725K | Modern CNN | Efficient performance | `mnist_convnext_*.yaml` |
  | **Vision Transformer** | 38K-821K | Transformer | Large-scale datasets | `mnist_vit_*.yaml` |
--- 9,14 ----
***************
*** 45,81 ****
  python src/train.py model=mnist_sdn_68k    # 68K parameters
  ```

- ### SimpleMLP
- **Type**: Fully-connected neural network without BatchNorm
- **Best for**: Batch size = 1 scenarios, inference-only applications
-
- **Architecture**:
- - **Input**: Flattened 28×28 images (784 features)
- - **Hidden layers**: Configurable (default: [256, 128])
- - **Normalization**: None (crucial for batch_size=1)
- - **Activation**: ReLU
- - **Dropout**: Configurable (default: 0.1)
- - **Output**: Configurable classes
-
- **Key Difference from SimpleDenseNet**:
- SimpleMLP omits BatchNorm layers, making it suitable for scenarios where batch_size=1 is required (e.g., real-time inference, audio synthesis applications).
-
- **Characteristics**:
- - **Parameters**: Configurable (8K-68K typical range)
- - **Speed**: Fast training and inference ⚡
- - **Memory**: Low requirements
- - **Batch size**: Works with any batch size including 1
-
- **Usage**:
- ```bash
- # Train SimpleMLP
- python src/train.py model=simple_mlp
-
- # With specific configuration
- python src/train.py model=simple_mlp_256_128  # Hidden layers [256, 128]
- python src/train.py trainer.datamodule.batch_size=1  # Batch size 1
- ```
-
  ### SimpleCNN
  **Type**: Convolutional neural network
  **Best for**: Image classification with spatial structure preservation
--- 44,49 ----
***************
*** 98,109 ****
  - **Primary head**: Digit classification (10 classes)
  - **Secondary heads**: Thickness (5 classes), Smoothness (3 classes)

- **Auxiliary Features Support**:
- SimpleCNN can incorporate auxiliary scalar features alongside image data:
- - **Hybrid input**: Image tensor + auxiliary scalar vector
- - **Feature fusion**: Auxiliary features processed through separate MLP, concatenated with CNN features
- - **Use case**: Incorporating measured parameters, metadata, or other scalar inputs
-
  **Usage**:
  ```bash
  # Single-head CNN
--- 66,71 ----
***************
*** 117,125 ****
  # Different sizes
  python src/train.py model=mnist_cnn_8k     # 8K parameters
  python src/train.py model=mnist_cnn_421k   # 421K parameters
-
- # With auxiliary features (requires compatible dataset)
- python src/train.py model.net.auxiliary_input_size=5  # 5 auxiliary features
  ```

  ### ConvNeXt-V2
--- 79,84 ----
Only in synthmatch-image-proc/docs: auraloss_usage.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/configuration.md synthmatch-image-proc/docs/configuration.md
*** lightning-hydra-template-extended/docs/configuration.md	Mon Jul  7 23:27:09 2025
--- synthmatch-image-proc/docs/configuration.md	Tue Aug 26 14:29:30 2025
***************
*** 253,259 ****
  data_dir: /path/to/your/data
  log_dir: /path/to/your/logs

! # Hardware optimization
  data:
    num_workers: 8
    pin_memory: true
--- 253,259 ----
  data_dir: /path/to/your/data
  log_dir: /path/to/your/logs

! # Hardware optimization (see below)
  data:
    num_workers: 8
    pin_memory: true
***************
*** 261,266 ****
--- 261,284 ----
  trainer:
    accelerator: mps  # or gpu, cpu
  ```
+
+ #### Hardware Dependencies for `num_workers` and `pin_memory`
+
+ These parameters are highly **hardware dependent**
+   - CUDA (NVIDIA GPUs): Can use num_workers > 0 and pin_memory: true
+   - CPU training: Can use num_workers > 0 but pin_memory: false
+   - Linux/Windows with CUDA: Full multiprocessing support
+   - MacOS + MPS requires `num_workers: 0` and `pin_memory: false`
+       - MPS tensors can't be serialized across processes: When PyTorch
+         tries to share data between the main process and worker
+         processes, it needs to serialize tensors. MPS tensors have
+         special GPU memory references that can't be safely passed
+         between processes.
+       - Error: RuntimeError: "_share_filename_: only available on CPU" happens because MPS tensors try to use CPU-only sharing mechanisms.
+       - Autograd limitations: The error "Cowardly refusing to
+         serialize non-leaf tensor which requires_grad" shows that
+         tensors with gradients can't cross process boundaries, which
+         is especially problematic with MPS.

  ## 🔄 Dynamic Configuration

diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/features.md synthmatch-image-proc/docs/features.md
*** lightning-hydra-template-extended/docs/features.md	Tue Aug 26 15:41:31 2025
--- synthmatch-image-proc/docs/features.md	Tue Aug 26 14:29:30 2025
***************
*** 30,37 ****
  | Architecture | Parameters | Description |
  |-------------|------------|-------------|
  | **SimpleDenseNet** | 8K-68K | Original fully-connected network |
! | **SimpleMLP** | 8K-68K | MLP without BatchNorm (for batch_size=1) |
! | **SimpleCNN** | 8K-3.3M | Convolutional neural network with auxiliary support |
  | **ConvNeXt-V2** | 18K-725K | Modern CNN with Global Response Normalization |
  | **Vision Transformer** | 38K-821K | Transformer on image patches |
  | **EfficientNet** | 22K-7M | Highly efficient CNN architecture |
--- 30,36 ----
  | Architecture | Parameters | Description |
  |-------------|------------|-------------|
  | **SimpleDenseNet** | 8K-68K | Original fully-connected network |
! | **SimpleCNN** | 8K-3.3M | Convolutional neural network |
  | **ConvNeXt-V2** | 18K-725K | Modern CNN with Global Response Normalization |
  | **Vision Transformer** | 38K-821K | Transformer on image patches |
  | **EfficientNet** | 22K-7M | Highly efficient CNN architecture |
***************
*** 54,92 ****
  - **Training**: `make train`, `make trc` (CNN), `make trcn` (ConvNeXt)
  - **Quick tests**: `make tq`, `make tqc`, `make tqcn`
  - **Benchmarks**: `make cb10c` (CIFAR-10), `make cbs` (full suite)
- - **Utilities**: `make lc` (list configs), `make tb` (launch TensorBoard)
- - **Advanced**: `make tg` (gradient tracking), `make ca` (architecture comparison)
-
- ### 7. Advanced Training Features
-
- #### Soft Target Support
- Reduce quantization artifacts in regression tasks:
- ```yaml
- # Enable soft targets with Gaussian distribution
- data:
-   target_width: 0.1  # Controls softness (0.0 = hard targets)
- ```
-
- #### Auxiliary Feature Support
- Combine CNN features with scalar auxiliary inputs:
- ```python
- # Forward pass with auxiliary features
- output = model(image_tensor, auxiliary_scalar_features)
- ```
-
- #### Gradient Statistics Tracking
- Monitor gradient flow during training:
- ```bash
- # Train with gradient statistics
- make tg  # or python src/train.py callbacks=grouped_progress_bar_with_gradients
- ```
-
- #### Flexible Data Transforms
- Handles variable channel images (e.g., RGB + feature channels):
- ```python
- from src.data.flexible_transforms import FlexibleNormalize, ChannelAwareCompose
- # Automatically adapts to 3, 5, or more channels
- ```


  ## 📊 Expected Performance
--- 53,58 ----
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/multihead.md synthmatch-image-proc/docs/multihead.md
*** lightning-hydra-template-extended/docs/multihead.md	Tue Jul 15 03:20:30 2025
--- synthmatch-image-proc/docs/multihead.md	Tue Aug 26 14:29:30 2025
***************
*** 543,558 ****
  #### Basic Training
  ```bash
  # Train with VIMH dataset
! python src/train.py experiment=vimh_cnn

  # Train with specific dataset
! python src/train.py experiment=vimh_cnn \
    data.data_dir=data-vimh/vimh-32x32_8000Hz_1p0s_256dss_resonarium_2p
  ```

  #### Configuration Example
  ```yaml
! # configs/experiment/vimh_cnn.yaml
  defaults:
    - override /data: vimh
    - override /model: vimh_cnn_64k
--- 543,558 ----
  #### Basic Training
  ```bash
  # Train with VIMH dataset
! python src/train.py experiment=vimh_cnn_64k

  # Train with specific dataset
! python src/train.py experiment=vimh_cnn_64k \
    data.data_dir=data-vimh/vimh-32x32_8000Hz_1p0s_256dss_resonarium_2p
  ```

  #### Configuration Example
  ```yaml
! # configs/experiment/vimh_cnn_64k.yaml
  defaults:
    - override /data: vimh
    - override /model: vimh_cnn_64k
Only in lightning-hydra-template-extended/docs: png
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/vimh.md synthmatch-image-proc/docs/vimh.md
*** lightning-hydra-template-extended/docs/vimh.md	Tue Aug 26 15:03:15 2025
--- synthmatch-image-proc/docs/vimh.md	Tue Aug 26 15:53:48 2025
***************
*** 20,66 ****

  ### Binary Layout Per Sample
  ```
! Metadata: 6 bytes (height, width, channels) - three 16-bit unsigned integers, little-endian
  Label Data: 1 + 2N bytes
!   - Byte 0: N (number of varying parameters, 0-255)
!   - Bytes 1,2: param1_id (0-255), param1_val (0-255)
!   - Bytes 3,4: param2_id (0-255), param2_val (0-255)
    - ...
!   - Bytes 2N-1,2N: paramN_id (0-255), paramN_val (0-255)
! Image Data: height*width*channels bytes (raw pixel values, typically 0-255)
! Total size: 6 + 1 + 2N + height*width*channels bytes per sample
  ```

  ### Example: 2-Parameter Dataset (32x32x3 like CIFAR-100)
  ```
! Metadata: [32, 32, 3] (height=32, width=32, channels=3) - 6 bytes
! Labels: [2, 0, 191, 1, 127] (N=2 parameters) - 5 bytes
! Image: 3072 bytes (32*32*3 RGB spectrogram/image data)
! Total: 6 + 5 + 3072 = 3083 bytes per sample
  ```

  If note_number=51.5 (range 50-52) and note_velocity=81.0 (range 80-82):
  ```
! Complete sample: [32, 32, 3, 2, 0, 191, 1, 127, <3072 image bytes>]
!   Metadata: height=32, width=32, channels=3 (6 bytes)
!   Labels: N=2 parameters (1 byte)
!   param_0 (note_number): id=0, val=191 → dequantized to 51.5 (2 bytes)
!   param_1 (note_velocity): id=1, val=127 → dequantized to 81.0 (2 bytes)
!   Image data: 3072 bytes of RGB pixel values
  ```

  ### Example: VIMH Metadata for MNIST Images (28x28x1)
  ```
! Metadata: [28, 28, 1] (height=28, width=28, channels=1) - 6 bytes
! Labels: [1, 0, 128] (N=1 parameter) - 3 bytes
  Image: 784 bytes (28*28*1 grayscale)
! Total: 6 + 3 + 784 = 793 bytes per sample
!
! Sample breakdown:
!   Metadata: height=28, width=28, channels=1 (6 bytes)
!   Labels: N=1 parameter (1 byte)
!   param_0: id=0, val=128 (2 bytes) - could represent digit thickness/style
!   Image data: 784 bytes of grayscale pixel values
  ```

  ## Dataset Structure
--- 20,62 ----

  ### Binary Layout Per Sample
  ```
! Metadata: 14 bytes
!   - Bytes 0-5: height, width, channels (three 16-bit fields)
!   - Bytes 6-9: spec_min (float32) - minimum dB value before normalization
!   - Bytes 10-13: spec_max (float32) - maximum dB value before normalization
  Label Data: 1 + 2N bytes
!   - Byte 0: N (number of varying parameters)
!   - Bytes 1,2: param1_id, param1_val
!   - Bytes 3,4: param2_id, param2_val
    - ...
!   - Bytes 2N-1,2N: paramN_id, paramN_val
! Image Data: height*width*channels bytes (normalized to [0,255])
  ```

  ### Example: 2-Parameter Dataset (32x32x3 like CIFAR-100)
  ```
! Metadata: [32, 32, 3, -65.2, -12.4] (height=32, width=32, channels=3, spec_min=-65.2dB, spec_max=-12.4dB)
! Labels: [2, 0, 191, 1, 127] (N=2 parameters)
! Image: 3072 bytes (32*32*3 RGB spectrogram, normalized to [0,255])
! Total: 14 + 5 + 3072 = 3091 bytes per sample
  ```

  If note_number=51.5 (range 50-52) and note_velocity=81.0 (range 80-82):
  ```
! Complete sample: [32, 32, 3, -65.2, -12.4, 2, 0, 191, 1, 127, <3072 image bytes>]
!   height=32, width=32, channels=3
!   spec_min=-65.2dB, spec_max=-12.4dB (normalization scale factors)
!   N=2 parameters
!   param_0 (note_number): 191 → 51.5
!   param_1 (note_velocity): 127 → 81.0
  ```

  ### Example: VIMH Metadata for MNIST Images (28x28x1)
  ```
! Metadata: [28, 28, 1, 0.0, 255.0] (height=28, width=28, channels=1, spec_min=0.0, spec_max=255.0)
! Labels: [1, 0, 128] (N=1 parameter)
  Image: 784 bytes (28*28*1 grayscale)
! Total: 14 + 3 + 784 = 801 bytes per sample
  ```

  ## Dataset Structure
***************
*** 87,94 ****
    "varying_parameters": 2,
    "parameter_names": ["note_number", "note_velocity"],
    "label_encoding": {
!     "format": "[height] [width] [channels] [N] [param1_id] [param1_val] [param2_id] [param2_val] ...",
!     "metadata_bytes": 3,
      "N_range": [0, 255],
      "param_id_range": [0, 255],
      "param_val_range": [0, 255]
--- 83,94 ----
    "varying_parameters": 2,
    "parameter_names": ["note_number", "note_velocity"],
    "label_encoding": {
!     "format": "[height] [width] [channels] [spec_min] [spec_max] [N] [param1_id] [param1_val] [param2_id] [param2_val] ...",
!     "metadata_bytes": 14,
!     "scale_factors": {
!       "spec_min": "float32 - minimum dB value before normalization",
!       "spec_max": "float32 - maximum dB value before normalization"
!     },
      "N_range": [0, 255],
      "param_id_range": [0, 255],
      "param_val_range": [0, 255]
***************
*** 105,110 ****
--- 105,111 ----
  - **Efficient**: 8-bit quantization provides ~100 perceptual resolution steps
  - **Multihead CNN ready**: Enables training CNNs with multiple output heads
  - **Format flexible**: Supports spectrograms (32x32x3), MNIST (28x28x1), and custom sizes
+ - **Level-aware**: Scale factors preserve absolute level information for spectrograms

  ## Use Cases

***************
*** 131,140 ****

  ### Limitations
  - Maximum 255 varying parameters per sample
! - Parameter values quantized to 0-255 range (8-bit resolution)
! - Image dimensions limited to 65,535×65,535 (16-bit metadata fields)
! - Maximum 65,535 channels per image (16-bit metadata field)
! - Parameter precision: ~100 perceptual steps (8-bit quantization)

  ## Makefile Targets

--- 132,140 ----

  ### Limitations
  - Maximum 255 varying parameters per sample
! - Parameter values quantized to 0-255 range
! - Image dimensions limited to 255x255 (due to single byte metadata)
! - Maximum 255 channels per image

  ## Makefile Targets

***************
*** 147,152 ****
--- 147,159 ----

  ## Technical Implementation

+ ### Normalization Scale Factors
+ Each sample includes scale factors that preserve absolute level information:
+ - **spec_min**: Minimum spectrogram value in dB before normalization (float32)
+ - **spec_max**: Maximum spectrogram value in dB before normalization (float32)
+ - **Purpose**: Enables networks to learn level-aware relationships between parameters and spectral content
+ - **Usage**: Can be used as additional conditioning inputs during training
+
  ### Label Encoding
  Each sample's parameters are encoded as a sequence of (parameter_id, parameter_value) pairs:
  - **parameter_id**: Index into the varying parameters list (0-255)
***************
*** 168,222 ****

  ## Version History

  - **v2.0**: Added height, width, channels metadata for full generalization
  - **v1.0**: Initial implementation based on CIFAR-100 structure

- ## File Reading/Writing
-
- ### Python Usage
- ```python
- from src.data.vimh_dataset import VIMHDataset, VIMHDataModule
-
- # Load existing VIMH dataset
- dataset = VIMHDataset(data_dir="data-vimh/my_dataset", format="binary")
-
- # Access samples
- sample = dataset[0]  # Returns (image, labels) tuple
- image, labels = sample
- # image: torch.Tensor of shape (C, H, W)
- # labels: dict with parameter names and values
-
- # Use with DataModule for training
- datamodule = VIMHDataModule(data_dir="data-vimh/my_dataset")
- datamodule.setup()
- train_loader = datamodule.train_dataloader()
- ```
-
- ### Dataset Auto-Configuration
- VIMH datasets automatically configure neural network models based on their metadata:
-
- ```python
- # Model automatically configures from VIMH dataset info
- python src/train.py experiment=vimh_cnn_16kdss
- # Reads data-vimh/*/vimh_dataset_info.json to determine:
- # - Input image dimensions (height, width, channels)
- # - Number of output heads (equal to varying_parameters)
- # - Parameter names for head naming
- # - Loss function selection based on parameter types
- ```
-
- ### Performance Considerations
- - **Binary format**: Fastest loading, smallest file size
- - **Pickle format**: Python-friendly but larger files
- - **Memory usage**: ~793 bytes per MNIST sample, ~3083 bytes per CIFAR-100 sample
- - **Loading speed**: ~10x faster initialization compared to traditional formats
-
  ## Related Formats

  VIMH was originally inspired by CIFAR-100 but extends it significantly:
! - **CIFAR-100**: Fixed 32x32x3 images, 2 labels (coarse/fine classes)
! - **VIMH**: Variable image dimensions, 0-255 continuous parameters with self-describing metadata
! - **HDF5**: Alternative format, but VIMH is more compact and self-describing
! - **TFRecord**: Similar concept, but VIMH uses simpler binary format

! The format maintains some conceptual CIFAR-100 compatibility for 32x32x3 images while enabling much broader applications through its generalized, self-describing design.
--- 175,188 ----

  ## Version History

+ - **v2.1**: Added normalization scale factors (spec_min, spec_max) for level-aware learning
  - **v2.0**: Added height, width, channels metadata for full generalization
  - **v1.0**: Initial implementation based on CIFAR-100 structure

  ## Related Formats

  VIMH was originally inspired by CIFAR-100 but extends it significantly:
! - **CIFAR-100**: Fixed 32x32x3 images, 2 labels (coarse/fine)
! - **VIMH**: Variable image dimensions, 0-255 parameters with metadata

! The format maintains some CIFAR-100 compatibility for 32x32x3 images while enabling much broader applications through its generalized design.
Only in synthmatch-image-proc/docs: vimh.md.orig
Only in synthmatch-image-proc/docs: vimh_generate.md
Only in synthmatch-image-proc/docs: vimh_generation.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/vimh_loss_functions.md synthmatch-image-proc/docs/vimh_loss_functions.md
*** lightning-hydra-template-extended/docs/vimh_loss_functions.md	Tue Jul 15 08:12:29 2025
--- synthmatch-image-proc/docs/vimh_loss_functions.md	Tue Aug 26 14:29:30 2025
***************
*** 8,14 ****
  2. **Discrete predictions**: Uses argmax, losing continuity information
  3. **Poor generalization**: Model doesn't learn the ordinal relationship between values

! ## Solution: Distance-Aware Loss Functions

  ### 1. OrdinalRegressionLoss (Recommended)

--- 8,14 ----
  2. **Discrete predictions**: Uses argmax, losing continuity information
  3. **Poor generalization**: Model doesn't learn the ordinal relationship between values

! ## Solution: Distance-Aware Loss Functions and Synthesis Validation

  ### 1. OrdinalRegressionLoss (Recommended)

***************
*** 88,93 ****
--- 88,195 ----
  - Still uses classification metrics
  - Distance-aware penalties

+ ### 4. SynthesisValidationLoss (Most Principled)
+
+ **Best for**: Synthesis parameter estimation with true round-trip validation
+
+ ```python
+ from src.models.losses import SynthesisValidationLoss
+
+ criterion = SynthesisValidationLoss(
+     param_ranges={
+         'note_number': (36.0, 96.0),
+         'note_velocity': (1.0, 127.0),
+         'log10_decay_time': (-2.0, 0.0)
+     },
+     param_bounds={...},  # Same as param_ranges
+     sample_rate=8000,
+     duration=1.0,
+     loss_function="mse"  # or "auraloss_MultiResolutionSTFTLoss"
+ )
+ ```
+
+ **How it works**:
+ 1. Takes predicted synthesis parameters (normalized [0,1])
+ 2. Denormalizes to actual parameter values
+ 3. Synthesizes audio using SimpleSynth
+ 4. Converts synthesized audio to spectrogram
+ 5. Compares with target spectrogram using specified loss
+
+ **Benefits**:
+ - True round-trip validation
+ - Tests if parameters can reproduce original audio
+ - Physically meaningful validation
+ - Supports both standard and auraloss functions
+
+ ### 5. MultiScaleSpectralLoss (NEW - PNP Integration)
+
+ **Best for**: Multi-resolution spectral analysis with proven research backing
+
+ ```python
+ from src.models.losses import MultiScaleSpectralLoss
+
+ criterion = MultiScaleSpectralLoss(
+     max_n_fft=2048,      # Maximum STFT window size
+     num_scales=6,        # Number of different STFT scales
+     p=1.0               # L1 norm for distance computation
+ )
+ ```
+
+ **How it works**:
+ - Computes STFT at multiple resolutions simultaneously (e.g., 2048, 1024, 512, 256, 128, 64)
+ - Uses different hop lengths for each scale (typically n_fft/4)
+ - Compares magnitude spectrograms at each scale using configurable distance metric
+ - Averages loss across all scales for comprehensive spectral analysis
+
+ **Benefits**:
+ - **Research-proven**: Adapted from published PNP research (ICASSP, TASLP papers)
+ - **Multi-scale analysis**: Captures both fine and coarse spectral features
+ - **Superior to basic losses**: More perceptually aware than MSE or L1 alone
+ - **Configurable**: Adjustable number of scales and window sizes
+ - **Gradient-friendly**: Fully differentiable for end-to-end training
+ - **Drop-in replacement**: Can replace any spectral loss function
+
+ **Usage in SynthesisValidationLoss**:
+ ```python
+ criterion = SynthesisValidationLoss(
+     param_ranges={...},
+     param_bounds={...},
+     loss_function="multiscale_spectral"  # Uses MultiScaleSpectralLoss
+ )
+ ```
+
+ ### 6. Auraloss Integration
+
+ **Best for**: Audio-perceptual loss functions on spectrograms
+
+ ```python
+ # Via loss_configs in model YAML
+ loss_configs:
+   param_0:
+     _target_: auraloss.freq.MultiResolutionSTFTLoss
+     fft_sizes: [1024, 2048, 4096]
+     hop_sizes: [256, 512, 1024]
+     win_lengths: [1024, 2048, 4096]
+     spectrogram_mode: true  # Important!
+ ```
+
+ **Available Auraloss Functions**:
+
+ **Frequency Domain**:
+ - `STFTLoss`: Basic STFT-based loss
+ - `MultiResolutionSTFTLoss`: Multi-resolution STFT loss
+ - `MelSTFTLoss`: Mel-scaled STFT loss
+ - `RandomResolutionSTFTLoss`: Random resolution STFT
+ - `SumAndDifferenceSTFTLoss`: Sum/difference STFT
+
+ **Time Domain**:
+ - `ESRLoss`: Error-to-signal ratio
+ - `DCLoss`: DC error loss
+ - `LogCoshLoss`: Log hyperbolic cosine
+ - `SNRLoss`: Signal-to-noise ratio
+ - `SISDRLoss`: Scale-invariant SDR
+ - `SDSDRLoss`: Scale-dependent SDR
+
  ## Configuration Examples

  ### Original CrossEntropyLoss (Current)
***************
*** 118,123 ****
--- 220,310 ----
      alpha: 0.1
  ```

+ ### NEW: MultiScaleSpectralLoss (PNP Integration)
+ ```yaml
+ # configs/model/vimh_cnn_64k_multiscale.yaml
+ loss_configs:
+   param_0:
+     _target_: src.models.losses.MultiScaleSpectralLoss
+     max_n_fft: 1024        # Adjusted for 8kHz audio
+     num_scales: 4          # Fewer scales for smaller audio
+     p: 1.0                 # L1 norm (robust to outliers)
+ ```
+
+ ### Enhanced Synthesis Validation with MultiScaleSpectralLoss
+ ```yaml
+ # configs/model/vimh_cnn_64k_synthesis_multiscale.yaml
+ loss_configs:
+   param_0:
+     _target_: src.models.losses.SynthesisValidationLoss
+     param_ranges:
+       note_number: [36.0, 96.0]
+       note_velocity: [1.0, 127.0]
+       log10_decay_time: [-2.0, 0.0]
+     param_bounds:
+       note_number: [36.0, 96.0]
+       note_velocity: [1.0, 127.0]
+       log10_decay_time: [-2.0, 0.0]
+     sample_rate: 8000
+     duration: 1.0
+     spectrogram_config:
+       stft:
+         type: mel
+         n_fft: 512
+         n_window: 128
+         hop_length: 64
+         window_type: rectangular
+       mel:
+         freq_min: 40.0
+         freq_max_ratio: 0.9
+       height: 32
+       width: 32
+     loss_function: multiscale_spectral  # NEW: Uses MultiScaleSpectralLoss
+ ```
+
+ ### Legacy Synthesis Validation with Auraloss
+ ```yaml
+ # configs/model/vimh_cnn_64k_synthesis_auraloss.yaml
+ loss_configs:
+   param_0:
+     _target_: src.models.losses.SynthesisValidationLoss
+     param_ranges:
+       note_number: [36.0, 96.0]
+       note_velocity: [1.0, 127.0]
+       log10_decay_time: [-2.0, 0.0]
+     param_bounds:
+       note_number: [36.0, 96.0]
+       note_velocity: [1.0, 127.0]
+       log10_decay_time: [-2.0, 0.0]
+     sample_rate: 8000
+     duration: 1.0
+     spectrogram_config:
+       stft:
+         type: mel
+         n_fft: 512
+         n_window: 128
+         hop_length: 64
+         window_type: rectangular
+       mel:
+         freq_min: 40.0
+         freq_max_ratio: 0.9
+       height: 32
+       width: 32
+     loss_function: auraloss_MultiResolutionSTFTLoss
+ ```
+
+ ### Direct Auraloss on Spectrograms
+ ```yaml
+ # configs/model/vimh_cnn_64k_auraloss.yaml
+ loss_configs:
+   param_0:
+     _target_: auraloss.freq.MultiResolutionSTFTLoss
+     fft_sizes: [1024, 2048, 4096]
+     hop_sizes: [256, 512, 1024]
+     win_lengths: [1024, 2048, 4096]
+     spectrogram_mode: true  # Important for spectrogram inputs
+ ```
+
  ## Usage

  ### Training with Ordinal Regression Loss
***************
*** 148,165 ****

  ```python
  def _compute_predictions(self, logits: torch.Tensor, criterion, head_name: str) -> torch.Tensor:
!     if self._is_regression_loss(criterion):
          if isinstance(criterion, OrdinalRegressionLoss):
!             # Weighted average of probabilities
              probs = F.softmax(logits, dim=1)
!             class_centers = torch.arange(criterion.num_classes, device=logits.device)
              preds = torch.sum(probs * class_centers.unsqueeze(0), dim=1)
          else:
!             # Direct regression output
!             preds = logits.squeeze(-1)
      else:
!         # Classification: argmax
          preds = torch.argmax(logits, dim=1)
      return preds
  ```

--- 335,376 ----

  ```python
  def _compute_predictions(self, logits: torch.Tensor, criterion, head_name: str) -> torch.Tensor:
!     """Compute predictions based on loss function type."""
!     if self.output_mode == "regression":
!         # For pure regression mode, logits are already sigmoid-activated [0,1] values
!         # Convert to parameter space using datamodule bounds
!         normalized_preds = logits.squeeze(-1)
!
!         # Get parameter bounds for denormalization
!         param_bounds = None
!         try:
!             if hasattr(self.trainer, 'datamodule') and hasattr(self.trainer.datamodule, 'param_bounds'):
!                 param_bounds = self.trainer.datamodule.param_bounds
!         except RuntimeError:
!             param_bounds = None
!
!         if param_bounds and head_name in param_bounds:
!             param_min, param_max = param_bounds[head_name]
!             preds = param_min + normalized_preds * (param_max - param_min)
!         else:
!             preds = normalized_preds  # Fallback to normalized values
!     elif self._is_regression_loss(criterion):
          if isinstance(criterion, OrdinalRegressionLoss):
!             # For ordinal regression, use weighted average of class probabilities
              probs = F.softmax(logits, dim=1)
!             class_centers = torch.arange(criterion.num_classes, device=logits.device, dtype=torch.float32)
              preds = torch.sum(probs * class_centers.unsqueeze(0), dim=1)
+         elif isinstance(criterion, QuantizedRegressionLoss):
+             # For quantized regression, output should be single continuous value
+             preds = logits.squeeze(-1) if logits.dim() > 1 else logits
+             preds = torch.clamp(preds, 0, criterion.num_classes - 1)
          else:
!             # For standard regression losses, assume single output
!             preds = logits.squeeze(-1) if logits.dim() > 1 else logits
      else:
!         # Classification: use argmax
          preds = torch.argmax(logits, dim=1)
+
      return preds
  ```

***************
*** 235,253 ****

  ## Best Practices

! 1. **Use OrdinalRegressionLoss** for VIMH datasets with quantized continuous parameters
! 2. **Let parameter ranges auto-configure** from dataset metadata
! 3. **Start with L1 regression loss** (robust to outliers)
! 4. **Set alpha=0.1** for classification regularization
! 5. **Monitor both accuracy and loss** to understand model behavior
! 6. **Compare with CrossEntropyLoss** to validate improvements
! 7. **Interpret loss values as parameter deviations** in perceptual units

  ## Future Enhancements

  - **Custom metrics**: Add distance-based accuracy metrics
  - **Adaptive weighting**: Learn optimal alpha during training
  - **Multi-scale loss**: Combine losses at different scales
  - **Curriculum learning**: Start with classification, gradually shift to regression

! This distance-aware loss function framework provides a much more appropriate approach for training on quantized continuous parameters in VIMH datasets.
--- 446,561 ----

  ## Best Practices

! 1. **Use MultiScaleSpectralLoss** for spectral analysis - proven research backing and superior perceptual awareness
! 2. **Use OrdinalRegressionLoss** for VIMH datasets with quantized continuous parameters
! 3. **Combine both**: Use SynthesisValidationLoss with `loss_function: multiscale_spectral` for synthesis parameter estimation
! 4. **Let parameter ranges auto-configure** from dataset metadata
! 5. **Start with L1 regression loss** (robust to outliers)
! 6. **Set alpha=0.1** for classification regularization
! 7. **Monitor both accuracy and loss** to understand model behavior
! 8. **Compare with CrossEntropyLoss** to validate improvements
! 9. **Interpret loss values as parameter deviations** in perceptual units
! 10. **Use tested small steps**: Start with MultiScaleSpectralLoss before advanced features

+ ## Auraloss Integration Details
+
+ ### Installation
+
+ ```bash
+ pip install 'auraloss[all]>=0.4.0'
+ ```
+
+ ### Using Auraloss with Synthesis Validation
+
+ The most principled approach combines synthesis validation with auraloss:
+
+ 1. **Predict synthesis parameters** from spectrograms
+ 2. **Synthesize audio** using predicted parameters
+ 3. **Convert to spectrogram** matching original format
+ 4. **Compare spectrograms** using auraloss functions
+
+ ```yaml
+ # Example: Synthesis validation with MultiResolutionSTFTLoss
+ loss_configs:
+   param_0:
+     _target_: src.models.losses.SynthesisValidationLoss
+     param_ranges: {...}
+     param_bounds: {...}
+     loss_function: auraloss_MultiResolutionSTFTLoss
+ ```
+
+ ### Supported Loss Functions in Synthesis Validation
+
+ **Standard Losses**:
+ - `mse`: Standard MSE loss
+ - `l1`: L1 loss
+ - `huber`: Huber loss
+
+ **NEW: PNP-Integrated Losses** (Recommended):
+ - `multiscale_spectral`: MultiScaleSpectralLoss - proven multi-resolution STFT from PNP research
+
+ **Legacy Auraloss Functions**:
+ - `auraloss_STFTLoss`: Basic STFT loss
+ - `auraloss_MultiResolutionSTFTLoss`: Multi-resolution STFT
+ - `auraloss_MelSTFTLoss`: Mel-scaled STFT
+
+ **Recommendation**: Use `multiscale_spectral` for new implementations as it provides:
+ - Better perceptual analysis than basic MSE/L1
+ - Research-proven effectiveness (ICASSP, TASLP papers)
+ - Superior gradient properties for synthesis validation
+ - No external dependencies (unlike auraloss functions)
+
+ ### Direct Auraloss on Spectrograms
+
+ For models that output spectrograms directly:
+
+ ```python
+ # In model config
+ loss_configs:
+   spectrogram_output:
+     _target_: auraloss.freq.MultiResolutionSTFTLoss
+     fft_sizes: [1024, 2048, 4096]
+     hop_sizes: [256, 512, 1024]
+     win_lengths: [1024, 2048, 4096]
+     spectrogram_mode: true  # Critical for spectrogram inputs!
+ ```
+
+ ### Performance Considerations
+
+ 1. **Synthesis validation**: More expensive due to audio synthesis
+ 2. **Batch size**: Currently limited to batch_size=1 for synthesis validation
+ 3. **Auraloss functions**: Computationally heavier than standard losses
+ 4. **Memory usage**: Multi-resolution losses use more memory
+
+ ### Code Organization
+
+ The synthesis utilities have been consolidated:
+
+ - `src/utils/synth_utils.py`: Shared synthesis components
+   - `SimpleSynth`: Sawtooth synthesizer
+   - `SpectrogramProcessor`: Audio to spectrogram conversion
+   - `STFT`, `MelFilter`: Spectrogram generation components
+ - `src/models/losses.py`: Loss implementations
+   - `AuralossWrapper`: Adapts auraloss for spectrograms
+   - `SynthesisValidationLoss`: Round-trip validation
+   - `create_loss_function`: Factory for config-based creation
+
+ ## Best Practices Summary
+
+ 1. **For synthesis parameters**: Use `SynthesisValidationLoss` with auraloss
+ 2. **For direct spectrograms**: Use `AuralossWrapper` with `spectrogram_mode=true`
+ 3. **For quantized parameters**: Use `OrdinalRegressionLoss` for distance awareness
+ 4. **Start simple**: Begin with MSE, then try auraloss functions
+ 5. **Monitor synthesis**: Check if synthesized audio matches expectations
+ 6. **Test thoroughly**: Run tests with `pytest tests/test_auraloss_integration.py`
+
  ## Future Enhancements

  - **Custom metrics**: Add distance-based accuracy metrics
  - **Adaptive weighting**: Learn optimal alpha during training
  - **Multi-scale loss**: Combine losses at different scales
  - **Curriculum learning**: Start with classification, gradually shift to regression
+ - **Batch synthesis**: Extend synthesis validation to handle larger batches
+ - **Inverse spectrogram**: Add support for Griffin-Lim reconstruction

! This comprehensive loss function framework provides multiple approaches for training on audio spectrograms, from simple distance-aware losses to sophisticated synthesis-based validation with perceptual audio losses.
Only in lightning-hydra-template-extended/docs: viz
Only in synthmatch-image-proc: generate_vimh.py
Only in synthmatch-image-proc: get_three.sh
Only in lightning-hydra-template-extended: lht-avix-diffs.txt
Only in lightning-hydra-template-extended: lht-sip-diffs.txt
Only in synthmatch-image-proc/logs: my_experiment_20250726_1549
Only in synthmatch-image-proc/logs: my_experiment_20250726_1550
Only in synthmatch-image-proc/logs: my_experiment_20250726_1625
Only in synthmatch-image-proc/logs: my_experiment_20250726_1649
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-34-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-43-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-43-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-49-11
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-49-48
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-51-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-53-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-54-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-55-52
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_20-00-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_20-01-02
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_20-06-18
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-00-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-01-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-02-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-02-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-03-52
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-04-56
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-12-09
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-13-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-14-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_16-32-28
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_02-34-12
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_02-34-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_13-43-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_13-46-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_14-15-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_14-16-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_14-18-49
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_20-23-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_20-23-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_20-23-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_20-29-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-37-49
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-37-51
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-37-53
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-37-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-44-31
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-44-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-46-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_22-31-48
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_22-37-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-13-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-23-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-48-31
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-48-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-49-09
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-49-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-49-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-49-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-52-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-03_00-55-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-03_02-15-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-03_02-17-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_18-13-44
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_19-14-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_19-15-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_20-16-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_20-26-31
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_20-27-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_21-57-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_21-59-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_22-25-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_22-25-38
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_18-50-20
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-00-38
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-01-42
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-02-02
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-27-46
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-28-02
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-29-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-32-50
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-37-52
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-41-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-28-18
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-35-59
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-36-19
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-36-51
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-37-12
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-47-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-53-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_21-09-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_21-12-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_21-15-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-05-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-10-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-23-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-25-22
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-30-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-46-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-59-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-31-18
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-31-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-31-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-31-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-36-55
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-52-52
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_00-05-25
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_00-37-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_00-37-37
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_00-50-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_01-03-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_01-16-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_02-09-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_02-11-35
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_02-45-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_03-02-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_03-31-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-02-20
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-03-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-04-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-04-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-06-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-49-13
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-49-48
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-07_22-56-48
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-25-16
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-25-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-26-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-28-40
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-28-56
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-30-09
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-30-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-35-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-45-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-45-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-50-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-52-10
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-57-29
Only in synthmatch-image-proc/logs/train/runs: 2025-07-09_23-53-10
Only in synthmatch-image-proc/logs/train/runs: 2025-07-09_23-53-48
Only in synthmatch-image-proc/logs/train/runs: 2025-07-10_00-37-02
Only in synthmatch-image-proc/logs/train/runs: 2025-07-10_02-02-32
Only in synthmatch-image-proc/logs/train/runs: 2025-07-10_02-17-10
Only in synthmatch-image-proc/logs/train/runs: 2025-07-10_02-17-29
Only in synthmatch-image-proc/logs/train/runs: 2025-07-10_02-27-21
Only in synthmatch-image-proc/logs/train/runs: 2025-07-10_02-38-04
Only in synthmatch-image-proc/logs/train/runs: 2025-07-10_02-40-40
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-10_03-48-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-10-44
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-11-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-15-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-18-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-26-38
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-35-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-41-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-43-40
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-48-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-49-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-55-25
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-56-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-56-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-58-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-13-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-14-28
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-21-41
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-21-59
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-22-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-22-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-23-21
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-25-21
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-27-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-28-49
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-29-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-29-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-29-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_21-48-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-11-12
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-22-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-24-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-43-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-56-27
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_00-43-33
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_00-51-20
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_00-51-42
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_00-52-06
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_00-53-42
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_00-54-20
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_00-59-39
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_00-59-53
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-00-12
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-00-15
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-00-35
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-02-28
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-05-54
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-06-21
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-06-38
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-06-51
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-09-33
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-15-01
Only in synthmatch-image-proc/logs/train/runs: 2025-07-13_01-16-25
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-19-55
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-20-08
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-20-22
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-20-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-20-51
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-23-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-30-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-31-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-37-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-38-19
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-38-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-38-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-39-02
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-39-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-41-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-44-35
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-47-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-51-13
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-52-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-53-49
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_01-03-11
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_01-05-16
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-31-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-31-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-31-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-32-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-32-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-32-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-33-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-57-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-57-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_02-22-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-13
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-41
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-56
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-06-11
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-08-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-11-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-14-56
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-18-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-19-35
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-21-00
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-27-19
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-27-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-28-10
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-28-37
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-28-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-32-09
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-34-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-47-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-55-08
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_04-29-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_04-30-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_05-48-41
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_05-48-59
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_05-54-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_06-00-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_09-53-28
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-13-07
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-13-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-13-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-14-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-14-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-18-13
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-26-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-29-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-29-37
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-31-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-33-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-35-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-36-07
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-36-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-36-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-36-41
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-37-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-37-18
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-41-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-42-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-42-53
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-48-53
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-49-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-49-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-52-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-46-35
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-47-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-49-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-52-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-52-46
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-54-45
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_03-17-10
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_03-24-50
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_03-26-44
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_03-28-38
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_03-29-31
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_03-41-50
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_03-42-57
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_04-02-10
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_04-38-22
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_04-38-49
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_04-44-07
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_04-45-56
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_04-47-00
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_04-51-27
Only in synthmatch-image-proc/logs/train/runs: 2025-07-16_05-07-35
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-09-09
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-09-43
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-15-07
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-15-16
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-15-24
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-15-31
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-16-00
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-17-06
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-18-43
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-19-13
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-19-26
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-19-34
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-20-52
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-22-37
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-22-45
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-22-54
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-23-01
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-54-39
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-54-46
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-56-07
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-56-16
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-56-25
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_00-56-32
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_01-04-19
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_01-05-47
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_01-07-16
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_01-08-07
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-16-31
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-17-19
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-17-48
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-19-12
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-19-28
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-19-42
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-20-55
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-23-24
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_09-29-40
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-20-49
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-21-20
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-21-43
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-27-58
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-28-15
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-36-27
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-37-10
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-37-43
Only in synthmatch-image-proc/logs/train/runs: 2025-07-18_10-44-20
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_13-29-23
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_13-46-27
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_14-00-28
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_14-07-02
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_15-37-35
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_15-43-28
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_15-43-39
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_15-43-51
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_15-44-04
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_15-44-22
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_15-46-19
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_15-48-15
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-27-08
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-27-16
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-43-50
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-44-01
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-44-13
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-44-24
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-44-37
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-46-34
Only in synthmatch-image-proc/logs/train/runs: 2025-07-26_16-48-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-45-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-46-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-46-22
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-46-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-46-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_19-26-42
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_19-33-20
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_19-42-07
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_22-56-44
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-24_23-12-00
Only in synthmatch-image-proc/logs: vimh_cnn_perceptual_spectral
Only in synthmatch-image-proc/logs: vimh_cnn_synthesis_multiscale
Only in lightning-hydra-template-extended/logs: vimh_training
Only in synthmatch-image-proc: outputs
Only in synthmatch-image-proc: paper
Only in synthmatch-image-proc: print_vimh.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/requirements.txt synthmatch-image-proc/requirements.txt
*** lightning-hydra-template-extended/requirements.txt	Wed Jul 16 04:59:15 2025
--- synthmatch-image-proc/requirements.txt	Sun Aug 24 12:16:16 2025
***************
*** 25,27 ****
--- 25,32 ----
  torchview       # model visualization
  torchviz        # computational graph visualization
  # sh            # for running bash commands in some tests (linux/macos only)
+ tensorboard>=2.10.0
+ matplotlib
+
+ # --------- audio processing --------- #
+ auraloss[all]>=0.4.0    # audio-focused loss functions
Only in synthmatch-image-proc: single_step_inference.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/data/cifar100mh_dataset.py synthmatch-image-proc/src/data/cifar100mh_dataset.py
*** lightning-hydra-template-extended/src/data/cifar100mh_dataset.py	Thu Aug 14 16:42:32 2025
--- synthmatch-image-proc/src/data/cifar100mh_dataset.py	Tue Aug 26 14:29:30 2025
***************
*** 155,162 ****
                      metadata[f'{param_name}_info'] = {
                          'value': param_value,
                          'description': mapping_info.get('description', ''),
!                         'range': [mapping_info.get('min'), mapping_info.get('max')],
!                         'scale': mapping_info.get('scale', 'linear')
                      }

          return metadata
--- 155,161 ----
                      metadata[f'{param_name}_info'] = {
                          'value': param_value,
                          'description': mapping_info.get('description', ''),
!                         'range': [mapping_info.get('min'), mapping_info.get('max')]
                      }

          return metadata
Only in synthmatch-image-proc/src/data: custom_cifar100_datamodule.py
Only in lightning-hydra-template-extended/src/data: dataset_wrapper.py
Only in lightning-hydra-template-extended/src/data: flexible_transforms.py
Only in synthmatch-image-proc/src/data: multihead_dataset_base.py.orig
Only in lightning-hydra-template-extended/src/data: obsolete
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/data/vimh_datamodule.py synthmatch-image-proc/src/data/vimh_datamodule.py
*** lightning-hydra-template-extended/src/data/vimh_datamodule.py	Tue Aug 26 15:14:49 2025
--- synthmatch-image-proc/src/data/vimh_datamodule.py	Tue Aug 26 14:29:30 2025
***************
*** 1,6 ****
! from typing import Any, Dict, Optional, Tuple, List
  import torch
  import json
  from pathlib import Path
  from lightning import LightningDataModule
  from torch.utils.data import DataLoader, Dataset
--- 1,7 ----
! from typing import Any, Dict, Optional, Tuple, List, Union
  import torch
  import json
+ import os
  from pathlib import Path
  from lightning import LightningDataModule
  from torch.utils.data import DataLoader, Dataset
***************
*** 62,89 ****
          self,
          data_dir: str = "data-vimh/",
          batch_size: int = 64,
!         num_workers: int = 0,
!         pin_memory: bool = False,
          persistent_workers: bool = True,
          train_transform: Optional[transforms.Compose] = None,
          val_transform: Optional[transforms.Compose] = None,
          test_transform: Optional[transforms.Compose] = None,
-         target_width: float = 0.0,
      ) -> None:
          """Initialize a `VIMHDataModule`.

          :param data_dir: The data directory containing VIMH files. Defaults to `"data-vimh/"`.
          :param batch_size: The batch size. Defaults to `64`.
!         :param num_workers: The number of workers. Defaults to `0`.
!         :param pin_memory: Whether to pin memory. Defaults to `False`.
          :param persistent_workers: Whether to use persistent workers. Defaults to `True`.
          :param train_transform: Optional transforms for training data.
          :param val_transform: Optional transforms for validation data.
          :param test_transform: Optional transforms for test data.
-         :param target_width: Standard deviation for soft targets (0.0 = hard targets).
          """
          super().__init__()

          # persistent_workers requires num_workers > 0
          self.persistent_workers = persistent_workers and num_workers > 0

--- 63,92 ----
          self,
          data_dir: str = "data-vimh/",
          batch_size: int = 64,
!         num_workers: Union[int, str] = "auto",
!         pin_memory: Union[bool, str] = "auto",
          persistent_workers: bool = True,
          train_transform: Optional[transforms.Compose] = None,
          val_transform: Optional[transforms.Compose] = None,
          test_transform: Optional[transforms.Compose] = None,
      ) -> None:
          """Initialize a `VIMHDataModule`.

          :param data_dir: The data directory containing VIMH files. Defaults to `"data-vimh/"`.
          :param batch_size: The batch size. Defaults to `64`.
!         :param num_workers: The number of workers. Defaults to `"auto"` (platform-aware).
!         :param pin_memory: Whether to pin memory. Defaults to `"auto"` (platform-aware).
          :param persistent_workers: Whether to use persistent workers. Defaults to `True`.
          :param train_transform: Optional transforms for training data.
          :param val_transform: Optional transforms for validation data.
          :param test_transform: Optional transforms for test data.
          """
          super().__init__()

+         # Auto-detect platform-specific settings
+         num_workers = self._auto_detect_num_workers(num_workers)
+         pin_memory = self._auto_detect_pin_memory(pin_memory)
+
          # persistent_workers requires num_workers > 0
          self.persistent_workers = persistent_workers and num_workers > 0

***************
*** 131,136 ****
--- 134,171 ----
          self.parameter_bounds: Dict[str, Tuple[float, float]] = {}  # Parameter (min, max) for regression
          self.image_shape: Tuple[int, int, int] = (3, 32, 32)  # Default, will be updated

+     def _auto_detect_num_workers(self, num_workers: Union[int, str]) -> int:
+         """Auto-detect optimal number of workers based on platform and device."""
+         if isinstance(num_workers, int):
+             return num_workers
+
+         if num_workers == "auto":
+             # Check if MPS is available and likely to be used
+             if torch.backends.mps.is_available() and torch.backends.mps.is_built():
+                 # MPS doesn't support multiprocessing well
+                 return 0
+             else:
+                 # For CPU and CUDA, use multiple workers
+                 return min(4, os.cpu_count() or 1)
+
+         raise ValueError(f"Invalid num_workers value: {num_workers}. Must be int or 'auto'")
+
+     def _auto_detect_pin_memory(self, pin_memory: Union[bool, str]) -> bool:
+         """Auto-detect pin_memory setting based on platform and device."""
+         if isinstance(pin_memory, bool):
+             return pin_memory
+
+         if pin_memory == "auto":
+             # Check if MPS is available and likely to be used
+             if torch.backends.mps.is_available() and torch.backends.mps.is_built():
+                 # MPS doesn't support pinned memory
+                 return False
+             else:
+                 # For CPU and CUDA, pinned memory can be beneficial
+                 return True
+
+         raise ValueError(f"Invalid pin_memory value: {pin_memory}. Must be bool or 'auto'")
+
      def _adjust_transforms_for_image_size(self, height: int, width: int) -> None:
          """Adjust transforms based on actual image dimensions."""
          # Update the default train transforms with proper padding/cropping
***************
*** 514,528 ****
                  self.data_train = VIMHDataset(
                      self.hparams.data_dir,
                      train=True,
!                     transform=self.train_transform,
!                     target_width=self.hparams.target_width
                  )

                  self.data_test = VIMHDataset(
                      self.hparams.data_dir,
                      train=False,
!                     transform=self.test_transform,
!                     target_width=self.hparams.target_width
                  )

                  # For validation, we'll use the test dataset with val transforms
--- 549,561 ----
                  self.data_train = VIMHDataset(
                      self.hparams.data_dir,
                      train=True,
!                     transform=self.train_transform
                  )

                  self.data_test = VIMHDataset(
                      self.hparams.data_dir,
                      train=False,
!                     transform=self.test_transform
                  )

                  # For validation, we'll use the test dataset with val transforms
***************
*** 530,537 ****
                  self.data_val = VIMHDataset(
                      self.hparams.data_dir,
                      train=False,
!                     transform=self.val_transform,
!                     target_width=self.hparams.target_width
                  )

              except Exception as e:
--- 563,569 ----
                  self.data_val = VIMHDataset(
                      self.hparams.data_dir,
                      train=False,
!                     transform=self.val_transform
                  )

              except Exception as e:
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/data/vimh_dataset.py synthmatch-image-proc/src/data/vimh_dataset.py
*** lightning-hydra-template-extended/src/data/vimh_dataset.py	Tue Aug 26 15:13:52 2025
--- synthmatch-image-proc/src/data/vimh_dataset.py	Tue Aug 26 14:29:30 2025
***************
*** 23,30 ****
          data_path: str,
          train: bool = True,
          transform: Optional[callable] = None,
!         target_transform: Optional[callable] = None,
!         target_width: float = 0.0
      ):
          """Initialize VIMH dataset.

--- 23,29 ----
          data_path: str,
          train: bool = True,
          transform: Optional[callable] = None,
!         target_transform: Optional[callable] = None
      ):
          """Initialize VIMH dataset.

***************
*** 32,50 ****
          :param train: Whether to load training or test data
          :param transform: Optional transform to apply to images
          :param target_transform: Optional transform to apply to labels
-         :param target_width: Standard deviation for soft targets (0.0 = hard targets)
          """
          self.train = train
          self.transform = transform
          self.target_transform = target_transform
-         self.target_width = target_width

          # Determine the correct file path
          data_path = Path(data_path)
          if data_path.is_dir():
              # Load from directory structure
              self.data_dir = data_path
!             self.batch_file = self.data_dir / ('train_batch' if train else 'test_batch')
              self.metadata_file = self.data_dir / 'vimh_dataset_info.json'
          else:
              # Single file specified
--- 31,63 ----
          :param train: Whether to load training or test data
          :param transform: Optional transform to apply to images
          :param target_transform: Optional transform to apply to labels
          """
          self.train = train
          self.transform = transform
          self.target_transform = target_transform

          # Determine the correct file path
          data_path = Path(data_path)
          if data_path.is_dir():
              # Load from directory structure
              self.data_dir = data_path
!
!             # Check for both pickle and binary formats
!             if train:
!                 pickle_file = self.data_dir / 'train_batch'
!                 binary_file = self.data_dir / 'train'
!             else:
!                 pickle_file = self.data_dir / 'test_batch'
!                 binary_file = self.data_dir / 'test'
!
!             # Prefer pickle format if available, otherwise use binary
!             if pickle_file.exists():
!                 self.batch_file = pickle_file
!             elif binary_file.exists():
!                 self.batch_file = binary_file
!             else:
!                 raise FileNotFoundError(f"Neither {pickle_file} nor {binary_file} found")
!
              self.metadata_file = self.data_dir / 'vimh_dataset_info.json'
          else:
              # Single file specified
***************
*** 132,154 ****
              if actual_samples != expected_samples:
                  print(f"Warning: Expected {expected_samples} test samples, got {actual_samples}")

-     def _create_soft_targets(self, class_index: int, num_classes: int, target_width: float) -> torch.Tensor:
-         """Create soft targets as Gaussian distribution around true class.
-
-         :param class_index: True class index
-         :param num_classes: Total number of classes
-         :param target_width: Standard deviation for soft targets
-         :return: Soft target distribution or hard target index
-         """
-         if target_width == 0.0:
-             return class_index  # Hard targets (backward compatible)
-
-         # Soft targets - Gaussian distribution
-         class_indices = torch.arange(num_classes, dtype=torch.float32)
-         distances = (class_indices - class_index) ** 2
-         weights = torch.exp(-distances / (2 * target_width ** 2))
-         return weights / weights.sum()  # Normalize to probability distribution
-
      def _get_sample_metadata(self, idx: int) -> Dict[str, Any]:
          """Get metadata for a specific sample.

--- 145,150 ----
***************
*** 185,192 ****
                          'quantized_value': param_value,
                          'actual_value': actual_value,
                          'description': mapping_info.get('description', ''),
!                         'range': [param_min, param_max],
!                         'scale': mapping_info.get('scale', 'linear')
                      }

          return metadata
--- 181,187 ----
                          'quantized_value': param_value,
                          'actual_value': actual_value,
                          'description': mapping_info.get('description', ''),
!                         'range': [param_min, param_max]
                      }

          return metadata
***************
*** 199,212 ****
          """
          image, labels = super().__getitem__(idx)

-         # Apply soft targets if enabled
-         if self.target_width > 0.0:
-             soft_labels = {}
-             for param_name, quantized_value in labels.items():
-                 num_classes = self.heads_config.get(param_name, 256)  # Default to 256 classes
-                 soft_labels[param_name] = self._create_soft_targets(quantized_value, num_classes, self.target_width)
-             labels = soft_labels
-
          # Apply transforms if specified
          if self.transform is not None:
              image = self.transform(image)
--- 194,199 ----
***************
*** 291,321 ****
  def create_vimh_datasets(
      data_dir: str,
      transform: Optional[callable] = None,
!     target_transform: Optional[callable] = None,
!     target_width: float = 0.0
  ) -> Tuple[VIMHDataset, VIMHDataset]:
      """Create train and test VIMH datasets.

      :param data_dir: Directory containing the dataset files
      :param transform: Optional transform to apply to images
      :param target_transform: Optional transform to apply to labels
-     :param target_width: Standard deviation for soft targets (0.0 = hard targets)
      :return: Tuple of (train_dataset, test_dataset)
      """
      train_dataset = VIMHDataset(
          data_dir,
          train=True,
          transform=transform,
!         target_transform=target_transform,
!         target_width=target_width
      )

      test_dataset = VIMHDataset(
          data_dir,
          train=False,
          transform=transform,
!         target_transform=target_transform,
!         target_width=target_width
      )

      return train_dataset, test_dataset
--- 278,304 ----
  def create_vimh_datasets(
      data_dir: str,
      transform: Optional[callable] = None,
!     target_transform: Optional[callable] = None
  ) -> Tuple[VIMHDataset, VIMHDataset]:
      """Create train and test VIMH datasets.

      :param data_dir: Directory containing the dataset files
      :param transform: Optional transform to apply to images
      :param target_transform: Optional transform to apply to labels
      :return: Tuple of (train_dataset, test_dataset)
      """
      train_dataset = VIMHDataset(
          data_dir,
          train=True,
          transform=transform,
!         target_transform=target_transform
      )

      test_dataset = VIMHDataset(
          data_dir,
          train=False,
          transform=transform,
!         target_transform=target_transform
      )

      return train_dataset, test_dataset
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/components/convnext_v2.py synthmatch-image-proc/src/models/components/convnext_v2.py
*** lightning-hydra-template-extended/src/models/components/convnext_v2.py	Thu Aug 14 16:42:32 2025
--- synthmatch-image-proc/src/models/components/convnext_v2.py	Tue Aug 26 14:29:30 2025
***************
*** 457,463 ****

  def convnext_v2_cifar100_10m(input_size: int = 32, in_chans: int = 3, output_size: int = 100, **kwargs):
      """ConvNeXt-V2 optimized for CIFAR-100 (~10M parameters)
-
      Scaled architecture for improved CIFAR-100 performance:
      - Optimized depth/width balance (4, 6, 18, 6) layers
      - Increased channel dimensions (42, 84, 168, 336)
--- 457,462 ----
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/components/simple_cnn.py synthmatch-image-proc/src/models/components/simple_cnn.py
*** lightning-hydra-template-extended/src/models/components/simple_cnn.py	Tue Aug 26 15:16:52 2025
--- synthmatch-image-proc/src/models/components/simple_cnn.py	Tue Aug 26 14:29:30 2025
***************
*** 18,25 ****
          output_mode: str = "classification",
          parameter_names: Optional[List[str]] = None,
          parameter_ranges: Optional[Dict[str, Tuple[float, float]]] = None,
-         auxiliary_input_size: int = 0,
-         auxiliary_hidden_size: int = 32,
      ) -> None:
          """Initialize a SimpleCNN module.

--- 18,23 ----
***************
*** 34,41 ****
          :param output_mode: Output mode - "classification" or "regression".
          :param parameter_names: List of parameter names for regression mode.
          :param parameter_ranges: Dict mapping parameter names to (min, max) ranges.
-         :param auxiliary_input_size: Size of auxiliary scalar input vector (0 = no aux input).
-         :param auxiliary_hidden_size: Hidden size for auxiliary input processing.
          """
          super().__init__()

--- 32,37 ----
***************
*** 43,51 ****
          self.output_mode = output_mode
          self.parameter_names = parameter_names or []
          self.parameter_ranges = parameter_ranges or {}
-         self.auxiliary_input_size = auxiliary_input_size
-         self.auxiliary_hidden_size = auxiliary_hidden_size
-         self.fc_hidden = fc_hidden

          # Handle configuration based on output mode
          if output_mode == "regression":
--- 39,44 ----
***************
*** 104,131 ****
              nn.Dropout(dropout),
          )

-         # Auxiliary input processing (if enabled)
-         if auxiliary_input_size > 0:
-             self.auxiliary_net = nn.Sequential(
-                 nn.Linear(auxiliary_input_size, auxiliary_hidden_size),
-                 nn.ReLU(),
-                 nn.Dropout(dropout / 2),  # Less dropout for auxiliary features
-                 nn.Linear(auxiliary_hidden_size, auxiliary_hidden_size),
-                 nn.ReLU(),
-             )
-             # Combined feature size includes auxiliary features
-             combined_feature_size = fc_hidden + auxiliary_hidden_size
-         else:
-             self.auxiliary_net = None
-             combined_feature_size = fc_hidden
-
          # Multiple heads or single head for backward compatibility
          if self.is_multihead:
              if output_mode == "regression":
                  # For regression, create heads with sigmoid activation
                  self.heads = nn.ModuleDict({
                      head_name: nn.Sequential(
!                         nn.Linear(combined_feature_size, 1),
                          nn.Sigmoid()
                      )
                      for head_name in heads_config.keys()
--- 97,109 ----
              nn.Dropout(dropout),
          )

          # Multiple heads or single head for backward compatibility
          if self.is_multihead:
              if output_mode == "regression":
                  # For regression, create heads with sigmoid activation
                  self.heads = nn.ModuleDict({
                      head_name: nn.Sequential(
!                         nn.Linear(fc_hidden, 1),
                          nn.Sigmoid()
                      )
                      for head_name in heads_config.keys()
***************
*** 133,139 ****
              else:
                  # Classification heads
                  self.heads = nn.ModuleDict({
!                     head_name: nn.Linear(combined_feature_size, num_classes)
                      for head_name, num_classes in heads_config.items()
                  })
          else:
--- 111,117 ----
              else:
                  # Classification heads
                  self.heads = nn.ModuleDict({
!                     head_name: nn.Linear(fc_hidden, num_classes)
                      for head_name, num_classes in heads_config.items()
                  })
          else:
***************
*** 141,201 ****
              head_name, num_classes = next(iter(heads_config.items()))
              if output_mode == "regression":
                  self.classifier = nn.Sequential(
!                     nn.Linear(combined_feature_size, 1),
                      nn.Sigmoid()
                  )
              else:
!                 self.classifier = nn.Linear(combined_feature_size, num_classes)

!     def forward(self, x: torch.Tensor, auxiliary: Optional[torch.Tensor] = None):
          """Perform a single forward pass through the network.

          :param x: Input tensor of shape (batch_size, channels, height, width).
-         :param auxiliary: Optional auxiliary input tensor of shape (batch_size, auxiliary_input_size).
          :return: A tensor of logits (single head) or dict of logits (multihead).
          """
-         # Process main input through CNN
          x = self.conv_layers(x)
          shared_features = self.shared_features(x)

-         # Combine with auxiliary features if provided
-         if self.auxiliary_net is not None and auxiliary is not None:
-             auxiliary_features = self.auxiliary_net(auxiliary)
-             combined_features = torch.cat([shared_features, auxiliary_features], dim=1)
-         else:
-             combined_features = shared_features
-
          if self.is_multihead:
              return {
!                 head_name: head(combined_features)
                  for head_name, head in self.heads.items()
              }
          else:
              # Single head output (backward compatibility)
!             return self.classifier(combined_features)
!
!     def _build_heads(self, heads_config: Dict[str, int]) -> None:
!         """Rebuild heads for auto-configuration (regression mode only)."""
!         if self.output_mode != "regression":
!             raise ValueError("_build_heads only supports regression mode")
!
!         # Calculate combined feature size (same as in __init__)
!         if self.auxiliary_input_size > 0:
!             combined_feature_size = self.fc_hidden + self.auxiliary_hidden_size
!         else:
!             combined_feature_size = self.fc_hidden
!
!         # Create regression heads with sigmoid activation
!         self.heads = nn.ModuleDict({
!             head_name: nn.Sequential(
!                 nn.Linear(combined_feature_size, 1),
!                 nn.Sigmoid()
!             )
!             for head_name in heads_config.keys()
!         })
!
!         self.heads_config = heads_config
!         self.is_multihead = len(heads_config) > 1


  if __name__ == "__main__":
--- 119,147 ----
              head_name, num_classes = next(iter(heads_config.items()))
              if output_mode == "regression":
                  self.classifier = nn.Sequential(
!                     nn.Linear(fc_hidden, 1),
                      nn.Sigmoid()
                  )
              else:
!                 self.classifier = nn.Linear(fc_hidden, num_classes)

!     def forward(self, x: torch.Tensor):
          """Perform a single forward pass through the network.

          :param x: Input tensor of shape (batch_size, channels, height, width).
          :return: A tensor of logits (single head) or dict of logits (multihead).
          """
          x = self.conv_layers(x)
          shared_features = self.shared_features(x)

          if self.is_multihead:
              return {
!                 head_name: head(shared_features)
                  for head_name, head in self.heads.items()
              }
          else:
              # Single head output (backward compatibility)
!             return self.classifier(shared_features)


  if __name__ == "__main__":
Only in lightning-hydra-template-extended/src/models/components: simple_mlp.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/losses.py synthmatch-image-proc/src/models/losses.py
*** lightning-hydra-template-extended/src/models/losses.py	Fri Aug 15 20:35:24 2025
--- synthmatch-image-proc/src/models/losses.py	Tue Aug 26 14:29:30 2025
***************
*** 3,11 ****
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
! from typing import Optional, Tuple


  class OrdinalRegressionLoss(nn.Module):
      """
      Ordinal regression loss for quantized continuous parameters in perceptual units.
--- 3,121 ----
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
! from typing import Optional, Tuple, Dict, Any, List

+ try:
+     import auraloss
+     AURALOSS_AVAILABLE = True
+ except ImportError:
+     AURALOSS_AVAILABLE = False

+
+ # ==============================================================================
+ # PNP-Adapted Loss Functions
+ # Adapted from /l/pnp/src/pnp_synth/neural/loss.py for proven multi-scale spectral analysis
+ # ==============================================================================
+
+ class DistanceLoss(nn.Module):
+     """Base class for distance-based losses, adapted from PNP codebase."""
+     def __init__(self, p: float = 2.0):
+         super().__init__()
+         self.p = p
+         self.ops = []  # Will be populated by subclasses
+
+     def dist(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
+         """Compute distance between tensors."""
+         if self.p == 1.0:
+             return torch.abs(x - y).mean()
+         elif self.p == 2.0:
+             return torch.norm(x - y, p=self.p)
+         else:
+             return torch.norm(x - y, p=self.p)
+
+     def forward(self, x: torch.Tensor, y: torch.Tensor, transform_y: bool = True) -> torch.Tensor:
+         """Forward pass computing multi-scale distance."""
+         loss = torch.tensor(0.0, device=x.device, dtype=x.dtype)
+         for op in self.ops:
+             loss += self.dist(op(x), op(y) if transform_y else y)
+         loss /= len(self.ops) if self.ops else 1
+         return loss
+
+
+ class MagnitudeSTFT(nn.Module):
+     """STFT magnitude computation module, adapted from PNP."""
+     def __init__(self, n_fft: int, hop_length: int):
+         super().__init__()
+         self.n_fft = n_fft
+         self.hop_length = hop_length
+
+     def forward(self, x: torch.Tensor) -> torch.Tensor:
+         """Compute STFT magnitude."""
+         # Ensure we have proper window device placement
+         window = torch.hann_window(self.n_fft, device=x.device, dtype=x.dtype)
+
+         return torch.stft(
+             x,
+             n_fft=self.n_fft,
+             hop_length=self.hop_length,
+             window=window,
+             return_complex=True,
+         ).abs()
+
+
+ class MultiScaleSpectralLoss(DistanceLoss):
+     """
+     Multi-resolution STFT loss from PNP codebase, adapted for our synthesis matching project.
+
+     This proven implementation provides superior spectral analysis compared to basic auraloss
+     by using multiple STFT resolutions simultaneously. It has been validated in published
+     research (ICASSP, TASLP papers) and provides robust multi-scale spectral comparison.
+
+     Args:
+         max_n_fft: Maximum STFT window size (power of 2)
+         num_scales: Number of different STFT scales to use
+         hop_lengths: Optional list of hop lengths (auto-computed if None)
+         mag_w: Weight for magnitude loss (currently unused but kept for compatibility)
+         logmag_w: Weight for log-magnitude loss (currently unused but kept for compatibility)
+         p: Norm to use for distance computation (1.0 for L1, 2.0 for L2)
+     """
+
+     def __init__(
+         self,
+         max_n_fft: int = 2048,
+         num_scales: int = 6,
+         hop_lengths: Optional[List[int]] = None,
+         mag_w: float = 1.0,
+         logmag_w: float = 0.0,
+         p: float = 1.0,
+     ):
+         super().__init__(p=p)
+
+         # Ensure we can create all scales
+         assert max_n_fft // (2 ** (num_scales - 1)) > 1, \
+             f"max_n_fft={max_n_fft} too small for num_scales={num_scales}"
+
+         # Create STFT window sizes at multiple scales
+         self.max_n_fft = max_n_fft
+         self.n_ffts = [max_n_fft // (2**i) for i in range(num_scales)]
+         self.hop_lengths = (
+             [n // 4 for n in self.n_ffts] if hop_lengths is None else hop_lengths
+         )
+
+         # Weights for different components (kept for future extensibility)
+         self.mag_w = mag_w
+         self.logmag_w = logmag_w
+
+         self.create_ops()
+
+     def create_ops(self):
+         """Create STFT operators for each scale."""
+         self.ops = [
+             MagnitudeSTFT(n_fft, self.hop_lengths[i])
+             for i, n_fft in enumerate(self.n_ffts)
+         ]
+
+
  class OrdinalRegressionLoss(nn.Module):
      """
      Ordinal regression loss for quantized continuous parameters in perceptual units.
***************
*** 304,306 ****
--- 414,901 ----
          total_loss = cross_entropy_loss + distance_penalty

          return total_loss.mean()
+
+
+ class AuralossWrapper(nn.Module):
+     """
+     Wrapper for auraloss functions to provide consistent interface with other losses.
+
+     Since auraloss functions expect audio waveforms but we're working with spectrograms,
+     this wrapper can handle the conversion or adapt the loss computation.
+
+     Args:
+         loss_name: Name of the auraloss function to use
+         loss_config: Configuration dictionary for the loss function
+         spectrogram_mode: Whether to use loss directly on spectrograms (True) or
+                          attempt to convert to audio (False)
+     """
+
+     def __init__(
+         self,
+         loss_name: str,
+         loss_config: Optional[Dict[str, Any]] = None,
+         spectrogram_mode: bool = True,
+     ):
+         super().__init__()
+
+         if not AURALOSS_AVAILABLE:
+             raise ImportError(
+                 "auraloss is not installed. Install with: pip install auraloss[all]"
+             )
+
+         self.loss_name = loss_name
+         self.loss_config = loss_config or {}
+         self.spectrogram_mode = spectrogram_mode
+
+         # Create the auraloss function
+         self.auraloss_fn = self._create_auraloss_function(loss_name, self.loss_config)
+
+     def _create_auraloss_function(self, loss_name: str, config: Dict[str, Any]):
+         """Create the appropriate auraloss function."""
+         # Frequency domain losses
+         if loss_name == "stft":
+             return auraloss.freq.STFTLoss(**config)
+         elif loss_name == "mrstft":
+             return auraloss.freq.MultiResolutionSTFTLoss(**config)
+         elif loss_name == "melstft":
+             return auraloss.freq.MelSTFTLoss(**config)
+         elif loss_name == "randomstft":
+             return auraloss.freq.RandomResolutionSTFTLoss(**config)
+         elif loss_name == "sumdiff_stft":
+             return auraloss.freq.SumAndDifferenceSTFTLoss(**config)
+
+         # Time domain losses
+         elif loss_name == "esr":
+             return auraloss.time.ESRLoss(**config)
+         elif loss_name == "dc":
+             return auraloss.time.DCLoss(**config)
+         elif loss_name == "logcosh":
+             return auraloss.time.LogCoshLoss(**config)
+         elif loss_name == "snr":
+             return auraloss.time.SNRLoss(**config)
+         elif loss_name == "sisdr":
+             return auraloss.time.SISDRLoss(**config)
+         elif loss_name == "sdsdr":
+             return auraloss.time.SDSDRLoss(**config)
+
+         else:
+             raise ValueError(f"Unknown auraloss function: {loss_name}")
+
+     def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
+         """
+         Forward pass through the auraloss function.
+
+         Args:
+             input: Model predictions (batch_size, channels, height, width) for spectrograms
+             target: Target spectrograms (batch_size, channels, height, width)
+
+         Returns:
+             Loss value
+         """
+         if self.spectrogram_mode:
+             # For spectrogram-based losses, we need to adapt the input format
+             # Most auraloss functions expect audio waveforms, but some frequency-domain
+             # losses might work with spectrograms directly
+
+             if self.loss_name in ["stft", "mrstft", "melstft", "randomstft", "sumdiff_stft"]:
+                 # These are frequency-domain losses that might work with spectrograms
+                 # We'll need to convert spectrogram images back to audio-like tensors
+
+                 # Reshape from (batch, channels, height, width) to (batch, channels, -1)
+                 batch_size, channels, height, width = input.shape
+
+                 # Create a longer sequence to avoid padding issues
+                 # Minimum sequence length should be larger than the window size
+                 min_seq_len = max(4096, height * width)  # Ensure sufficient length
+
+                 # Flatten and pad if necessary
+                 input_flat = input.view(batch_size, channels, height * width)
+                 target_flat = target.view(batch_size, channels, height * width)
+
+                 # Pad to minimum sequence length if needed
+                 if input_flat.size(-1) < min_seq_len:
+                     pad_len = min_seq_len - input_flat.size(-1)
+                     input_flat = torch.nn.functional.pad(input_flat, (0, pad_len), mode='constant', value=0)
+                     target_flat = torch.nn.functional.pad(target_flat, (0, pad_len), mode='constant', value=0)
+
+                 # Convert to audio-like format (batch, channels, seq_len)
+                 return self.auraloss_fn(input_flat, target_flat)
+             else:
+                 # Time-domain losses need audio waveforms
+                 # This is a simplified conversion - in practice, you'd want to use
+                 # proper inverse STFT or other spectrogram-to-audio conversion
+
+                 # For now, treat spectrograms as flattened audio
+                 batch_size, channels, height, width = input.shape
+                 input_flat = input.view(batch_size, channels, height * width)
+                 target_flat = target.view(batch_size, channels, height * width)
+
+                 # Ensure minimum length for time-domain losses
+                 min_seq_len = max(2048, height * width)
+                 if input_flat.size(-1) < min_seq_len:
+                     pad_len = min_seq_len - input_flat.size(-1)
+                     input_flat = torch.nn.functional.pad(input_flat, (0, pad_len), mode='constant', value=0)
+                     target_flat = torch.nn.functional.pad(target_flat, (0, pad_len), mode='constant', value=0)
+
+                 return self.auraloss_fn(input_flat, target_flat)
+         else:
+             # Direct audio mode - input and target should be audio waveforms
+             return self.auraloss_fn(input, target)
+
+
+ def create_loss_function(loss_config: Dict[str, Any]) -> nn.Module:
+     """
+     Factory function to create loss functions based on configuration.
+
+     Args:
+         loss_config: Configuration dictionary with '_target_' key and parameters
+
+     Returns:
+         Configured loss function
+     """
+     if '_target_' not in loss_config:
+         raise ValueError("Loss configuration must contain '_target_' key")
+
+     target = loss_config['_target_']
+     params = {k: v for k, v in loss_config.items() if k != '_target_'}
+
+     # Handle auraloss functions
+     if target.startswith('auraloss.'):
+         # Extract loss name from target path
+         loss_name = target.split('.')[-1].lower()
+
+         # Map common auraloss targets to our wrapper
+         auraloss_mapping = {
+             'stftloss': 'stft',
+             'multiresolutionstftloss': 'mrstft',
+             'melstftloss': 'melstft',
+             'randomresolutionstftloss': 'randomstft',
+             'sumanddifferencestftloss': 'sumdiff_stft',
+             'esrloss': 'esr',
+             'dcloss': 'dc',
+             'logcoshloss': 'logcosh',
+             'snrloss': 'snr',
+             'sisdrloss': 'sisdr',
+             'sdsdrloss': 'sdsdr',
+         }
+
+         if loss_name in auraloss_mapping:
+             # Extract spectrogram_mode from params before passing to AuralossWrapper
+             spectrogram_mode = params.pop('spectrogram_mode', True)
+             return AuralossWrapper(
+                 loss_name=auraloss_mapping[loss_name],
+                 loss_config=params,
+                 spectrogram_mode=spectrogram_mode
+             )
+
+     # Handle standard PyTorch losses
+     if target == 'torch.nn.CrossEntropyLoss':
+         return nn.CrossEntropyLoss(**params)
+     elif target == 'torch.nn.MSELoss':
+         return nn.MSELoss(**params)
+     elif target == 'torch.nn.L1Loss':
+         return nn.L1Loss(**params)
+     elif target == 'torch.nn.HuberLoss':
+         return nn.HuberLoss(**params)
+
+     # Handle custom losses
+     elif target == 'src.models.losses.OrdinalRegressionLoss':
+         return OrdinalRegressionLoss(**params)
+     elif target == 'src.models.losses.QuantizedRegressionLoss':
+         return QuantizedRegressionLoss(**params)
+     elif target == 'src.models.losses.WeightedCrossEntropyLoss':
+         return WeightedCrossEntropyLoss(**params)
+     elif target == 'src.models.losses.NormalizedRegressionLoss':
+         return NormalizedRegressionLoss(**params)
+     elif target == 'src.models.losses.AuralossWrapper':
+         return AuralossWrapper(**params)
+     elif target == 'src.models.losses.SynthesisValidationLoss':
+         return SynthesisValidationLoss(**params)
+     elif target == 'src.models.losses.MultiScaleSpectralLoss':
+         return MultiScaleSpectralLoss(**params)
+
+     else:
+         raise ValueError(f"Unknown loss function target: {target}")
+
+
+ class SynthesisValidationLoss(nn.Module):
+     """
+     Synthesis-based validation loss that implements proper round-trip validation.
+
+     This loss function:
+     1. Takes predicted synthesis parameters
+     2. Synthesizes audio using SimpleSynth
+     3. Converts synthesized audio to spectrogram
+     4. Compares with original target spectrogram using specified loss function
+
+     This approach tests whether the estimated parameters can actually reproduce
+     the original audio, providing more meaningful validation than direct parameter comparison.
+
+     Args:
+         param_ranges: Dictionary mapping parameter names to (min, max) tuples
+         param_bounds: Dictionary mapping parameter names to (min, max) tuples (for denormalization)
+         sample_rate: Audio sample rate
+         duration: Audio duration in seconds
+         spectrogram_config: Configuration for spectrogram generation
+         loss_function: Loss function to use for spectrogram comparison
+         device: Device to run computations on
+     """
+
+     def __init__(
+         self,
+         param_ranges: Dict[str, Tuple[float, float]],
+         param_bounds: Dict[str, Tuple[float, float]],
+         sample_rate: int = 8000,
+         duration: float = 1.0,
+         spectrogram_config: Optional[Dict[str, Any]] = None,
+         loss_function: str = "mse",
+         device: str = "cpu"
+     ):
+         super().__init__()
+
+         self.param_ranges = param_ranges
+         self.param_bounds = param_bounds
+         self.sample_rate = sample_rate
+         self.duration = duration
+         self.device = device
+
+         # Import synthesis components from shared utilities
+         try:
+             from ..utils.synth_utils import SimpleSynth, SpectrogramProcessor
+             self.SimpleSynth = SimpleSynth
+             self.SpectrogramProcessor = SpectrogramProcessor
+         except ImportError as e:
+             raise ImportError(f"Could not import synthesis components: {e}")
+
+         # Create synthesizer
+         self.synthesizer = self.SimpleSynth(sample_rate=sample_rate)
+
+         # Default spectrogram configuration
+         if spectrogram_config is None:
+             spectrogram_config = {
+                 'stft': {
+                     'type': 'mel',
+                     'n_fft': 512,
+                     'n_window': 128,
+                     'hop_length': 64,
+                     'window_type': 'rectangular'
+                 },
+                 'mel': {
+                     'freq_min': 40.0,
+                     'freq_max_ratio': 0.9
+                 },
+                 'height': 32,
+                 'width': 32
+             }
+
+         self.spectrogram_config = spectrogram_config
+
+         # Create spectrogram processor
+         self.spectrogram_processor = self.SpectrogramProcessor(
+             sample_rate=sample_rate,
+             height=spectrogram_config['height'],
+             width=spectrogram_config['width'],
+             stft_config=spectrogram_config['stft'],
+             mel_config=spectrogram_config['mel']
+         )
+
+         # Create loss function for spectrogram comparison
+         if loss_function == "mse":
+             self.spectrogram_loss = nn.MSELoss()
+         elif loss_function == "l1":
+             self.spectrogram_loss = nn.L1Loss()
+         elif loss_function == "huber":
+             self.spectrogram_loss = nn.HuberLoss()
+         elif loss_function == "multiscale_spectral":
+             # Use PNP's proven MultiScaleSpectralLoss - optimized for 8kHz audio
+             self.spectrogram_loss = MultiScaleSpectralLoss(
+                 max_n_fft=1024,  # Smaller for 8kHz audio (1 second = 8000 samples)
+                 num_scales=4,    # 4 scales: 1024, 512, 256, 128
+                 p=1.0           # L1 norm (robust to outliers)
+             )
+         else:
+             # Try to use auraloss if available
+             try:
+                 if loss_function.startswith("auraloss_"):
+                     auraloss_name = loss_function.replace("auraloss_", "")
+                     auraloss_config = {
+                         '_target_': f'auraloss.freq.{auraloss_name}',
+                         'spectrogram_mode': True
+                     }
+                     self.spectrogram_loss = create_loss_function(auraloss_config)
+                 else:
+                     raise ValueError(f"Unknown loss function: {loss_function}")
+             except:
+                 raise ValueError(f"Unknown loss function: {loss_function}")
+
+     def denormalize_parameters(self, normalized_params: torch.Tensor, param_names: List[str]) -> Dict[str, float]:
+         """
+         Convert normalized parameters [0,1] to actual parameter values.
+
+         Args:
+             normalized_params: Tensor of shape (batch_size, num_params) with values in [0,1]
+             param_names: List of parameter names in order
+
+         Returns:
+             Dictionary mapping parameter names to actual values
+         """
+         batch_size = normalized_params.shape[0]
+         if batch_size != 1:
+             raise ValueError(f"Only batch_size=1 supported for synthesis validation, got {batch_size}")
+
+         params_dict = {'duration': self.duration}
+         normalized_params = normalized_params.squeeze(0)  # Remove batch dimension
+
+         for i, param_name in enumerate(param_names):
+             if param_name in self.param_bounds:
+                 min_val, max_val = self.param_bounds[param_name]
+                 normalized_val = normalized_params[i].item()
+                 actual_val = min_val + normalized_val * (max_val - min_val)
+                 params_dict[param_name] = actual_val
+             else:
+                 # Use raw normalized value if no bounds specified
+                 params_dict[param_name] = normalized_params[i].item()
+
+         return params_dict
+
+     def synthesize_and_convert_to_spectrogram(self, params_dict: Dict[str, float]) -> torch.Tensor:
+         """
+         Synthesize audio from parameters and convert to spectrogram.
+
+         Args:
+             params_dict: Dictionary of synthesis parameters
+
+         Returns:
+             Spectrogram tensor of shape (height, width)
+         """
+         # Generate audio
+         audio = self.synthesizer.generate_audio(params_dict)
+
+         # Convert to spectrogram
+         spectrogram, spec_min, spec_max = self.spectrogram_processor.audio_to_spectrogram(params_dict, audio)
+
+         # Convert to tensor and normalize to [0,1] range
+         spectrogram_tensor = torch.from_numpy(spectrogram).float() / 255.0
+
+         return spectrogram_tensor
+
+     def forward(self, predictions: torch.Tensor, targets: torch.Tensor, param_names: List[str]) -> torch.Tensor:
+         """
+         Forward pass of synthesis validation loss.
+
+         Args:
+             predictions: Predicted parameters, shape (batch_size, num_params), values in [0,1]
+             targets: Target spectrogram, shape (batch_size, height, width) or (batch_size, height, width, channels)
+             param_names: List of parameter names corresponding to prediction order
+
+         Returns:
+             Loss value
+         """
+         batch_size = predictions.shape[0]
+
+         # Currently only support batch_size=1 for synthesis validation
+         # Could be extended to handle batches in the future
+         if batch_size != 1:
+             raise ValueError(f"Only batch_size=1 supported for synthesis validation, got {batch_size}")
+
+         # Denormalize parameters
+         params_dict = self.denormalize_parameters(predictions, param_names)
+
+         # Handle MultiScaleSpectralLoss differently (works with audio, not spectrograms)
+         if isinstance(self.spectrogram_loss, MultiScaleSpectralLoss):
+             try:
+                 # Generate synthesized audio
+                 synthesized_audio = self.synthesizer.generate_audio(params_dict)
+                 synthesized_audio_tensor = torch.from_numpy(synthesized_audio).float().to(predictions.device)
+
+                 # We need target audio, but we only have target spectrogram
+                 # This is a limitation - MultiScaleSpectralLoss needs audio comparison
+                 # For now, we'll use a placeholder approach and reconstruct audio from spectrogram
+                 # In practice, this should be enhanced with proper audio targets
+
+                 # For now, create a simple sine wave as target based on fundamental frequency
+                 # This is a simplified approach - in reality we'd need proper audio reconstruction
+                 sample_rate = self.sample_rate
+                 duration = self.duration
+                 t = torch.linspace(0, duration, int(sample_rate * duration), device=predictions.device)
+
+                 # Extract fundamental frequency from parameters (simplified)
+                 # This is a placeholder - should be improved based on actual synthesis parameters
+                 if 'note_number' in params_dict:
+                     # MIDI note to frequency conversion
+                     midi_note = params_dict['note_number']
+                     freq = 440.0 * (2.0 ** ((midi_note - 69.0) / 12.0))
+                 else:
+                     freq = 440.0  # Default A4
+
+                 # Use the synthesized audio as both prediction and target for self-consistency check
+                 # This is a better approach than creating artificial target audio
+                 # The loss will be between synthesized audio and a reference sine wave with the same parameters
+                 target_audio = torch.sin(2 * torch.pi * freq * t).to(predictions.device)
+
+                 # Apply ADSR envelope if velocity info available
+                 if 'note_velocity' in params_dict:
+                     velocity_factor = float(params_dict['note_velocity']) / 127.0
+                     target_audio = target_audio * velocity_factor
+
+                 # Apply decay if available
+                 if 'log10_decay_time' in params_dict:
+                     decay_time = 10 ** float(params_dict['log10_decay_time'])
+                     decay_envelope = torch.exp(-t / decay_time)
+                     target_audio = target_audio * decay_envelope
+
+                 # Add batch dimension for MultiScaleSpectralLoss
+                 synthesized_batch = synthesized_audio_tensor.unsqueeze(0)  # (1, samples)
+                 target_batch = target_audio.unsqueeze(0)  # (1, samples)
+
+                 # Ensure same length
+                 min_length = min(synthesized_batch.shape[1], target_batch.shape[1])
+                 synthesized_batch = synthesized_batch[:, :min_length]
+                 target_batch = target_batch[:, :min_length]
+
+                 loss = self.spectrogram_loss(synthesized_batch, target_batch)
+                 return loss
+
+             except Exception as e:
+                 # If synthesis fails, return a large penalty loss
+                 return torch.tensor(1000.0, device=predictions.device, requires_grad=True)
+
+         else:
+             # Standard spectrogram-based comparison (existing code)
+             try:
+                 synthesized_spectrogram = self.synthesize_and_convert_to_spectrogram(params_dict)
+             except Exception as e:
+                 # If synthesis fails, return a large penalty loss
+                 return torch.tensor(1000.0, device=predictions.device, requires_grad=True)
+
+             # Ensure target spectrogram is in correct format
+             target_spectrogram = targets.squeeze(0)  # Remove batch dimension
+             if len(target_spectrogram.shape) == 3:
+                 # If multi-channel, take first channel
+                 target_spectrogram = target_spectrogram[:, :, 0]
+
+             # Normalize target to [0,1] range if needed
+             if target_spectrogram.max() > 1.0:
+                 target_spectrogram = target_spectrogram / 255.0
+
+             # Ensure both spectrograms have the same shape
+             if synthesized_spectrogram.shape != target_spectrogram.shape:
+                 # Resize synthesized spectrogram to match target
+                 synthesized_spectrogram = torch.nn.functional.interpolate(
+                     synthesized_spectrogram.unsqueeze(0).unsqueeze(0),
+                     size=target_spectrogram.shape,
+                     mode='bilinear',
+                     align_corners=False
+                 ).squeeze(0).squeeze(0)
+
+             # Compute loss between spectrograms
+             if isinstance(self.spectrogram_loss, AuralossWrapper):
+                 # For auraloss, we need to add batch and channel dimensions
+                 synthesized_4d = synthesized_spectrogram.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)
+                 target_4d = target_spectrogram.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)
+                 loss = self.spectrogram_loss(synthesized_4d, target_4d)
+             else:
+                 # For standard losses, use 2D tensors
+                 loss = self.spectrogram_loss(synthesized_spectrogram, target_spectrogram)
+
+             return loss
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/multihead_module.py synthmatch-image-proc/src/models/multihead_module.py
*** lightning-hydra-template-extended/src/models/multihead_module.py	Fri Aug 15 20:35:24 2025
--- synthmatch-image-proc/src/models/multihead_module.py	Tue Aug 26 14:29:30 2025
***************
*** 5,11 ****
  from torchmetrics import MaxMetric, MeanMetric
  from torchmetrics.classification.accuracy import Accuracy
  from ..data.multihead_dataset_base import MultiheadDatasetBase
! from .losses import OrdinalRegressionLoss, QuantizedRegressionLoss, WeightedCrossEntropyLoss, NormalizedRegressionLoss


  class MultiheadLitModule(LightningModule):
--- 5,11 ----
  from torchmetrics import MaxMetric, MeanMetric
  from torchmetrics.classification.accuracy import Accuracy
  from ..data.multihead_dataset_base import MultiheadDatasetBase
! from .losses import OrdinalRegressionLoss, QuantizedRegressionLoss, WeightedCrossEntropyLoss, NormalizedRegressionLoss, AuralossWrapper, SynthesisValidationLoss, create_loss_function


  class MultiheadLitModule(LightningModule):
***************
*** 59,64 ****
--- 59,65 ----
          compile: bool = False,
          auto_configure_from_dataset: bool = True,
          output_mode: str = "classification",
+         loss_configs: Optional[Dict[str, Dict[str, Any]]] = None,
      ) -> None:
          """Initialize a `MultiheadLitModule`.

***************
*** 71,76 ****
--- 72,78 ----
          :param compile: Whether to compile the model.
          :param auto_configure_from_dataset: Whether to auto-configure heads from dataset.
          :param output_mode: Output mode - "classification" or "regression".
+         :param loss_configs: Optional dict of loss configurations for creating losses from config.
          """
          super().__init__()

***************
*** 79,84 ****
--- 81,87 ----
          self._initial_criteria = criteria
          self._initial_criterion = criterion
          self._initial_loss_weights = loss_weights
+         self._initial_loss_configs = loss_configs
          self.output_mode = output_mode

          # Backward compatibility handling
***************
*** 86,93 ****
              criteria = {'head_0': criterion}
          elif criteria is None:
              # Will be configured later in setup() if auto_configure_from_dataset is True
!             if not auto_configure_from_dataset:
!                 raise ValueError("Must provide either 'criterion' or 'criteria' or set auto_configure_from_dataset=True")
              criteria = {}

          # If auto_configure_from_dataset is True but we have a network with heads_config,
--- 89,97 ----
              criteria = {'head_0': criterion}
          elif criteria is None:
              # Will be configured later in setup() if auto_configure_from_dataset is True
!             # OR if we have loss_configs to create them from
!             if not auto_configure_from_dataset and not loss_configs:
!                 raise ValueError("Must provide either 'criterion' or 'criteria' or set auto_configure_from_dataset=True or provide loss_configs")
              criteria = {}

          # If auto_configure_from_dataset is True but we have a network with heads_config,
***************
*** 159,166 ****
--- 163,183 ----
          # Initialize criteria if not already set
          if not self.criteria:
              self.criteria = {}
+
+             # Check if we have loss configs to use
+             if self._initial_loss_configs:
                  for head_name in head_configs.keys():
+                     if head_name in self._initial_loss_configs:
+                         # Create loss from config
+                         loss_config = self._initial_loss_configs[head_name]
+                         self.criteria[head_name] = create_loss_function(loss_config)
+                     else:
+                         # Default to CrossEntropyLoss if no config provided
                          self.criteria[head_name] = torch.nn.CrossEntropyLoss()
+             else:
+                 # No loss configs provided, use defaults
+                 for head_name in head_configs.keys():
+                     self.criteria[head_name] = torch.nn.CrossEntropyLoss()

          # Initialize loss weights if not already set
          if not self.loss_weights:
***************
*** 253,258 ****
--- 270,279 ----
              NormalizedRegressionLoss,
          )
          return isinstance(criterion, regression_losses)
+
+     def _is_auraloss(self, criterion) -> bool:
+         """Check if a loss function is an auraloss wrapper."""
+         return isinstance(criterion, AuralossWrapper)

      def _compute_predictions(self, logits: torch.Tensor, criterion, head_name: str) -> torch.Tensor:
          """Compute predictions based on loss function type."""
Only in synthmatch-image-proc/src/models: soft_target_loss.py
Only in synthmatch-image-proc/src: synth_simple.py
Only in lightning-hydra-template-extended/src/utils: custom_progress_bar.py
Only in lightning-hydra-template-extended/src/utils: gradient_stats_callback.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/utils/synth_utils.py synthmatch-image-proc/src/utils/synth_utils.py
*** lightning-hydra-template-extended/src/utils/synth_utils.py	Thu Aug 14 18:50:17 2025
--- synthmatch-image-proc/src/utils/synth_utils.py	Tue Aug 26 14:29:30 2025
***************
*** 5,16 ****
  and the SynthesisValidationLoss to avoid code duplication.
  """

- from typing import Any, Dict, List, Optional, Tuple
-
  import numpy as np
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  # Constants from generate_vimh.py
  DEFAULT_NOTE_NUMBER = 69.0
--- 5,15 ----
  and the SynthesisValidationLoss to avoid code duplication.
  """

  import numpy as np
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
+ from typing import Dict, Any, Tuple, Optional, List

  # Constants from generate_vimh.py
  DEFAULT_NOTE_NUMBER = 69.0
***************
*** 24,73 ****
  class STFT(nn.Module):
      """STFT module from generate_vimh.py"""

!     def __init__(
!         self,
!         fftsize: int,
!         winsize: int,
!         hopsize: int,
!         complex: bool = False,
!         window_type: str = "rectangular",
!     ):
!         super().__init__()
          self.fftsize = fftsize
          self.winsize = winsize
          self.hopsize = hopsize
          self.window_type = window_type

          # Create window based on type
!         if window_type == "rectangular":
              window = torch.ones(winsize)
!         elif window_type == "hann":
              window = torch.hann_window(winsize, periodic=False)
!         elif window_type == "hamming":
              window = torch.hamming_window(winsize, periodic=False)
!         elif window_type == "blackman":
              window = torch.blackman_window(winsize, periodic=False)
          else:
!             raise ValueError(
!                 f"Unsupported window type: {window_type}. Supported: 'rectangular', 'hann', 'hamming', 'blackman'"
!             )

!         self.register_buffer("window", window, persistent=False)
          self.complex = complex

      def compute_stft_kernel(self):
          # use CPU STFT of dirac impulses to derive conv1d weights if we're using that b/c torch.stft failed
          diracs = torch.eye(self.winsize)
!         w = torch.stft(
!             diracs,
!             n_fft=self.fftsize,
!             hop_length=self.hopsize,
!             win_length=self.winsize,
!             window=self.window.to(diracs),
!             center=False,
!             normalized=True,
!             return_complex=True,
!         )
          w = torch.view_as_real(w)
          # squash real/complex, transpose to (1, winsize+2, winsize)
          w = w.flatten(1).T[:, np.newaxis]
--- 23,56 ----
  class STFT(nn.Module):
      """STFT module from generate_vimh.py"""

!     def __init__(self, fftsize: int, winsize: int, hopsize: int, complex: bool = False, window_type: str = 'rectangular'):
!         super(STFT, self).__init__()
          self.fftsize = fftsize
          self.winsize = winsize
          self.hopsize = hopsize
          self.window_type = window_type

          # Create window based on type
!         if window_type == 'rectangular':
              window = torch.ones(winsize)
!         elif window_type == 'hann':
              window = torch.hann_window(winsize, periodic=False)
!         elif window_type == 'hamming':
              window = torch.hamming_window(winsize, periodic=False)
!         elif window_type == 'blackman':
              window = torch.blackman_window(winsize, periodic=False)
          else:
!             raise ValueError(f"Unsupported window type: {window_type}. Supported: 'rectangular', 'hann', 'hamming', 'blackman'")

!         self.register_buffer('window', window, persistent=False)
          self.complex = complex

      def compute_stft_kernel(self):
          # use CPU STFT of dirac impulses to derive conv1d weights if we're using that b/c torch.stft failed
          diracs = torch.eye(self.winsize)
!         w = torch.stft(diracs, n_fft=self.fftsize, hop_length=self.hopsize, win_length=self.winsize,
!                        window=self.window.to(diracs), center=False, normalized=True,
!                        return_complex=True)
          w = torch.view_as_real(w)
          # squash real/complex, transpose to (1, winsize+2, winsize)
          w = w.flatten(1).T[:, np.newaxis]
***************
*** 79,109 ****
          batchsize, channels = x.shape[:2]
          x = x.reshape((-1,) + x.shape[2:])
          # we apply the STFT
!         if not hasattr(self, "stft_kernel"):
              try:
!                 x = torch.stft(
!                     x,
!                     n_fft=self.fftsize,
!                     hop_length=self.hopsize,
!                     win_length=self.winsize,
!                     window=self.window,
!                     center=False,
!                     normalized=True,
!                     return_complex=True,
!                 )
                  x = torch.view_as_real(x)
              except RuntimeError as exc:
!                 if len(exc.args) > 0 and (
!                     ("doesn't support" in exc.args[0]) or ("only supports" in exc.args[0])
!                 ):
                      # half precision STFT not supported everywhere, improvise!
                      # compute equivalent conv1d weights and register as buffer
!                     self.register_buffer(
!                         "stft_kernel", self.compute_stft_kernel().to(x), persistent=False
!                     )
                  else:
                      raise
!         if hasattr(self, "stft_kernel"):
              # we use the conv1d replacement if we found that stft() fails
              x = F.conv1d(x[:, None], self.stft_kernel, stride=self.hopsize)
              # split real/complex and move to the end
--- 62,83 ----
          batchsize, channels = x.shape[:2]
          x = x.reshape((-1,) + x.shape[2:])
          # we apply the STFT
!         if not hasattr(self, 'stft_kernel'):
              try:
!                 x = torch.stft(x, n_fft=self.fftsize, hop_length=self.hopsize, win_length=self.winsize,
!                                window=self.window, center=False, normalized=True, return_complex=True)
                  x = torch.view_as_real(x)
              except RuntimeError as exc:
!                 if len(exc.args) > 0 and (("doesn't support" in exc.args[0]) or
!                                           ("only supports" in exc.args[0])):
                      # half precision STFT not supported everywhere, improvise!
                      # compute equivalent conv1d weights and register as buffer
!                     self.register_buffer('stft_kernel',
!                                          self.compute_stft_kernel().to(x),
!                                          persistent=False)
                  else:
                      raise
!         if hasattr(self, 'stft_kernel'):
              # we use the conv1d replacement if we found that stft() fails
              x = F.conv1d(x[:, None], self.stft_kernel, stride=self.hopsize)
              # split real/complex and move to the end
***************
*** 116,130 ****
          return x


! def create_mel_filterbank(
!     sample_rate: float,
!     frame_len: int,
!     num_bands: int,
!     min_freq: float,
!     max_freq: float,
!     norm: bool = True,
!     crop: bool = False,
! ) -> torch.Tensor:
      """
      Creates a mel filterbank of `num_bands` triangular filters, with the first
      filter starting at `min_freq` and the last one stopping at `max_freq`.
--- 90,98 ----
          return x


! def create_mel_filterbank(sample_rate: float, frame_len: int, num_bands: int,
!                           min_freq: float, max_freq: float, norm: bool = True,
!                           crop: bool = False) -> torch.Tensor:
      """
      Creates a mel filterbank of `num_bands` triangular filters, with the first
      filter starting at `min_freq` and the last one stopping at `max_freq`.
***************
*** 139,145 ****
      # create filterbank
      input_bins = (frame_len // 2) + 1
      if crop:
!         input_bins = min(input_bins, int(np.ceil(max_freq * frame_len / float(sample_rate))))
      x = torch.arange(input_bins, dtype=peaks_bin.dtype)[:, np.newaxis]
      l, c, r = peaks_bin[0:-2], peaks_bin[1:-1], peaks_bin[2:]
      # triangles are the minimum of two linear functions f(x) = a*x + b
--- 107,115 ----
      # create filterbank
      input_bins = (frame_len // 2) + 1
      if crop:
!         input_bins = min(input_bins,
!                          int(np.ceil(max_freq * frame_len /
!                                      float(sample_rate))))
      x = torch.arange(input_bins, dtype=peaks_bin.dtype)[:, np.newaxis]
      l, c, r = peaks_bin[0:-2], peaks_bin[1:-1], peaks_bin[2:]
      # triangles are the minimum of two linear functions f(x) = a*x + b
***************
*** 164,182 ****
      Transform a spectrogram created with the given `sample_rate` and `winsize`
      into a mel spectrogram of `num_bands` from `min_freq` to `max_freq`.
      """

-     def __init__(
-         self, sample_rate: float, winsize: int, num_bands: int, min_freq: float, max_freq: float
-     ):
-         super().__init__()
-         melbank = create_mel_filterbank(
-             sample_rate, winsize, num_bands, min_freq, max_freq, crop=True
-         )
-         self.register_buffer("bank", melbank, persistent=False)
-
      def forward(self, x):
          x = x.transpose(-1, -2)  # put fft bands last
!         x = x[..., : self.bank.shape[0]]  # remove unneeded fft bands
          x = x.matmul(self.bank)  # turn fft bands into mel bands
          x = x.transpose(-1, -2)  # put time last
          return x
--- 134,149 ----
      Transform a spectrogram created with the given `sample_rate` and `winsize`
      into a mel spectrogram of `num_bands` from `min_freq` to `max_freq`.
      """
+     def __init__(self, sample_rate: float, winsize: int, num_bands: int,
+                  min_freq: float, max_freq: float):
+         super(MelFilter, self).__init__()
+         melbank = create_mel_filterbank(sample_rate, winsize, num_bands,
+                                         min_freq, max_freq, crop=True)
+         self.register_buffer('bank', melbank, persistent=False)

      def forward(self, x):
          x = x.transpose(-1, -2)  # put fft bands last
!         x = x[..., :self.bank.shape[0]]  # remove unneeded fft bands
          x = x.matmul(self.bank)  # turn fft bands into mel bands
          x = x.transpose(-1, -2)  # put time last
          return x
***************
*** 185,198 ****
  class SpectrogramProcessor:
      """Handles efficient spectrogram generation with reusable modules."""

!     def __init__(
!         self,
          sample_rate: int,
          height: int,
          width: int,
          stft_config: Dict[str, Any],
!         mel_config: Dict[str, Any],
!     ):
          self.sample_rate = sample_rate
          self.height = height
          self.width = width
--- 152,163 ----
  class SpectrogramProcessor:
      """Handles efficient spectrogram generation with reusable modules."""

!     def __init__(self,
                   sample_rate: int,
                   height: int,
                   width: int,
                   stft_config: Dict[str, Any],
!                  mel_config: Dict[str, Any]):
          self.sample_rate = sample_rate
          self.height = height
          self.width = width
***************
*** 200,242 ****
          self.mel_config = mel_config

          # Extract spectrogram configuration
!         self.spectrogram_type = stft_config["type"]

!         self.n_fft = stft_config.get("n_fft", 512)
!         self.n_window = stft_config.get("n_window", 128)
!         self.hop_length = stft_config.get("hop_length", 64)
!         self.window_type = stft_config.get("window_type", "rectangular")
!         self.bins_per_harmonic = stft_config.get("bins_per_harmonic", 1.0)

!         self.freq_min = mel_config.get("freq_min", 40.0)
!         self.freq_max_ratio = mel_config.get("freq_max_ratio", 0.9)

          # Validate configuration
!         if self.spectrogram_type not in ["mel", "stft"]:
!             raise ValueError(
!                 f"Unknown spectrogram type: {self.spectrogram_type}. Must be 'mel' or 'stft'."
!             )

          # Create reusable modules
          self.stft_module = None
          self.mel_filter_module: Optional[MelFilter] = None

          # Create default module
!         self.stft_module = STFT(
!             fftsize=self.n_fft,
!             winsize=self.n_window,
!             hopsize=self.hop_length,
!             window_type=self.window_type,
!         )

!         if self.spectrogram_type == "mel":
              freq_max = self.freq_max_ratio * sample_rate / 2
              self.mel_filter_module = MelFilter(
                  sample_rate=sample_rate,
                  winsize=self.n_fft,
                  num_bands=height,
                  min_freq=self.freq_min,
!                 max_freq=freq_max,
              )

      def _prepare_audio_tensor(self, audio: np.ndarray) -> torch.Tensor:
--- 165,200 ----
          self.mel_config = mel_config

          # Extract spectrogram configuration
!         self.spectrogram_type = stft_config['type']

!         self.n_fft = stft_config.get('n_fft', 512)
!         self.n_window = stft_config.get('n_window', 128)
!         self.hop_length = stft_config.get('hop_length', 64)
!         self.window_type = stft_config.get('window_type', 'rectangular')
!         self.bins_per_harmonic = stft_config.get('bins_per_harmonic', 1.0)

!         self.freq_min = mel_config.get('freq_min', 40.0)
!         self.freq_max_ratio = mel_config.get('freq_max_ratio', 0.9)

          # Validate configuration
!         if self.spectrogram_type not in ['mel', 'stft']:
!             raise ValueError(f"Unknown spectrogram type: {self.spectrogram_type}. Must be 'mel' or 'stft'.")

          # Create reusable modules
          self.stft_module = None
          self.mel_filter_module: Optional[MelFilter] = None

          # Create default module
!         self.stft_module = STFT(fftsize=self.n_fft, winsize=self.n_window, hopsize=self.hop_length, window_type=self.window_type)

!         if self.spectrogram_type == 'mel':
              freq_max = self.freq_max_ratio * sample_rate / 2
              self.mel_filter_module = MelFilter(
                  sample_rate=sample_rate,
                  winsize=self.n_fft,
                  num_bands=height,
                  min_freq=self.freq_min,
!                 max_freq=freq_max
              )

      def _prepare_audio_tensor(self, audio: np.ndarray) -> torch.Tensor:
***************
*** 267,273 ****
              # In-place normalization to avoid memory copies
              spec_normalized = spec_np.copy()
              spec_normalized -= spec_min
!             spec_normalized /= spec_max - spec_min
              # Apply smooth scaling with rounding to reduce stepping artifacts
              spec_normalized = np.round(spec_normalized * QUANTIZATION_LEVELS)
              spec_normalized = np.clip(spec_normalized, 0, QUANTIZATION_LEVELS)
--- 225,231 ----
              # In-place normalization to avoid memory copies
              spec_normalized = spec_np.copy()
              spec_normalized -= spec_min
!             spec_normalized /= (spec_max - spec_min)
              # Apply smooth scaling with rounding to reduce stepping artifacts
              spec_normalized = np.round(spec_normalized * QUANTIZATION_LEVELS)
              spec_normalized = np.clip(spec_normalized, 0, QUANTIZATION_LEVELS)
***************
*** 275,283 ****
          else:
              return np.zeros_like(spec_np, dtype=np.uint8), spec_min, spec_max

!     def audio_to_spectrogram(
!         self, params: Dict[str, float], audio: np.ndarray
!     ) -> Tuple[np.ndarray, float, float]:
          """Process audio into spectrogram based on configured type.

          Returns:
--- 233,239 ----
          else:
              return np.zeros_like(spec_np, dtype=np.uint8), spec_min, spec_max

!     def audio_to_spectrogram(self, params: Dict[str, float], audio: np.ndarray) -> Tuple[np.ndarray, float, float]:
          """Process audio into spectrogram based on configured type.

          Returns:
***************
*** 291,302 ****
              with torch.no_grad():
                  # Compute STFT first (required for both 'stft' and 'mel' types)
                  # STFT transforms audio from time domain to frequency domain
!                 spec = self.stft_module(
!                     audio_tensor
!                 )  # (batch, channels, freq [0:n_fft/2+1], time)

                  # Apply mel filtering if needed (converts linear frequency bins to mel-scale)
!                 if self.spectrogram_type == "mel" and self.mel_filter_module is not None:
                      spec = self.mel_filter_module(spec)

                  # Convert to dB
--- 247,256 ----
              with torch.no_grad():
                  # Compute STFT first (required for both 'stft' and 'mel' types)
                  # STFT transforms audio from time domain to frequency domain
!                 spec = self.stft_module(audio_tensor) # (batch, channels, freq [0:n_fft/2+1], time)

                  # Apply mel filtering if needed (converts linear frequency bins to mel-scale)
!                 if self.spectrogram_type == 'mel' and self.mel_filter_module is not None:
                      spec = self.mel_filter_module(spec)

                  # Convert to dB
***************
*** 308,314 ****
              # Slice frequency bins and time frames based on spectrogram type
              max_time_frame = min(self.width, spec_db.shape[1])

!             if self.spectrogram_type == "mel":
                  # For mel spectrograms, keep all frequency bins (no DC bin to skip)
                  max_freq_bin = min(self.height, spec_db.shape[0])
                  spec_db = spec_db[:max_freq_bin, :max_time_frame]
--- 262,268 ----
              # Slice frequency bins and time frames based on spectrogram type
              max_time_frame = min(self.width, spec_db.shape[1])

!             if self.spectrogram_type == 'mel':
                  # For mel spectrograms, keep all frequency bins (no DC bin to skip)
                  max_freq_bin = min(self.height, spec_db.shape[0])
                  spec_db = spec_db[:max_freq_bin, :max_time_frame]
***************
*** 326,332 ****
  def check_params(params: Dict[str, float], *required_params: str) -> None:
      """Check if required parameters exist in params dict and log warnings for missing ones."""
      import logging
-
      logger = logging.getLogger(__name__)

      for param_name in required_params:
--- 280,285 ----
***************
*** 344,361 ****

      def generate_audio(self, params: Dict[str, float]) -> np.ndarray:
          """Generate audio with given parameters."""
!         check_params(params, "note_number", "note_velocity", "duration", "log10_decay_time")

          try:
!             note_number = params.get("note_number", DEFAULT_NOTE_NUMBER)
!             note_velocity = params.get("note_velocity", DEFAULT_NOTE_VELOCITY)
!             duration = params.get("duration", DEFAULT_DURATION)

!             if "log10_decay_time" in params:
!                 log10_decay_time = params["log10_decay_time"]
!                 decay_time = 10.0**log10_decay_time
              else:
!                 decay_time = 10.0**DEFAULT_LOG10_DECAY_TIME

              # Validate parameters
              if duration <= 0:
--- 297,314 ----

      def generate_audio(self, params: Dict[str, float]) -> np.ndarray:
          """Generate audio with given parameters."""
!         check_params(params, 'note_number', 'note_velocity', 'duration', 'log10_decay_time')

          try:
!             note_number = params.get('note_number', DEFAULT_NOTE_NUMBER)
!             note_velocity = params.get('note_velocity', DEFAULT_NOTE_VELOCITY)
!             duration = params.get('duration', DEFAULT_DURATION)

!             if 'log10_decay_time' in params:
!                 log10_decay_time = params['log10_decay_time']
!                 decay_time = 10.0 ** log10_decay_time
              else:
!                 decay_time = 10.0 ** DEFAULT_LOG10_DECAY_TIME

              # Validate parameters
              if duration <= 0:
Only in synthmatch-image-proc/src/utils: vimh_utils.py.orig
Only in synthmatch-image-proc/tests: test_auraloss_integration.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/tests/test_configs.py synthmatch-image-proc/tests/test_configs.py
*** lightning-hydra-template-extended/tests/test_configs.py	Wed Jul 16 03:10:35 2025
--- synthmatch-image-proc/tests/test_configs.py	Tue Aug 26 14:29:30 2025
***************
*** 54,60 ****

          # Test that net has the required parameters for regression mode
          assert cfg.model.net.parameter_names == []  # Will be auto-configured
-
          # Test instantiation without HydraConfig.set_config requires parameter_names
          # Since parameter_names is empty, instantiation will fail, so we skip this test
          # The auto-configuration happens during setup() in the Lightning module
--- 54,59 ----
Only in synthmatch-image-proc/tests: test_multihead_loss_configs.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/tests/test_vimh_datasets.py synthmatch-image-proc/tests/test_vimh_datasets.py
*** lightning-hydra-template-extended/tests/test_vimh_datasets.py	Tue Jul 15 02:28:35 2025
--- synthmatch-image-proc/tests/test_vimh_datasets.py	Tue Aug 26 14:29:30 2025
***************
*** 62,73 ****
          'varying_parameters': 2,
          'parameter_names': ['note_number', 'note_velocity'],
          'label_encoding': {
!             'format': '[height] [width] [channels] [N] [param1_id] [param1_val] [param2_id] [param2_val] ...',
!             'metadata_bytes': 6,
              'N_range': [0, 255],
              'param_id_range': [0, 255],
              'param_val_range': [0, 255]
          },
          'parameter_mappings': {
              'note_number': {
                  'min': 50.0,
--- 62,99 ----
          'varying_parameters': 2,
          'parameter_names': ['note_number', 'note_velocity'],
          'label_encoding': {
!             'format': '[height] [width] [channels] [spec_min] [spec_max] [N] [param1_id] [param1_val] [param2_id] [param2_val] ...',
!             'metadata_bytes': 14,  # 6 bytes for dimensions + 8 bytes for scale factors
!             'scale_factors': {
!                 'spec_min': 'float32 - minimum dB value before normalization',
!                 'spec_max': 'float32 - maximum dB value before normalization'
!             },
              'N_range': [0, 255],
              'param_id_range': [0, 255],
              'param_val_range': [0, 255]
          },
+         'fixed_parameters': {
+             'fixed_note': {
+                 'value': 43.350,
+                 'description': 'Fixed note number for testing'
+             }
+         },
+         'pre_emphasis_coefficient': 0.0,
+         'spectrogram_config': {
+             'sample_rate': 8000,
+             'type': 'stft',
+             'n_fft': 512,
+             'n_window': 512,
+             'hop_length': 64,
+             'window_type': 'rectangular',
+             'bins_per_harmonic': 1.0,
+             'n_bins': 32,
+             'method': 'efficient_leaf'
+         },
+         'mel_config': {
+             'freq_min': 40.0,
+             'freq_max_ratio': 0.9
+         },
          'parameter_mappings': {
              'note_number': {
                  'min': 50.0,
***************
*** 615,620 ****
--- 641,696 ----
          assert 'note_velocity' in heads_config
          assert heads_config['note_number'] == 256
          assert heads_config['note_velocity'] == 256
+
+     def test_new_metadata_fields(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test that new metadata fields are properly loaded."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         dataset = VIMHDataset(str(temp_dir), train=True)
+
+         # Test that metadata is loaded with new fields
+         metadata_file = temp_dir / 'vimh_dataset_info.json'
+         assert metadata_file.exists()
+
+         import json
+         with open(metadata_file, 'r') as f:
+             metadata = json.load(f)
+
+         # Test new fields exist
+         assert 'fixed_parameters' in metadata
+         assert 'pre_emphasis_coefficient' in metadata
+         assert 'spectrogram_config' in metadata
+         assert 'mel_config' in metadata
+
+         # Test fixed parameters
+         fixed_params = metadata['fixed_parameters']
+         assert 'fixed_note' in fixed_params
+         assert fixed_params['fixed_note']['value'] == 43.350
+         assert 'description' in fixed_params['fixed_note']
+
+         # Test pre_emphasis_coefficient
+         assert metadata['pre_emphasis_coefficient'] == 0.0
+
+         # Test spectrogram_config
+         spec_config = metadata['spectrogram_config']
+         assert spec_config['type'] == 'stft'
+         assert spec_config['n_fft'] == 512
+         assert spec_config['n_window'] == 512
+         assert spec_config['hop_length'] == 64
+         assert spec_config['window_type'] == 'rectangular'
+         assert spec_config['bins_per_harmonic'] == 1.0
+
+         # Test mel_config
+         mel_config = metadata['mel_config']
+         assert mel_config['freq_min'] == 40.0
+         assert mel_config['freq_max_ratio'] == 0.9
+
+         # Test updated label encoding
+         label_encoding = metadata['label_encoding']
+         assert label_encoding['metadata_bytes'] == 14  # Updated from 6 to 14
+         assert 'scale_factors' in label_encoding
+         assert 'spec_min' in label_encoding['scale_factors']
+         assert 'spec_max' in label_encoding['scale_factors']


  class TestVIMHIntegration:
Only in synthmatch-image-proc/tests: test_vimh_format.py
