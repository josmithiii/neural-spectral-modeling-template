Only in lightning-hydra-template-extended: .envrc
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/.vscode/launch.json avix/.vscode/launch.json
*** lightning-hydra-template-extended/.vscode/launch.json	Fri Aug 15 14:00:35 2025
--- avix/.vscode/launch.json	Thu Aug 21 09:29:20 2025
***************
*** 2,13 ****
      "version": "0.2.0",
      "configurations": [
          {
              "name": "avix-cnn",
              "type": "python",
              "request": "launch",
              "program": "${workspaceFolder}/src/train.py",
              "args": [
!                 "experiment=avix_cnn_16kdss",
                  "trainer=mps"
              ],
              "console": "integratedTerminal",
--- 2,83 ----
      "version": "0.2.0",
      "configurations": [
          {
+             "name": "direct_gradient_normalized",
+             "type": "python",
+             "request": "launch",
+             "program": "${workspaceFolder}/src/train.py",
+             "args": [
+                 "experiment=avix_densenet_72dss_64k_jnd_dg_normalized",
+                 "trainer=mps"
+             ],
+             "console": "integratedTerminal",
+             "cwd": "${workspaceFolder}",
+             "env": {},
+             "justMyCode": false
+         },
+         {
+             "name": "direct_gradient_training_1m",
+             "type": "python",
+             "request": "launch",
+             "program": "${workspaceFolder}/src/train.py",
+             "args": [
+                 "experiment=direct_gradient_training",
+                 "trainer=mps"
+             ],
+             "console": "integratedTerminal",
+             "cwd": "${workspaceFolder}",
+             "env": {},
+             "justMyCode": false
+         },
+         {
+             "name": "avix-sdn-64k",
+             "type": "python",
+             "request": "launch",
+             "program": "${workspaceFolder}/src/train.py",
+             "args": [
+                 "experiment=avix_densenet_72dss_64k_jnd",
+                 "trainer=mps"
+             ],
+             "console": "integratedTerminal",
+             "cwd": "${workspaceFolder}",
+             "env": {},
+             "justMyCode": false
+         },
+         {
+             "name": "avix-mlp-64k",
+             "type": "python",
+             "request": "launch",
+             "program": "${workspaceFolder}/src/train.py",
+             "args": [
+                 "experiment=avix_mlp_72dss_64k_jnd",
+                 "trainer=mps"
+             ],
+             "console": "integratedTerminal",
+             "cwd": "${workspaceFolder}",
+             "env": {},
+             "justMyCode": false
+         },
+         {
+             "name": "avix-cnn-jnd",
+             "type": "python",
+             "request": "launch",
+             "program": "${workspaceFolder}/src/train.py",
+             "args": [
+                 "experiment=avix_cnn_72dss_jnd",
+                 "trainer=mps"
+             ],
+             "console": "integratedTerminal",
+             "cwd": "${workspaceFolder}",
+             "env": {},
+             "justMyCode": false
+         },
+         {
              "name": "avix-cnn",
              "type": "python",
              "request": "launch",
              "program": "${workspaceFolder}/src/train.py",
              "args": [
!                 "experiment=avix_cnn_72dss",
                  "trainer=mps"
              ],
              "console": "integratedTerminal",
Only in avix: AVIX_DEVEL_PLAN.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/CLAUDE.md avix/CLAUDE.md
*** lightning-hydra-template-extended/CLAUDE.md	Thu Aug 14 16:56:15 2025
--- avix/CLAUDE.md	Sat Aug 23 20:33:35 2025
***************
*** 70,75 ****
--- 70,85 ----
  make deactivate  # Shows alias setup for 'd' command
  ```

+ ### Cleaning and Maintenance
+ ```bash
+ # Clean autogenerated files
+ make clean       # or make c - remove cache, pyc files, DS_Store
+ make clean-logs  # or make cl - remove all logs
+
+ # Sync with main branch
+ make sync        # or make s - pull latest changes
+ ```
+
  ### Visualization and Analysis
  ```bash
  # Generate model architecture diagrams
***************
*** 130,139 ****
  python examples/vimh_training.py --demo --save-plots
  ```

- ### Environment Management
- ```bash
- source .venv/bin/activate.csh
- ```

  ## Troubleshooting Notes
  - When you see "No module named 'rootutils'", it means we need to say `source .venv/bin/activate`
--- 140,145 ----
***************
*** 144,150 ****

  ### Extended Template Features
  This is an **extended** Lightning-Hydra-Template with major enhancements:
! - **Multiple architectures**: SimpleDenseNet, SimpleCNN, ConvNeXt-V2, ViT, EfficientNet
  - **CIFAR benchmark suite**: CIFAR-10/100 with literature-competitive baselines
  - **VIMH (Variable Image MultiHead)**: Advanced multihead dataset format with auto-configuration
  - **Configurable losses**: Hydra-managed loss functions, no hardcoding
--- 150,156 ----

  ### Extended Template Features
  This is an **extended** Lightning-Hydra-Template with major enhancements:
! - **Multiple architectures**: SimpleDenseNet (MLP with BatchNorm), SimpleCNN, ConvNeXt-V2, ViT, EfficientNet
  - **CIFAR benchmark suite**: CIFAR-10/100 with literature-competitive baselines
  - **VIMH (Variable Image MultiHead)**: Advanced multihead dataset format with auto-configuration
  - **Configurable losses**: Hydra-managed loss functions, no hardcoding
***************
*** 215,220 ****
--- 221,227 ----
  - User values fast iteration and minimal boilerplate for research
  - Makefile provides convenient shortcuts for common tasks with meaningful abbreviations
  - MPS (Metal Performance Shaders) support for Mac training available and nearly always used by user
+ - Use `make help` to see all available make targets with descriptions

  ## Important Development Reminders

Only in avix: CODE_REVIEW.md
Only in avix: CONDITIONING_SUPPORT_PLAN.md
Only in avix: ClaudeNotes.txt
Only in avix: ClaudeNotes.txt.orig
Only in avix: CommitMessages.txt
Only in avix: GRADIENT_DISCUSSION.md
Only in avix: HydraSweeps.txt
Only in avix: MAYBE_DO.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/Makefile avix/Makefile
*** lightning-hydra-template-extended/Makefile	Sun Aug 24 20:09:08 2025
--- avix/Makefile	Sat Aug 23 20:33:35 2025
***************
*** 1,6 ****
--- 1,12 ----
+ # POSIX-compatible Python variable
+ PYTHON ?= .venv/bin/python
+
  h help:  ## Show help
  	@grep -E '^[.a-zA-Z0-9_ -]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-30s\033[0m %s\n", $$1, $$2}'

+ cd clean-data: ## Clean synthesized datasets in ./data-vimh/.
+ 	/bin/rm -rf ./data-vimh/*
+
  c clean: ## Clean autogenerated files
  	rm -rf dist
  	find . -type f -name "*.DS_Store" -ls -delete
***************
*** 28,33 ****
--- 34,48 ----
  	@echo "Add to ~/.tcshrc: alias d 'echo deactivate && deactivate'"
  	@echo "Then just type: d"

+ # GENERATE DATASET
+
+ DATASET = dataset_8000Hz_1p0s_16384dss_simple_2p
+
+ ds dataset: ./data/$(DATASET)
+
+ ./data/$(DATASET):
+ 	(cd /l/smc && ./doit 3 simple && cp -p ./local_data/$(DATASET) ./data/)
+
  # TRAINING TARGETS "tr"

  tr train train-sdn: ## Train the default model (a small SimpleDenseNet)
***************
*** 144,149 ****
--- 159,181 ----
  	python src/train.py model=mnist_convnext_68k trainer.max_epochs=3 tags="[arch_comparison,convnext]"
  	@echo "=== Check logs/ directory for results comparison ==="

+ ss simp-small: ## Generate small SimpleSynth dataset (VIMH format)
+ 	. .venv/bin/activate && time python generate_vimh.py --pickle ++dataset.size=256
+
+ sl simp-large: ## Generate large SimpleSynth dataset (VIMH format)
+ 	. .venv/bin/activate && time python generate_vimh.py --pickle ++dataset.size=16384
+
+ sa simp-avix: ## Generate the complete SimpleSynth AVIX dataset (VIMH format)
+ 	. .venv/bin/activate && time python generate_vimh.py --avix --pickle --shuffle
+
+ san simp-avix-normalized: ## Generate the complete SimpleSynth AVIX dataset (VIMH format)
+ 	. .venv/bin/activate && time python generate_vimh.py --avix --pickle --shuffle --temporal-envelope --spectral-envelope --normalize
+
+ # shuffle-simp-avix: ## Shuffle the dataset created by "make simp-avix" (obsolete - use --shuffle option)
+ # 	python shuffle_vimh_dataset.py \
+ # 		--input data-vimh/avix-vimh-32x32x1_8000Hz_1p0s_72dss_simple_2p \
+ # 		--output data-vimh/avix-vimh-32x32x1_8000Hz_1p0s_72dss_simple_2p_shuffled
+
  # EXPERIMENTS "e" - Reproducible Configuration Examples

  e esdn exp-sdn: ## Run original example experiment (reproducible baseline)
***************
*** 278,280 ****
--- 310,468 ----
  	@echo "=== Complete CIFAR benchmark suite finished ==="

  allqt all-quick-tests: tqa cbqa ## All quick tests
+
+
+ # AVIX TRAINING TARGETS "avix" - Direct Parameter Optimization
+
+ av avix avix-cnn-72dss: ## Train AVIX CNN on 16K dataset
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_cnn_72dss $(ARGS)
+
+ aj avixj avix-cnn-72dss-jnd: ## Train AVIX CNN on 16K dataset
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_cnn_72dss_jnd $(ARGS)
+
+ aac avix-architecture-comparison: ## HP: Train AVIX MLP,CNN ~11k params on 72-sample dataset
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py -m hparams_search=avix_architecture_comparison experiment=avix_cnn_72dss_jnd
+
+ # AVIX 11K PARAMETER ARCHITECTURE EXPERIMENTS
+
+ amlp avix-mlp: ## Train AVIX MLP baseline ~11k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_mlp_72dss_jnd $(ARGS)
+
+ adn avix-densenet: ## Train AVIX DenseNet ~11k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_jnd $(ARGS)
+
+ avit avix-vit: ## Train AVIX ViT ~11k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_vit_72dss_jnd $(ARGS)
+
+ # AVIX 64K PARAMETER ARCHITECTURE EXPERIMENTS
+
+ amlp64 avix-mlp-64k: ## Train AVIX MLP baseline ~64k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_mlp_72dss_64k_jnd $(ARGS)
+
+ amlp64d avix-mlp-64k-diagram: ## Show AVIX Simple Dense Net ~64k diagram
+ 	python viz/simple_model_diagram.py --config avix_mlp_64k_jnd  # ./configs/model/avix_mlp_64k_jnd.yaml
+
+ tbr:
+ 	@lsof -i :6007 >/dev/null 2>&1 && echo "TensorBoard already running on port 6007" || \
+ 		(echo "Starting TensorBoard on port 6007..." && tensorboard --logdir logs/train/runs/ --port 6007 &)
+ 	@echo "Open http://localhost:6007/#scalars&tagFilter=log10_decay_time"
+
+ tbmr:
+ 	@lsof -i :6008 >/dev/null 2>&1 && echo "TensorBoard already running on port 6008" || \
+ 		(echo "Starting TensorBoard on port 6008..." && tensorboard --logdir logs/train/runs/ --port 6008 &)
+ 	@echo "Open http://localhost:6008/#scalars&tagFilter=log10_decay_time"
+
+ asdn64 avix-simple-dense-net-64k: ## Train AVIX Simple Dense Net ~64k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd $(ARGS)  # ./configs/experiment/avix_densenet_72dss_64k_jnd.yaml
+ 	(make tbr)
+
+ dgb asdn64dgb avix-simple-dense-net-64k-direct-gradient-baseline: ## Train AVIX Simple Dense Net ~64k params using Direct Gradient Baseline (high LR, 100 epochs)
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg $(ARGS)  # ./configs/experiment/avix_densenet_72dss_64k_jnd_dg.yaml
+ 	(make tbr)
+
+ # Obsolete:
+ # dgf asdn64dgf avix-simple-dense-net-64k-direct-gradient-features: ## Train AVIX Simple Dense Net ~64k params using Direct Gradient + Spectral Features
+ # 	@echo "ARGS = $(ARGS)"
+ # 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg_features $(ARGS)  # ./configs/experiment/avix_densenet_72dss_64k_jnd_dg_features.yaml
+ # 	(make tbr)
+
+ dgn asdn64dgn avix-simple-dense-net-64k-direct-gradient-normalized: ## Train AVIX Simple Dense Net ~64k params using Direct Gradient + Spectral Normalization
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg_normalized $(ARGS)  # ./configs/experiment/avix_densenet_72dss_64k_jnd_dg_normalized.yaml
+ 	(make tbr)
+
+ dgbx asdn64dgbx avix-simple-dense-net-64k-direct-gradient-baseline-avix: ## Train AVIX Simple Dense Net ~64k params using Direct Gradient Baseline in AVIX mode
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg_fixed $(ARGS)  # ./configs/experiment/avix_densenet_72dss_64k_jnd_dg_fixed.yaml
+ 	(make tbr)
+
+ dgnx asdn64dgnx avix-simple-dense-net-64k-direct-gradient-normalized-avix: ## Train AVIX Simple Dense Net ~64k params using pre-computed normalized channels
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg_normalized_precomputed $(ARGS)  # ./configs/experiment/avix_densenet_72dss_64k_jnd_dg_normalized_precomputed.yaml
+ 	(make tbr)
+
+ dgx asdn64dgx avix-simple-dense-net-64k-direct-gradient-unnormalized: ## Train AVIX Simple Dense Net ~64k params using unnormalized grayscale channels
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg_unnormalized $(ARGS)  # ./configs/experiment/avix_densenet_72dss_64k_jnd_dg_unnormalized.yaml
+ 	(make tbr)
+
+ dgt dg-tuned: ## Train AVIX SimpleDenseNet with tuned hyperparameters for log10_decay_time
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_jnd_dg_tuned $(ARGS)  # ./configs/experiment/avix_densenet_72dss_jnd_dg_tuned.yaml
+ 	(make tbr)
+
+ dgct dg-cnn-tuned: ## Train AVIX CNN with tuned hyperparameters for log10_decay_time
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_cnn_72dss_jnd_dg_tuned $(ARGS)  # ./configs/experiment/avix_cnn_72dss_jnd_dg_tuned.yaml
+ 	(make tbr)
+
+ dgch dg-cnn-hybrid: ## Train AVIX CNN with auxiliary scalar features (measured decay time)
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_cnn_72dss_jnd_dg_hybrid trainer.max_epochs=300 hydra.job_logging.root.level=INFO $(ARGS)  # ./configs/experiment/avix_cnn_72dss_jnd_dg_hybrid.yaml
+ 	(make tbr)
+
+ dglw dg-loss-weighted: ## Train with weighted loss prioritizing log10_decay_time
+ 	@echo "ARGS = $(ARGS)"
+ 	make dgx ARGS="model.optimizer.lr=0.0005 trainer.gradient_clip_val=0.05 model.loss_weights.log10_decay_time=3.0 model.loss_weights.note_velocity=1.0 callbacks.early_stopping.patience=50 $(ARGS)"
+
+ dgro dg-rmsprop: ## Train with RMSprop optimizer (often better for audio)
+ 	@echo "ARGS = $(ARGS)"
+ 	make dgx ARGS="model.optimizer._target_=torch.optim.RMSprop model.optimizer.lr=0.001 model.optimizer.momentum=0.9 trainer.gradient_clip_val=0.1 $(ARGS)"
+
+ dge direct-gradient-experiments: dgx dgnx dgt dgct dglw dgro ## Run all direct-gradient experiments (AVIX mode only, some normalized, two archs)
+ 	@echo "Open http://localhost:6007/#scalars&tagFilter=log10_decay_time"
+
+ dgw direct-gradient-winner: dgch ## Run current winning direct-gradient experiment
+ 	@echo "Open http://localhost:6007/#scalars&tagFilter=log10_decay_time"
+
+ dgwr direct-gradient-weighted-loss-rms-optimizer: dglw dgro ## Run weighted loss and rmsprop optimizer
+ 	@echo "Open http://localhost:6007/#scalars&tagFilter=log10_decay_time"
+
+ #OLD:dga direct-gradient-all: asdn64dg asdn64dgf asdn64dgn asdn64dgnc  ## Make all direct-gradient examples: asdn64dg asdn64dgf asdn64dgn asdn64dgnc
+
+ dgnf dgn-fixed: ## Fixed direct gradient training with proper config
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg_fixed $(ARGS)
+
+ avixo avix-ordinal: ## AVIX mode with ordinal L1 (asdn64 + more data)
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_64k_jnd_avix $(ARGS)
+
+ asdn64d avix-simple-dense-net-64k-diagram: ## Show AVIX Simple Dense Net ~64k diagram
+ 	python viz/simple_model_diagram.py --config avix_densenet_64k_jnd  # ./configs/model/avix_densenet_64k_jnd.yaml
+
+ asdn128 avix-simple-dense-net-128k: ## Train AVIX Simple Dense Net ~128k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_densenet_72dss_128k_jnd $(ARGS)
+
+ acnn64 avix-cnn-64k: ## Train AVIX CNN ~64k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_cnn_72dss_64k_jnd $(ARGS)
+
+ avit64 avix-vit-64k: ## Train AVIX ViT ~64k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=avix_vit_72dss_64k_jnd $(ARGS)
+
+ aac64 avix-architecture-comparison-64k: ## HP: Train AVIX all architectures ~64k params
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py -m hparams_search=avix_architecture_comparison_64k experiment=avix_cnn_72dss_64k_jnd
+
+ dgt1m direct_gradient_training_1m: ## Direct gradient training for synthesizer parameters
+ 	@echo "ARGS = $(ARGS)"
+ 	time python src/train.py experiment=direct_gradient_training.yaml $(ARGS) # ./configs/experiment/direct_gradient_training.yaml
+
+ # AVIX DENSENET HYPERPARAMETER SEARCHES
+ asdnsc avix-simple-dense-net-size-comparison: ## HP: Train Simple Dense Net on ~11k, ~64k params
+ 	python src/train.py -m hparams_search=avix_densenet_size_comparison
+ 	tensorboard --logdir logs/train/multiruns/ --port 6008 &
+ 	@echo open http://localhost:6008/
+ 	@echo open http://localhost:6008/ | pbcopy
Only in avix: Makefile.orig
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/README.md avix/README.md
*** lightning-hydra-template-extended/README.md	Tue Jul 15 03:12:31 2025
--- avix/README.md	Tue Aug 26 13:17:45 2025
***************
*** 1,1379 ****
! # Lightning-Hydra-Template-*Extended*

! ## Extensions

- This [Lightning-Hydra-Template-Extended](https://github.com/josmithiii/lightning-hydra-template-extended.git)
- project extends the original [Lightning-Hydra-Template](https://github.com/ashleve/lightning-hydra-template) to include:
- - **Multiple neural network architectures** with easy switching (CNN, EfficientNet, ViT, ConvNeXt-V2, ...)
- - CNN **Multihead Classification** with backward compatibility for original single-head usage
- - **New loss functions** via Hydra configuration
- - **New convenience make targets** for streamlined development and testing workflow
- - **Original Lightning-Hydra-Template Compatibility** for pre-existing configs, models, and datasets
-
- See [docs/extensions.md](docs/extensions.md) for details
-
- ## 🎯 Multihead Dataset Support (VIMH Format)
-
- This template now includes comprehensive support for **Variable Image MultiHead (VIMH)** datasets, enabling advanced multihead neural network training with self-describing metadata.
-
- ### ✨ Key Features
-
- - **Variable Image Dimensions**: Support for 32x32x3, 28x28x1, and other image formats
- - **Self-Describing Metadata**: JSON-based dataset configuration with parameter mappings
- - **8-bit Parameter Quantization**: Efficient storage and handling of continuous parameters
- - **Automatic Model Configuration**: Models auto-configure from dataset metadata
- - **Efficient Loading**: Optimized dimension detection with cross-validation
- - **Comprehensive Testing**: 27 tests covering all VIMH functionality
-
- ### 🚀 Quick Start
-
- ```bash
- # Train with VIMH dataset
- python src/train.py experiment=vimh_cnn
-
- # Run complete training example
- python examples/vimh_training.py
-
- # Quick demo with visualizations
- python examples/vimh_training.py --demo --save-plots
- ```
-
- ### 📊 Dataset Format
-
- VIMH datasets use a structured format with:
- - **Images**: Variable dimensions (e.g., 32x32x3, 28x28x1)
- - **Labels**: `[N] [param1_id] [param1_val] [param2_id] [param2_val] ...`
- - **Metadata**: JSON file with parameter mappings and dataset info
- - **Validation**: Cross-validation across directory name, JSON, and binary sources
-
- ### 🔧 Configuration
-
- ```yaml
- # configs/data/vimh.yaml
- _target_: src.data.vimh_datamodule.VIMHDataModule
- data_dir: data-vimh/vimh-32x32_8000Hz_1p0s_256dss_resonarium_2p
- batch_size: 128
- num_workers: 4
-
- # Model auto-configures from dataset
- # configs/experiment/vimh_cnn.yaml
- defaults:
-   - override /data: vimh
-   - override /model: vimh_cnn_64k
- ```
-
- ### 📈 Performance
-
- - **Loading Optimization**: 10x faster initialization with efficient dimension detection
- - **Memory Efficiency**: Optimized transform adjustment for different image sizes
- - **Training Speed**: Comparable to single-head models with minimal overhead
- - **Scalability**: Supports datasets up to 1M+ samples
-
- ### 🛠️ Use Cases
-
- - **Audio Synthesis**: Image-to-audio parameter mapping
- - **Computer Vision**: Multi-target regression tasks
- - **Scientific Computing**: Parameter prediction from visual data
- - **Research**: Multihead neural network architectures
-
- For detailed documentation, see [docs/vimh.md](docs/vimh.md) and [docs/multihead.md](docs/multihead.md).
-
- ---
-
- # Original **Lightning-Hydra-Template** README.md
-
  <div align="center">

  [![python](https://img.shields.io/badge/-Python_3.8_%7C_3.9_%7C_3.10-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
  [![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
  [![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
  [![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)
- [![black](https://img.shields.io/badge/Code%20Style-Black-black.svg?labelColor=gray)](https://black.readthedocs.io/en/stable/)
- [![isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/) <br>
- [![tests](https://github.com/ashleve/lightning-hydra-template/actions/workflows/test.yml/badge.svg)](https://github.com/ashleve/lightning-hydra-template/actions/workflows/test.yml)
- [![code-quality](https://github.com/ashleve/lightning-hydra-template/actions/workflows/code-quality-main.yaml/badge.svg)](https://github.com/ashleve/lightning-hydra-template/actions/workflows/code-quality-main.yaml)
- [![codecov](https://codecov.io/gh/ashleve/lightning-hydra-template/branch/main/graph/badge.svg)](https://codecov.io/gh/ashleve/lightning-hydra-template) <br>
- [![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)](https://github.com/ashleve/lightning-hydra-template#license)
- [![PRs](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](https://github.com/ashleve/lightning-hydra-template/pulls)
- [![contributors](https://img.shields.io/github/contributors/ashleve/lightning-hydra-template.svg)](https://github.com/ashleve/lightning-hydra-template/graphs/contributors)

- A clean template to kickstart your deep learning project 🚀⚡🔥<br>
- Click on [<kbd>Use this template</kbd>](https://github.com/ashleve/lightning-hydra-template/generate) to initialize new repository.
-
- _Suggestions are always welcome!_
-
  </div>

! <br>

! ## 📌  Introduction

! **Why you might want to use it:**

! ✅ Save on boilerplate <br>
! Easily add new models, datasets, tasks, experiments, and train on different accelerators, like multi-GPU, TPU or SLURM clusters.

! ✅ Education <br>
! Thoroughly commented. You can use this repo as a learning resource.

! ✅ Reusability <br>
! Collection of useful MLOps tools, configs, and code snippets. You can use this repo as a reference for various utilities.

! **Why you might not want to use it:**

- ❌ Things break from time to time <br>
- Lightning and Hydra are still evolving and integrate many libraries, which means sometimes things break. For the list of currently known problems visit [this page](https://github.com/ashleve/lightning-hydra-template/labels/bug).
-
- ❌ Not adjusted for data engineering <br>
- Template is not really adjusted for building data pipelines that depend on each other. It's more efficient to use it for model prototyping on ready-to-use data.
-
- ❌ Overfitted to simple use case <br>
- The configuration setup is built with simple lightning training in mind. You might need to put some effort to adjust it for different use cases, e.g. lightning fabric.
-
- ❌ Might not support your workflow <br>
- For example, you can't resume hydra-based multirun or hyperparameter search.
-
- > **Note**: _Keep in mind this is unofficial community project._
-
- <br>
-
- ## Main Technologies
-
- [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) - a lightweight PyTorch wrapper for high-performance AI research. Think of it as a framework for organizing your PyTorch code.
-
- [Hydra](https://github.com/facebookresearch/hydra) - a framework for elegantly configuring complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line.
-
- <br>
-
- ## Main Ideas
-
- - [**Rapid Experimentation**](#your-superpowers): thanks to hydra command line superpowers
- - [**Minimal Boilerplate**](#how-it-works): thanks to automating pipelines with config instantiation
- - [**Main Configs**](#main-config): allow you to specify default training configuration
- - [**Experiment Configs**](#experiment-config): allow you to override chosen hyperparameters and version control experiments
- - [**Workflow**](#workflow): comes down to 4 simple steps
- - [**Experiment Tracking**](#experiment-tracking): Tensorboard, W&B, Neptune, Comet, MLFlow and CSVLogger
- - [**Logs**](#logs): all logs (checkpoints, configs, etc.) are stored in a dynamically generated folder structure
- - [**Hyperparameter Search**](#hyperparameter-search): simple search is effortless with Hydra plugins like Optuna Sweeper
- - [**Tests**](#tests): generic, easy-to-adapt smoke tests for speeding up the development
- - [**Continuous Integration**](#continuous-integration): automatically test and lint your repo with Github Actions
- - [**Best Practices**](#best-practices): a couple of recommended tools, practices and standards
-
- <br>
-
- ## Project Structure
-
- The directory structure of new project looks like this:
-
- ```
- ├── .github                   <- Github Actions workflows
- │
- ├── configs                   <- Hydra configs
- │   ├── callbacks                <- Callbacks configs
- │   ├── data                     <- Data configs
- │   ├── debug                    <- Debugging configs
- │   ├── experiment               <- Experiment configs
- │   ├── extras                   <- Extra utilities configs
- │   ├── hparams_search           <- Hyperparameter search configs
- │   ├── hydra                    <- Hydra configs
- │   ├── local                    <- Local configs
- │   ├── logger                   <- Logger configs
- │   ├── model                    <- Model configs
- │   ├── paths                    <- Project paths configs
- │   ├── trainer                  <- Trainer configs
- │   │
- │   ├── eval.yaml             <- Main config for evaluation
- │   └── train.yaml            <- Main config for training
- │
- ├── data                   <- Project data
- │
- ├── logs                   <- Logs generated by hydra and lightning loggers
- │
- ├── notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
- │                             the creator's initials, and a short `-` delimited description,
- │                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
- │
- ├── scripts                <- Shell scripts
- │
- ├── src                    <- Source code
- │   ├── data                     <- Data scripts
- │   ├── models                   <- Model scripts
- │   ├── utils                    <- Utility scripts
- │   │
- │   ├── eval.py                  <- Run evaluation
- │   └── train.py                 <- Run training
- │
- ├── tests                  <- Tests of any kind
- │
- ├── .env.example              <- Example of file for storing private environment variables
- ├── .gitignore                <- List of files ignored by git
- ├── .pre-commit-config.yaml   <- Configuration of pre-commit hooks for code formatting
- ├── .project-root             <- File for inferring the position of project root directory
- ├── environment.yaml          <- File for installing conda environment
- ├── Makefile                  <- Makefile with commands like `make train` or `make test`
- ├── pyproject.toml            <- Configuration options for testing and linting
- ├── requirements.txt          <- File for installing python dependencies
- ├── setup.py                  <- File for installing project as a package
- └── README.md
- ```
-
- <br>
-
- ## 🚀  Quickstart
-
  ```bash
! # clone project
! git clone https://github.com/ashleve/lightning-hydra-template
! cd lightning-hydra-template

! # [OPTIONAL] create conda environment
! conda create -n myenv python=3.9
! conda activate myenv

! # install pytorch according to instructions
! # https://pytorch.org/get-started/
!
! # install requirements
  pip install -r requirements.txt
  ```

! Template contains example with MNIST classification.<br>
! When running `python src/train.py` you should see something like this:

! <div align="center">

- ![](https://github.com/ashleve/lightning-hydra-template/blob/resources/terminal.png)
-
- </div>
-
- ## ⚡  Your Superpowers
-
- <details>
- <summary><b>Override any config parameter from command line</b></summary>
-
  ```bash
! python train.py trainer.max_epochs=20 model.optimizer.lr=1e-4
! ```

! > **Note**: You can also add new parameters with `+` sign.

! ```bash
! python train.py +model.new_param="owo"
  ```

! </details>

! <details>
! <summary><b>Train on CPU, GPU, multi-GPU and TPU</b></summary>

! ```bash
! # train on CPU
! python train.py trainer=cpu

! # train on 1 GPU
! python train.py trainer=gpu

- # train on TPU
- python train.py +trainer.tpu_cores=8
-
- # train with DDP (Distributed Data Parallel) (4 GPUs)
- python train.py trainer=ddp trainer.devices=4
-
- # train with DDP (Distributed Data Parallel) (8 GPUs, 2 nodes)
- python train.py trainer=ddp trainer.devices=4 trainer.num_nodes=2
-
- # simulate DDP on CPU processes
- python train.py trainer=ddp_sim trainer.devices=2
-
- # accelerate training on mac
- python train.py trainer=mps
- ```
-
- > **Warning**: Currently there are problems with DDP mode, read [this issue](https://github.com/ashleve/lightning-hydra-template/issues/393) to learn more.
-
- </details>
-
- <details>
- <summary><b>Train with mixed precision</b></summary>
-
  ```bash
! # train with pytorch native automatic mixed precision (AMP)
! python train.py trainer=gpu +trainer.precision=16
! ```

! </details>

! <!-- deepspeed support still in beta
! <details>
! <summary><b>Optimize large scale models on multiple GPUs with Deepspeed</b></summary>
!
! ```bash
! python train.py +trainer.
  ```

! </details>
!  -->

! <details>
! <summary><b>Train model with any logger available in PyTorch Lightning, like W&B or Tensorboard</b></summary>

! ```yaml
! # set project and entity names in `configs/logger/wandb`
! wandb:
!   project: "your_project_name"
!   entity: "your_wandb_team_name"
  ```
!
! ```bash
! # train model with Weights&Biases (link to wandb dashboard should appear in the terminal)
! python train.py logger=wandb
  ```

! > **Note**: Lightning provides convenient integrations with most popular logging frameworks. Learn more [here](#experiment-tracking).

! > **Note**: Using wandb requires you to [setup account](https://www.wandb.com/) first. After that just complete the config as below.

! > **Note**: Click [here](https://wandb.ai/hobglob/template-dashboard/) to see example wandb dashboard generated with this template.

- </details>
-
- <details>
- <summary><b>Train model with chosen experiment config</b></summary>
-
- ```bash
- python train.py experiment=example
- ```
-
- > **Note**: Experiment configs are placed in [configs/experiment/](configs/experiment/).
-
- </details>
-
- <details>
- <summary><b>Attach some callbacks to run</b></summary>
-
- ```bash
- python train.py callbacks=default
- ```
-
- > **Note**: Callbacks can be used for things such as as model checkpointing, early stopping and [many more](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#built-in-callbacks).
-
- > **Note**: Callbacks configs are placed in [configs/callbacks/](configs/callbacks/).
-
- </details>
-
- <details>
- <summary><b>Use different tricks available in Pytorch Lightning</b></summary>
-
  ```yaml
! # gradient clipping may be enabled to avoid exploding gradients
! python train.py +trainer.gradient_clip_val=0.5
!
! # run validation loop 4 times during a training epoch
! python train.py +trainer.val_check_interval=0.25
!
! # accumulate gradients
! python train.py +trainer.accumulate_grad_batches=10
!
! # terminate training after 12 hours
! python train.py +trainer.max_time="00:12:00:00"
  ```

! > **Note**: PyTorch Lightning provides about [40+ useful trainer flags](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags).

! </details>

! <details>
! <summary><b>Easily debug</b></summary>

- ```bash
- # runs 1 epoch in default debugging mode
- # changes logging directory to `logs/debugs/...`
- # sets level of all command line loggers to 'DEBUG'
- # enforces debug-friendly configuration
- python train.py debug=default
-
- # run 1 train, val and test loop, using only 1 batch
- python train.py debug=fdr
-
- # print execution time profiling
- python train.py debug=profiler
-
- # try overfitting to 1 batch
- python train.py debug=overfit
-
- # raise exception if there are any numerical anomalies in tensors, like NaN or +/-inf
- python train.py +trainer.detect_anomaly=true
-
- # use only 20% of the data
- python train.py +trainer.limit_train_batches=0.2 \
- +trainer.limit_val_batches=0.2 +trainer.limit_test_batches=0.2
  ```
!
! > **Note**: Visit [configs/debug/](configs/debug/) for different debugging configs.
!
! </details>
!
! <details>
! <summary><b>Resume training from checkpoint</b></summary>
!
! ```yaml
! python train.py ckpt_path="/path/to/ckpt/name.ckpt"
  ```

! > **Note**: Checkpoint can be either path or URL.

! > **Note**: Currently loading ckpt doesn't resume logger experiment, but it will be supported in future Lightning release.

- </details>
-
- <details>
- <summary><b>Evaluate checkpoint on test dataset</b></summary>
-
- ```yaml
- python eval.py ckpt_path="/path/to/ckpt/name.ckpt"
- ```
-
- > **Note**: Checkpoint can be either path or URL.
-
- </details>
-
- <details>
- <summary><b>Create a sweep over hyperparameters</b></summary>
-
  ```bash
! # this will run 6 experiments one after the other,
! # each with different combination of batch_size and learning rate
! python train.py -m data.batch_size=32,64,128 model.lr=0.001,0.0005
  ```

! > **Note**: Hydra composes configs lazily at job launch time. If you change code or configs after launching a job/sweep, the final composed configs might be impacted.

! </details>

! <details>
! <summary><b>Create a sweep over hyperparameters with Optuna</b></summary>
!
  ```bash
! # this will run hyperparameter search defined in `configs/hparams_search/mnist_optuna.yaml`
! # over chosen experiment config
! python train.py -m hparams_search=mnist_optuna experiment=example
! ```

! > **Note**: Using [Optuna Sweeper](https://hydra.cc/docs/next/plugins/optuna_sweeper) doesn't require you to add any boilerplate to your code, everything is defined in a [single config file](configs/hparams_search/mnist_optuna.yaml).

! > **Warning**: Optuna sweeps are not failure-resistant (if one job crashes then the whole sweep crashes).
!
! </details>
!
! <details>
! <summary><b>Execute all experiments from folder</b></summary>
!
! ```bash
! python train.py -m 'experiment=glob(*)'
  ```

! > **Note**: Hydra provides special syntax for controlling behavior of multiruns. Learn more [here](https://hydra.cc/docs/next/tutorials/basic/running_your_app/multi-run). The command above executes all experiments from [configs/experiment/](configs/experiment/).
!
! </details>
!
! <details>
! <summary><b>Execute run for multiple different seeds</b></summary>
!
  ```bash
! python train.py -m seed=1,2,3,4,5 trainer.deterministic=True logger=csv tags=["benchmark"]
! ```

! > **Note**: `trainer.deterministic=True` makes pytorch more deterministic but impacts the performance.
!
! </details>
!
! <details>
! <summary><b>Execute sweep on a remote AWS cluster</b></summary>
!
! > **Note**: This should be achievable with simple config using [Ray AWS launcher for Hydra](https://hydra.cc/docs/next/plugins/ray_launcher). Example is not implemented in this template.
!
! </details>
!
! <!-- <details>
! <summary><b>Execute sweep on a SLURM cluster</b></summary>
!
! > This should be achievable with either [the right lightning trainer flags](https://pytorch-lightning.readthedocs.io/en/latest/clouds/cluster.html?highlight=SLURM#slurm-managed-cluster) or simple config using [Submitit launcher for Hydra](https://hydra.cc/docs/plugins/submitit_launcher). Example is not yet implemented in this template.
!
! </details> -->
!
! <details>
! <summary><b>Use Hydra tab completion</b></summary>
!
! > **Note**: Hydra allows you to autocomplete config argument overrides in shell as you write them, by pressing `tab` key. Read the [docs](https://hydra.cc/docs/tutorials/basic/running_your_app/tab_completion).
!
! </details>
!
! <details>
! <summary><b>Apply pre-commit hooks</b></summary>
!
! ```bash
  pre-commit run -a
  ```

! > **Note**: Apply pre-commit hooks to do things like auto-formatting code and configs, performing code analysis or removing output from jupyter notebooks. See [# Best Practices](#best-practices) for more.

! Update pre-commit hook versions in `.pre-commit-config.yaml` with:

! ```bash
! pre-commit autoupdate
  ```

! </details>

! <details>
! <summary><b>Run tests</b></summary>

! ```bash
! # run all tests
! pytest

! # run tests from specific file
! pytest tests/test_train.py

- # run all tests except the ones marked as slow
- pytest -k "not slow"
- ```
-
- </details>
-
- <details>
- <summary><b>Use tags</b></summary>
-
- Each experiment should be tagged in order to easily filter them across files or in logger UI:
-
- ```bash
- python train.py tags=["mnist","experiment_X"]
- ```
-
- > **Note**: You might need to escape the bracket characters in your shell with `python train.py tags=\["mnist","experiment_X"\]`.
-
- If no tags are provided, you will be asked to input them from command line:
-
- ```bash
- >>> python train.py tags=[]
- [2022-07-11 15:40:09,358][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
- [2022-07-11 15:40:09,359][src.utils.rich_utils][WARNING] - No tags provided in config. Prompting user to input tags...
- Enter a list of comma separated tags (dev):
- ```
-
- If no tags are provided for multirun, an error will be raised:
-
- ```bash
- >>> python train.py -m +x=1,2,3 tags=[]
- ValueError: Specify tags before launching a multirun!
- ```
-
- > **Note**: Appending lists from command line is currently not supported in hydra :(
-
- </details>
-
- <br>
-
- ## ❤️  Contributions
-
- This project exists thanks to all the people who contribute.
-
- ![Contributors](https://readme-contributors.now.sh/ashleve/lightning-hydra-template?extension=jpg&width=400&aspectRatio=1)
-
- Have a question? Found a bug? Missing a specific feature? Feel free to file a new issue, discussion or PR with respective title and description.
-
- Before making an issue, please verify that:
-
- - The problem still exists on the current `main` branch.
- - Your python dependencies are updated to recent versions.
-
- Suggestions for improvements are always welcome!
-
- <br>
-
- ## How It Works
-
- All PyTorch Lightning modules are dynamically instantiated from module paths specified in config. Example model config:
-
- ```yaml
- _target_: src.models.mnist_model.MNISTLitModule
- lr: 0.001
- net:
-   _target_: src.models.components.simple_dense_net.SimpleDenseNet
-   input_size: 784
-   lin1_size: 256
-   lin2_size: 256
-   lin3_size: 256
-   output_size: 10
- ```
-
- Using this config we can instantiate the object with the following line:
-
- ```python
- model = hydra.utils.instantiate(config.model)
- ```
-
- This allows you to easily iterate over new models! Every time you create a new one, just specify its module path and parameters in appropriate config file. <br>
-
- Switch between models and datamodules with command line arguments:
-
- ```bash
- python train.py model=mnist
- ```
-
- Example pipeline managing the instantiation logic: [src/train.py](src/train.py).
-
- <br>
-
- ## Main Config
-
- Location: [configs/train.yaml](configs/train.yaml) <br>
- Main project config contains default training configuration.<br>
- It determines how config is composed when simply executing command `python train.py`.<br>
-
- <details>
- <summary><b>Show main project config</b></summary>
-
- ```yaml
- # order of defaults determines the order in which configs override each other
- defaults:
-   - _self_
-   - data: mnist.yaml
-   - model: mnist.yaml
-   - callbacks: default.yaml
-   - logger: null # set logger here or use command line (e.g. `python train.py logger=csv`)
-   - trainer: default.yaml
-   - paths: default.yaml
-   - extras: default.yaml
-   - hydra: default.yaml
-
-   # experiment configs allow for version control of specific hyperparameters
-   # e.g. best hyperparameters for given model and datamodule
-   - experiment: null
-
-   # config for hyperparameter optimization
-   - hparams_search: null
-
-   # optional local config for machine/user specific settings
-   # it's optional since it doesn't need to exist and is excluded from version control
-   - optional local: default.yaml
-
-   # debugging config (enable through command line, e.g. `python train.py debug=default)
-   - debug: null
-
- # task name, determines output directory path
- task_name: "train"
-
- # tags to help you identify your experiments
- # you can overwrite this in experiment configs
- # overwrite from command line with `python train.py tags="[first_tag, second_tag]"`
- # appending lists from command line is currently not supported :(
- # https://github.com/facebookresearch/hydra/issues/1547
- tags: ["dev"]
-
- # set False to skip model training
- train: True
-
- # evaluate on test set, using best model weights achieved during training
- # lightning chooses best weights based on the metric specified in checkpoint callback
- test: True
-
- # simply provide checkpoint path to resume training
- ckpt_path: null
-
- # seed for random number generators in pytorch, numpy and python.random
- seed: null
- ```
-
- </details>
-
- <br>
-
- ## Experiment Config
-
- Location: [configs/experiment](configs/experiment)<br>
- Experiment configs allow you to overwrite parameters from main config.<br>
- For example, you can use them to version control best hyperparameters for each combination of model and dataset.
-
- <details>
- <summary><b>Show example experiment config</b></summary>
-
- ```yaml
- # @package _global_
-
- # to execute this experiment run:
- # python train.py experiment=example
-
- defaults:
-   - override /data: mnist.yaml
-   - override /model: mnist.yaml
-   - override /callbacks: default.yaml
-   - override /trainer: default.yaml
-
- # all parameters below will be merged with parameters from default configurations set above
- # this allows you to overwrite only specified parameters
-
- tags: ["mnist", "simple_dense_net"]
-
- seed: 12345
-
- trainer:
-   min_epochs: 10
-   max_epochs: 10
-   gradient_clip_val: 0.5
-
- model:
-   optimizer:
-     lr: 0.002
-   net:
-     lin1_size: 128
-     lin2_size: 256
-     lin3_size: 64
-
- data:
-   batch_size: 64
-
- logger:
-   wandb:
-     tags: ${tags}
-     group: "mnist"
- ```
-
- </details>
-
- <br>
-
- ## Workflow
-
- **Basic workflow**
-
- 1. Write your PyTorch Lightning module (see [models/mnist_module.py](src/models/mnist_module.py) for example)
- 2. Write your PyTorch Lightning datamodule (see [data/mnist_datamodule.py](src/data/mnist_datamodule.py) for example)
- 3. Write your experiment config, containing paths to model and datamodule
- 4. Run training with chosen experiment config:
-    ```bash
-    python src/train.py experiment=experiment_name.yaml
-    ```
-
- **Experiment design**
-
- _Say you want to execute many runs to plot how accuracy changes in respect to batch size._
-
- 1. Execute the runs with some config parameter that allows you to identify them easily, like tags:
-
-    ```bash
-    python train.py -m logger=csv data.batch_size=16,32,64,128 tags=["batch_size_exp"]
-    ```
-
- 2. Write a script or notebook that searches over the `logs/` folder and retrieves csv logs from runs containing given tags in config. Plot the results.
-
- <br>
-
- ## Logs
-
- Hydra creates new output directory for every executed run.
-
- Default logging structure:
-
- ```
- ├── logs
- │   ├── task_name
- │   │   ├── runs                        # Logs generated by single runs
- │   │   │   ├── YYYY-MM-DD_HH-MM-SS       # Datetime of the run
- │   │   │   │   ├── .hydra                  # Hydra logs
- │   │   │   │   ├── csv                     # Csv logs
- │   │   │   │   ├── wandb                   # Weights&Biases logs
- │   │   │   │   ├── checkpoints             # Training checkpoints
- │   │   │   │   └── ...                     # Any other thing saved during training
- │   │   │   └── ...
- │   │   │
- │   │   └── multiruns                   # Logs generated by multiruns
- │   │       ├── YYYY-MM-DD_HH-MM-SS       # Datetime of the multirun
- │   │       │   ├──1                        # Multirun job number
- │   │       │   ├──2
- │   │       │   └── ...
- │   │       └── ...
- │   │
- │   └── debugs                          # Logs generated when debugging config is attached
- │       └── ...
- ```
-
- </details>
-
- You can change this structure by modifying paths in [hydra configuration](configs/hydra).
-
- <br>
-
- ## Experiment Tracking
-
- PyTorch Lightning supports many popular logging frameworks: [Weights&Biases](https://www.wandb.com/), [Neptune](https://neptune.ai/), [Comet](https://www.comet.ml/), [MLFlow](https://mlflow.org), [Tensorboard](https://www.tensorflow.org/tensorboard/).
-
- These tools help you keep track of hyperparameters and output metrics and allow you to compare and visualize results. To use one of them simply complete its configuration in [configs/logger](configs/logger) and run:
-
- ```bash
- python train.py logger=logger_name
- ```
-
- You can use many of them at once (see [configs/logger/many_loggers.yaml](configs/logger/many_loggers.yaml) for example).
-
- You can also write your own logger.
-
- Lightning provides convenient method for logging custom metrics from inside LightningModule. Read the [docs](https://pytorch-lightning.readthedocs.io/en/latest/extensions/logging.html#automatic-logging) or take a look at [MNIST example](src/models/mnist_module.py).
-
- <br>
-
- ## Tests
-
- Template comes with generic tests implemented with `pytest`.
-
- ```bash
- # run all tests
- pytest
-
- # run tests from specific file
- pytest tests/test_train.py
-
- # run all tests except the ones marked as slow
- pytest -k "not slow"
- ```
-
- Most of the implemented tests don't check for any specific output - they exist to simply verify that executing some commands doesn't end up in throwing exceptions. You can execute them once in a while to speed up the development.
-
- Currently, the tests cover cases like:
-
- - running 1 train, val and test step
- - running 1 epoch on 1% of data, saving ckpt and resuming for the second epoch
- - running 2 epochs on 1% of data, with DDP simulated on CPU
-
- And many others. You should be able to modify them easily for your use case.
-
- There is also `@RunIf` decorator implemented, that allows you to run tests only if certain conditions are met, e.g. GPU is available or system is not windows. See the [examples](tests/test_train.py).
-
- <br>
-
- ## Hyperparameter Search
-
- You can define hyperparameter search by adding new config file to [configs/hparams_search](configs/hparams_search).
-
- <details>
- <summary><b>Show example hyperparameter search config</b></summary>
-
- ```yaml
- # @package _global_
-
- defaults:
-   - override /hydra/sweeper: optuna
-
- # choose metric which will be optimized by Optuna
- # make sure this is the correct name of some metric logged in lightning module!
- optimized_metric: "val/acc_best"
-
- # here we define Optuna hyperparameter search
- # it optimizes for value returned from function with @hydra.main decorator
- hydra:
-   sweeper:
-     _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
-
-     # 'minimize' or 'maximize' the objective
-     direction: maximize
-
-     # total number of runs that will be executed
-     n_trials: 20
-
-     # choose Optuna hyperparameter sampler
-     # docs: https://optuna.readthedocs.io/en/stable/reference/samplers.html
-     sampler:
-       _target_: optuna.samplers.TPESampler
-       seed: 1234
-       n_startup_trials: 10 # number of random sampling runs before optimization starts
-
-     # define hyperparameter search space
-     params:
-       model.optimizer.lr: interval(0.0001, 0.1)
-       data.batch_size: choice(32, 64, 128, 256)
-       model.net.lin1_size: choice(64, 128, 256)
-       model.net.lin2_size: choice(64, 128, 256)
-       model.net.lin3_size: choice(32, 64, 128, 256)
- ```
-
- </details>
-
- Next, execute it with: `python train.py -m hparams_search=mnist_optuna`
-
- Using this approach doesn't require adding any boilerplate to code, everything is defined in a single config file. The only necessary thing is to return the optimized metric value from the launch file.
-
- You can use different optimization frameworks integrated with Hydra, like [Optuna, Ax or Nevergrad](https://hydra.cc/docs/plugins/optuna_sweeper/).
-
- The `optimization_results.yaml` will be available under `logs/task_name/multirun` folder.
-
- This approach doesn't support resuming interrupted search and advanced techniques like prunning - for more sophisticated search and workflows, you should probably write a dedicated optimization task (without multirun feature).
-
- <br>
-
- ## Continuous Integration
-
- Template comes with CI workflows implemented in Github Actions:
-
- - `.github/workflows/test.yaml`: running all tests with pytest
- - `.github/workflows/code-quality-main.yaml`: running pre-commits on main branch for all files
- - `.github/workflows/code-quality-pr.yaml`: running pre-commits on pull requests for modified files only
-
- <br>
-
- ## Distributed Training
-
- Lightning supports multiple ways of doing distributed training. The most common one is DDP, which spawns separate process for each GPU and averages gradients between them. To learn about other approaches read the [lightning docs](https://lightning.ai/docs/pytorch/latest/advanced/speed.html).
-
- You can run DDP on mnist example with 4 GPUs like this:
-
- ```bash
- python train.py trainer=ddp
- ```
-
- > **Note**: When using DDP you have to be careful how you write your models - read the [docs](https://lightning.ai/docs/pytorch/latest/advanced/speed.html).
-
- <br>
-
- ## Accessing Datamodule Attributes In Model
-
- The simplest way is to pass datamodule attribute directly to model on initialization:
-
- ```python
- # ./src/train.py
- datamodule = hydra.utils.instantiate(config.data)
- model = hydra.utils.instantiate(config.model, some_param=datamodule.some_param)
- ```
-
- > **Note**: Not a very robust solution, since it assumes all your datamodules have `some_param` attribute available.
-
- Similarly, you can pass a whole datamodule config as an init parameter:
-
- ```python
- # ./src/train.py
- model = hydra.utils.instantiate(config.model, dm_conf=config.data, _recursive_=False)
- ```
-
- You can also pass a datamodule config parameter to your model through variable interpolation:
-
- ```yaml
- # ./configs/model/my_model.yaml
- _target_: src.models.my_module.MyLitModule
- lr: 0.01
- some_param: ${data.some_param}
- ```
-
- Another approach is to access datamodule in LightningModule directly through Trainer:
-
- ```python
- # ./src/models/mnist_module.py
- def on_train_start(self):
-   self.some_param = self.trainer.datamodule.some_param
- ```
-
- > **Note**: This only works after the training starts since otherwise trainer won't be yet available in LightningModule.
-
- <br>
-
- ## Best Practices
-
- <details>
- <summary><b>Use Miniconda</b></summary>
-
- It's usually unnecessary to install full anaconda environment, miniconda should be enough (weights around 80MB).
-
- Big advantage of conda is that it allows for installing packages without requiring certain compilers or libraries to be available in the system (since it installs precompiled binaries), so it often makes it easier to install some dependencies e.g. cudatoolkit for GPU support.
-
- It also allows you to access your environments globally which might be more convenient than creating new local environment for every project.
-
- Example installation:
-
- ```bash
- wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
- bash Miniconda3-latest-Linux-x86_64.sh
- ```
-
- Update conda:
-
- ```bash
- conda update -n base -c defaults conda
- ```
-
- Create new conda environment:
-
- ```bash
- conda create -n myenv python=3.10
- conda activate myenv
- ```
-
- </details>
-
- <details>
- <summary><b>Use automatic code formatting</b></summary>
-
- Use pre-commit hooks to standardize code formatting of your project and save mental energy.<br>
- Simply install pre-commit package with:
-
- ```bash
- pip install pre-commit
- ```
-
- Next, install hooks from [.pre-commit-config.yaml](.pre-commit-config.yaml):
-
- ```bash
- pre-commit install
- ```
-
- After that your code will be automatically reformatted on every new commit.
-
- To reformat all files in the project use command:
-
- ```bash
- pre-commit run -a
- ```
-
- To update hook versions in [.pre-commit-config.yaml](.pre-commit-config.yaml) use:
-
- ```bash
- pre-commit autoupdate
- ```
-
- </details>
-
- <details>
- <summary><b>Set private environment variables in .env file</b></summary>
-
- System specific variables (e.g. absolute paths to datasets) should not be under version control or it will result in conflict between different users. Your private keys also shouldn't be versioned since you don't want them to be leaked.<br>
-
- Template contains `.env.example` file, which serves as an example. Create a new file called `.env` (this name is excluded from version control in .gitignore).
- You should use it for storing environment variables like this:
-
- ```
- MY_VAR=/home/user/my_system_path
- ```
-
- All variables from `.env` are loaded in `train.py` automatically.
-
- Hydra allows you to reference any env variable in `.yaml` configs like this:
-
- ```yaml
- path_to_data: ${oc.env:MY_VAR}
- ```
-
- </details>
-
- <details>
- <summary><b>Name metrics using '/' character</b></summary>
-
- Depending on which logger you're using, it's often useful to define metric name with `/` character:
-
- ```python
- self.log("train/loss", loss)
- ```
-
- This way loggers will treat your metrics as belonging to different sections, which helps to get them organised in UI.
-
- </details>
-
- <details>
- <summary><b>Use torchmetrics</b></summary>
-
- Use official [torchmetrics](https://github.com/PytorchLightning/metrics) library to ensure proper calculation of metrics. This is especially important for multi-GPU training!
-
- For example, instead of calculating accuracy by yourself, you should use the provided `Accuracy` class like this:
-
- ```python
- from torchmetrics.classification.accuracy import Accuracy
-
-
- class LitModel(LightningModule):
-     def __init__(self)
-         self.train_acc = Accuracy()
-         self.val_acc = Accuracy()
-
-     def training_step(self, batch, batch_idx):
-         ...
-         acc = self.train_acc(predictions, targets)
-         self.log("train/acc", acc)
-         ...
-
-     def validation_step(self, batch, batch_idx):
-         ...
-         acc = self.val_acc(predictions, targets)
-         self.log("val/acc", acc)
-         ...
- ```
-
- Make sure to use different metric instance for each step to ensure proper value reduction over all GPU processes.
-
- Torchmetrics provides metrics for most use cases, like F1 score or confusion matrix. Read [documentation](https://torchmetrics.readthedocs.io/en/latest/#more-reading) for more.
-
- </details>
-
- <details>
- <summary><b>Follow PyTorch Lightning style guide</b></summary>
-
- The style guide is available [here](https://pytorch-lightning.readthedocs.io/en/latest/starter/style_guide.html).<br>
-
- 1. Be explicit in your init. Try to define all the relevant defaults so that the user doesn’t have to guess. Provide type hints. This way your module is reusable across projects!
-
-    ```python
-    class LitModel(LightningModule):
-        def __init__(self, layer_size: int = 256, lr: float = 0.001):
-    ```
-
- 2. Preserve the recommended method order.
-
-    ```python
-    class LitModel(LightningModule):
-
-        def __init__():
-            ...
-
-        def forward():
-            ...
-
-        def training_step():
-            ...
-
-        def training_step_end():
-            ...
-
-        def on_train_epoch_end():
-            ...
-
-        def validation_step():
-            ...
-
-        def validation_step_end():
-            ...
-
-        def on_validation_epoch_end():
-            ...
-
-        def test_step():
-            ...
-
-        def test_step_end():
-            ...
-
-        def on_test_epoch_end():
-            ...
-
-        def configure_optimizers():
-            ...
-
-        def any_extra_hook():
-            ...
-    ```
-
- </details>
-
- <details>
- <summary><b>Version control your data and models with DVC</b></summary>
-
- Use [DVC](https://dvc.org) to version control big files, like your data or trained ML models.<br>
- To initialize the dvc repository:
-
- ```bash
- dvc init
- ```
-
- To start tracking a file or directory, use `dvc add`:
-
- ```bash
- dvc add data/MNIST
- ```
-
- DVC stores information about the added file (or a directory) in a special .dvc file named data/MNIST.dvc, a small text file with a human-readable format. This file can be easily versioned like source code with Git, as a placeholder for the original data:
-
- ```bash
- git add data/MNIST.dvc data/.gitignore
- git commit -m "Add raw data"
- ```
-
- </details>
-
- <details>
- <summary><b>Support installing project as a package</b></summary>
-
- It allows other people to easily use your modules in their own projects.
- Change name of the `src` folder to your project name and complete the `setup.py` file.
-
- Now your project can be installed from local files:
-
- ```bash
- pip install -e .
- ```
-
- Or directly from git repository:
-
- ```bash
- pip install git+git://github.com/YourGithubName/your-repo-name.git --upgrade
- ```
-
- So any file can be easily imported into any other file like so:
-
- ```python
- from project_name.models.mnist_module import MNISTLitModule
- from project_name.data.mnist_datamodule import MNISTDataModule
- ```
-
- </details>
-
- <details>
- <summary><b>Keep local configs out of code versioning</b></summary>
-
- Some configurations are user/machine/installation specific (e.g. configuration of local cluster, or harddrive paths on a specific machine). For such scenarios, a file [configs/local/default.yaml](configs/local/) can be created which is automatically loaded but not tracked by Git.
-
- For example, you can use it for a SLURM cluster config:
-
- ```yaml
- # @package _global_
-
- defaults:
-   - override /hydra/launcher@_here_: submitit_slurm
-
- data_dir: /mnt/scratch/data/
-
- hydra:
-   launcher:
-     timeout_min: 1440
-     gpus_per_task: 1
-     gres: gpu:1
-   job:
-     env_set:
-       MY_VAR: /home/user/my/system/path
-       MY_KEY: asdgjhawi8y23ihsghsueity23ihwd
- ```
-
- </details>
-
- <br>
-
- ## Resources
-
- This template was inspired by:
-
- - [PyTorchLightning/deep-learning-project-template](https://github.com/PyTorchLightning/deep-learning-project-template)
- - [drivendata/cookiecutter-data-science](https://github.com/drivendata/cookiecutter-data-science)
- - [lucmos/nn-template](https://github.com/lucmos/nn-template)
-
- Other useful repositories:
-
- - [jxpress/lightning-hydra-template-vertex-ai](https://github.com/jxpress/lightning-hydra-template-vertex-ai) - lightning-hydra-template integration with Vertex AI hyperparameter tuning and custom training job
-
- </details>
-
- <br>
-
  ## License

! Lightning-Hydra-Template is licensed under the MIT License.

! ```
! MIT License

! Copyright (c) 2021 ashleve
!
! Permission is hereby granted, free of charge, to any person obtaining a copy
! of this software and associated documentation files (the "Software"), to deal
! in the Software without restriction, including without limitation the rights
! to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
! copies of the Software, and to permit persons to whom the Software is
! furnished to do so, subject to the following conditions:
!
! The above copyright notice and this permission notice shall be included in all
! copies or substantial portions of the Software.
!
! THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
! IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
! FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
! AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
! LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
! OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
! SOFTWARE.
! ```
!
! <br>
! <br>
! <br>
! <br>
!
! **DELETE EVERYTHING ABOVE FOR YOUR PROJECT**
!
! ______________________________________________________________________
!
! <div align="center">
!
! # Your Project Name
!
! <a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
! <a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
! <a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
! <a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a><br>
! [![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)
! [![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/paper/2020)
!
! </div>
!
! ## Description
!
! What it does
!
! ## Installation
!
! #### Pip
!
! ```bash
! # clone project
! git clone https://github.com/YourGithubName/your-repo-name
! cd your-repo-name
!
! # [OPTIONAL] create conda environment
! conda create -n myenv python=3.9
! conda activate myenv
!
! # install pytorch according to instructions
! # https://pytorch.org/get-started/
!
! # install requirements
! pip install -r requirements.txt
! ```
!
! #### Conda
!
! ```bash
! # clone project
! git clone https://github.com/YourGithubName/your-repo-name
! cd your-repo-name
!
! # create conda environment and install dependencies
! conda env create -f environment.yaml -n myenv
!
! # activate conda environment
! conda activate myenv
! ```
!
! ## How to run
!
! Train model with default configuration
!
! ```bash
! # train on CPU
! python src/train.py trainer=cpu
!
! # train on GPU
! python src/train.py trainer=gpu
! ```
!
! Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/)
!
! ```bash
! python src/train.py experiment=experiment_name.yaml
! ```
!
! You can override any parameter from command line like this
!
! ```bash
! python src/train.py trainer.max_epochs=20 data.batch_size=64
! ```
--- 1,235 ----
! # AVIX: Audio-Visual Index eXtraction

! **Direct Gradient Neural Networks for Audio Synthesis Parameter Prediction**

  <div align="center">

  [![python](https://img.shields.io/badge/-Python_3.8_%7C_3.9_%7C_3.10-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
  [![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
  [![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
  [![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)

  </div>

! ## Overview

! AVIX (Audio-Visual Index eXtraction) is a novel framework for training neural networks to predict continuous audio synthesis parameters directly from visual representations. Unlike traditional approaches that rely on classification or ordinal regression with discrete bins, AVIX uses **Direct Gradient (DG)** optimization to predict normalized parameter vectors without quantization artifacts.

! ## Key Innovations

! ### 🎯 Direct Gradient Training
! - Networks output normalized parameters in [0,1] range as indices into actual parameter space
! - Eliminates quantization artifacts from discretization
! - Uniform gradient flow across parameters with different scales
! - Efficient 8-bit storage of ground truth labels

! ### 🔊 Just-Noticeable Difference (JND) Weighting
! - Perceptually-weighted loss functions for audio applications
! - Focuses learning on perceptually significant parameter variations
! - Improves convergence for audio synthesis tasks
! - Mean absolute errors below 2% for frequency and Q-factor prediction

! ### 🏗️ Architecture Support
! - **CNN**: Lightweight (~64K params), 10x faster training than transformers
! - **Vision Transformer**: State-of-the-art accuracy for complex mappings
! - **ConvNeXt-V2**: Modern CNN architecture with transformer-like performance
! - **DenseNet**: Efficient feature reuse for parameter regression

! ## Installation

  ```bash
! # Clone repository
! git clone https://github.com/josmithiii/avix.git
! cd avix

! # Create and activate virtual environment
! python -m venv .venv
! source .venv/bin/activate  # On Mac/Linux
! # or
! source .venv/bin/activate.csh  # For csh/tcsh shells

! # Install dependencies
  pip install -r requirements.txt
  ```

! ## Quick Start

! ### Training with AVIX Experiments

  ```bash
! # Train CNN with JND-weighted loss on 72-sample dataset
! make evimh  # or: python src/train.py experiment=avix_cnn_72dss_jnd

! # Train with Direct Gradient optimization
! python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg

! # Quick test (1 epoch, limited batches)
! make tq  # Tests SimpleDenseNet quickly
  ```

! ### Available Experiments

! The project includes 50+ pre-configured experiments:

! | Experiment | Architecture | Parameters | Features |
! |------------|-------------|------------|----------|
! | `avix_cnn_72dss_jnd` | CNN | ~64K | JND weighting |
! | `avix_densenet_72dss_64k_jnd_dg` | DenseNet | ~64K | Direct Gradient + JND |
! | `avix_vit_72dss_jnd` | ViT | ~38K | Vision Transformer |
! | `avix_cnn_72dss_jnd_dg_hybrid` | CNN | ~64K | Hybrid loss approach |

! ### Makefile Shortcuts

  ```bash
! # Training shortcuts
! make t        # Basic training
! make tmps     # Train with MPS (Mac GPU)
! make evimh    # VIMH CNN experiment
! make tq       # Quick test (1 epoch)

! # Architecture comparisons
! make ca       # Compare architectures (3 epochs each)
! make td       # Generate architecture diagrams
! make tdss     # Compare sample architectures

! # Testing and validation
! make test     # Run tests (excluding slow)
! make test-all # Run all tests
  ```

! ## Dataset Format (VIMH)

! AVIX uses the **Variable Image MultiHead (VIMH)** format for self-describing datasets:

! ### Structure
  ```
! data-vimh/
! ├── dataset_name/
! │   ├── images/           # Variable dimension images (32x32x3, 28x28x1, etc.)
! │   ├── labels.txt        # Format: [N] [param1_id] [param1_val] [param2_id] [param2_val]
! │   └── metadata.json     # Parameter mappings and dataset configuration
  ```

! ### Features
! - **Auto-configuration**: Models automatically configure from dataset metadata
! - **8-bit quantization**: Efficient parameter storage
! - **Cross-validation**: Consistency checks across directory, JSON, and binary sources
! - **Variable dimensions**: Supports different image sizes without code changes

! ## Configuration System

! AVIX extends Lightning-Hydra-Template with comprehensive Hydra configuration:

  ```yaml
! # Train with custom configuration
! python src/train.py \
!     model=avix_cnn_64k_jnd \
!     data=vimh_avix_72dss \
!     trainer=mps \
!     trainer.max_epochs=100
  ```

! ### Key Configurations

! - **Loss Functions**: `configs/model/losses/` - JND, Direct Gradient, hybrid approaches
! - **Models**: `configs/model/avix_*.yaml` - Architecture configurations
! - **Data**: `configs/data/vimh_*.yaml` - Dataset configurations
! - **Experiments**: `configs/experiment/avix_*.yaml` - Complete training recipes

! ## Project Structure

  ```
! avix/
! ├── configs/              # Hydra configuration files
! │   ├── experiment/       # Pre-configured experiments
! │   ├── model/           # Model architectures and losses
! │   └── data/            # Dataset configurations
! ├── src/
! │   ├── models/
! │   │   ├── avix_module.py      # AVIX encoder implementation
! │   │   ├── multihead_module.py # Multihead support
! │   │   └── losses.py           # JND and DG loss functions
! │   ├── data/
! │   │   └── vimh_datamodule.py  # VIMH dataset loader
! │   └── train.py                 # Main training script
! ├── tests/                # Comprehensive test suite
! ├── paper/                # Conference paper and abstract
! └── docs/                 # Additional documentation
  ```

! ## Performance

! ### Gradient Tracking
! Built-in gradient statistics for optimization analysis:
! - Signal-to-noise ratio tracking
! - Per-layer gradient norms
! - Stability metrics across epochs

  ```bash
! # View gradient statistics
! tensorboard --logdir logs/train/runs/
  ```

! ### Benchmarks
! - **Resonator parameter extraction**: <2% MAE for frequency/Q-factor
! - **Training speed**: CNN 10x faster than ViT with comparable accuracy
! - **Memory efficiency**: Optimized for MPS (Mac) and CUDA devices

! ## Development

! ### Running Tests
  ```bash
! # Quick tests
! make test

! # All tests including slow
! make test-all

! # Specific test file
! pytest tests/test_avix.py
  ```

! ### Code Quality
  ```bash
! # Format code
! make format

! # Run pre-commit hooks
  pre-commit run -a
  ```

! ## Citation

! If you use AVIX in your research, please cite:

! ```bibtex
! @inproceedings{smith2025avix,
!   title={AVIX: Audio-Visual Index eXtraction via Direct Gradient Neural Networks},
!   author={Smith III, Julius O.},
!   booktitle={Audio Developer Conference},
!   year={2025}
! }
  ```

! ## Extensions to Lightning-Hydra-Template

! This project extends the original [Lightning-Hydra-Template](https://github.com/ashleve/lightning-hydra-template) with:

! - **Multiple neural network architectures** with easy switching
! - **Multihead regression** support via VIMH dataset format
! - **Configurable loss functions** including JND and Direct Gradient
! - **50+ make targets** for streamlined development
! - **Gradient tracking** and stability analysis
! - **Backward compatibility** with original template

! See [docs/extensions.md](docs/extensions.md) for implementation details.

  ## License

! MIT License - see [LICENSE](LICENSE) file for details.

! ## Acknowledgments

! - Built on [Lightning-Hydra-Template](https://github.com/ashleve/lightning-hydra-template)
! - PyTorch Lightning and Hydra communities
! - Audio synthesis and computer vision research communities
Only in avix: THINGS_TO_TRY_NEXT.md
Only in avix: TODO.md
Only in avix: VIMH_CHANNEL_LABELS_PLAN.md
Only in avix: VIMH_NORMALIZATION_PLAN.md
Only in avix/archive: AVIX_PLAN_OPUS.html
Only in avix/archive: AVIX_PLAN_OPUS.md
Only in avix/archive: AVIX_PLAN_SONNET.html
Only in avix/archive: AVIX_PLAN_SONNET.md
Only in lightning-hydra-template-extended/archive: CONVNEXT_10M_PLAN.md
Only in lightning-hydra-template-extended/archive: CONVNEXT_PLAN.md
Only in lightning-hydra-template-extended/archive: CONVNEXT_PLAN_0.md
Only in avix/archive: FirstPromptAVIX.txt
Only in lightning-hydra-template-extended/archive: MULTIHEAD_PLAN.md
Only in lightning-hydra-template-extended/archive: README-EXTENSIONS-ORIGINAL.md
Only in lightning-hydra-template-extended/archive: README-PLAN-ORIGINAL.md
Only in lightning-hydra-template-extended/archive: REGRESSION_HEAD_PLAN.md
Only in lightning-hydra-template-extended/archive: REGRESSION_HEAD_PLAN.pdf
Only in lightning-hydra-template-extended/archive: SIZE_RENAME_PLAN.md
Only in avix/archive: SoftTargetsPlan.md
Only in lightning-hydra-template-extended/archive: TRAINING_EXAMPLE_PLAN.md
Only in avix/archive: VIMH_CHANNEL_LABELS_PLAN_0.md
Only in avix/archive: VIMH_NORMALIZATION_PLAN.md
Only in lightning-hydra-template-extended/archive: VIT_PLAN.md
Only in lightning-hydra-template-extended/archive: VIT_PLAN_0.md
Only in avix/archive: analyze_target_width.py
Only in lightning-hydra-template-extended/archive: cbq-log.md
Only in lightning-hydra-template-extended/archive: cbq.sh
Only in lightning-hydra-template-extended/archive: configs
Only in avix/archive: debug_avix_dataset.py
Only in avix/archive: debug_avix_targets.py
Only in avix/archive: debug_loss.py
Only in avix/archive: debug_normalization.py
Only in lightning-hydra-template-extended/archive: docs-TEMP-BACKUP
Only in lightning-hydra-template-extended/archive: lht0-diffs.txt
Only in lightning-hydra-template-extended/archive: multihead_data_details.md
Only in avix/archive: test_direct_gradient.py
Only in lightning-hydra-template-extended/archive: test_loss_comparison.py
Only in avix/archive: test_normalization.py
Only in lightning-hydra-template-extended/archive: test_param_configs.py
Only in lightning-hydra-template-extended/archive: trnm-log-2025-06-30.txt
Only in lightning-hydra-template-extended/archive: tve-log-2025-07-02.txt
Only in lightning-hydra-template-extended/archive: verify_convnext_10m.py
Only in avix/archive: verify_split_distribution.py
Only in lightning-hydra-template-extended/archive: vimh_lightning.md
Only in avix: asdn64-vs-dgn.txt
Only in avix: commit-1a6c52b-diffs.txt
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/callbacks/default.yaml avix/configs/callbacks/default.yaml
*** lightning-hydra-template-extended/configs/callbacks/default.yaml	Tue Jun 24 14:59:04 2025
--- avix/configs/callbacks/default.yaml	Sat Aug 23 20:34:22 2025
***************
*** 3,8 ****
--- 3,9 ----
    - early_stopping
    - model_summary
    - rich_progress_bar
+   - gradient_stats
    - _self_

  model_checkpoint:
Only in avix/configs/callbacks: default.yaml.orig
Only in avix/configs/callbacks: gradient_stats.yaml
Only in avix/configs/callbacks: grouped_progress_bar.yaml
Only in avix/configs/callbacks: grouped_progress_bar_with_gradients.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/callbacks/rich_progress_bar.yaml avix/configs/callbacks/rich_progress_bar.yaml
*** lightning-hydra-template-extended/configs/callbacks/rich_progress_bar.yaml	Tue Jun 24 14:59:04 2025
--- avix/configs/callbacks/rich_progress_bar.yaml	Sat Aug 23 20:33:35 2025
***************
*** 1,4 ****
! # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.RichProgressBar.html

  rich_progress_bar:
!   _target_: lightning.pytorch.callbacks.RichProgressBar
--- 1,5 ----
! # https://lightning.ai/docs/pytorch/latest/api/lightning.pytorch.callbacks.ProgressBar.html
! # Changed from RichProgressBar to default ProgressBar to fix Rich console issues

  rich_progress_bar:
!   _target_: lightning.pytorch.callbacks.ProgressBar
Only in avix/configs/data: avix_16kdss.yaml
Only in avix/configs/data: avix_16kdss.yaml.orig
Only in avix/configs/data: avix_72dss.yaml
Only in avix/configs/data: avix_72dss.yaml.orig
Only in avix/configs/data: avix_72dss_features.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/data/vimh.yaml avix/configs/data/vimh.yaml
*** lightning-hydra-template-extended/configs/data/vimh.yaml	Tue Jul 15 02:04:45 2025
--- avix/configs/data/vimh.yaml	Sat Aug 23 20:33:35 2025
***************
*** 1,6 ****
  _target_: src.data.vimh_datamodule.VIMHDataModule
! data_dir: ${paths.root_dir}/data-vimh/vimh-32x32_8000Hz_1p0s_256dss_resonarium_2p
  batch_size: 128 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
! num_workers: 2
  pin_memory: False
! persistent_workers: True
--- 1,6 ----
  _target_: src.data.vimh_datamodule.VIMHDataModule
! data_dir: ${paths.root_dir}/data-vimh/avix-vimh-32x32x3_8000Hz_1p0s_72dss_simple_2p_shuffled
  batch_size: 128 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
! num_workers: 0  # MPS doesn't support multiprocessing
  pin_memory: False
! persistent_workers: False
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/data/vimh_16kdss.yaml avix/configs/data/vimh_16kdss.yaml
*** lightning-hydra-template-extended/configs/data/vimh_16kdss.yaml	Tue Jul 15 23:50:08 2025
--- avix/configs/data/vimh_16kdss.yaml	Sat Aug 23 20:17:32 2025
***************
*** 1,7 ****
  _target_: src.data.vimh_datamodule.VIMHDataModule

  # Dataset directory
! data_dir: ${paths.root_dir}/data-vimh/vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p

  # Training parameters
  batch_size: 128
--- 1,7 ----
  _target_: src.data.vimh_datamodule.VIMHDataModule

  # Dataset directory
! data_dir: ${paths.root_dir}/data-vimh/avix-vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p

  # Training parameters
  batch_size: 128
Only in avix/configs/data: vimh_avix_72dss_64k_jnd_features_normalized.yaml
Only in avix/configs/data: vimh_avix_72dss_64k_jnd_normalized.yaml
Only in avix/configs/data: vimh_avix_72dss_64k_jnd_unnormalized.yaml
Only in avix/configs/experiment: avix_cnn_16kdss.yaml
Only in avix/configs/experiment: avix_cnn_16kdss.yaml.orig
Only in avix/configs/experiment: avix_cnn_72dss.yaml
Only in avix/configs/experiment: avix_cnn_72dss_64k_jnd.yaml
Only in avix/configs/experiment: avix_cnn_72dss_jnd.yaml
Only in avix/configs/experiment: avix_cnn_72dss_jnd_dg_hybrid.yaml
Only in avix/configs/experiment: avix_cnn_72dss_jnd_dg_tuned.yaml
Only in avix/configs/experiment: avix_convnext_16kdss.yaml
Only in avix/configs/experiment: avix_densenet_72dss_128k_jnd.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd_avix.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd_dg.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd_dg_features.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd_dg_fixed.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd_dg_normalized.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd_dg_normalized_precomputed.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd_dg_simple.yaml
Only in avix/configs/experiment: avix_densenet_72dss_64k_jnd_dg_unnormalized.yaml
Only in avix/configs/experiment: avix_densenet_72dss_jnd.yaml
Only in avix/configs/experiment: avix_densenet_72dss_jnd_dg_tuned.yaml
Only in avix/configs/experiment: avix_mlp_72dss_64k_jnd.yaml
Only in avix/configs/experiment: avix_mlp_72dss_jnd.yaml
Only in avix/configs/experiment: avix_quick.yaml
Only in avix/configs/experiment: avix_vit_16kdss.yaml
Only in avix/configs/experiment: avix_vit_72dss_64k_jnd.yaml
Only in avix/configs/experiment: avix_vit_72dss_jnd.yaml
Only in avix/configs/experiment: direct_gradient_training.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/extras/default.yaml avix/configs/extras/default.yaml
*** lightning-hydra-template-extended/configs/extras/default.yaml	Sat Jul 12 19:45:52 2025
--- avix/configs/extras/default.yaml	Sat Aug 23 20:33:35 2025
***************
*** 1,6 ****
--- 1,9 ----
  # disable python warnings if they annoy you
  ignore_warnings: False

+ # suppress specific DataLoader num_workers warnings (useful for MPS)
+ ignore_dataloader_warnings: True
+
  # ask user for tags if none are provided in the config
  enforce_tags: True

Only in avix/configs: generate_simple.yaml
Only in avix/configs/hparams_search: avix_architecture_comparison.yaml
Only in avix/configs/hparams_search: avix_architecture_comparison_64k.yaml
Only in avix/configs/hparams_search: avix_densenet_size_comparison.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/logger/tensorboard.yaml avix/configs/logger/tensorboard.yaml
*** lightning-hydra-template-extended/configs/logger/tensorboard.yaml	Tue Jun 24 14:59:04 2025
--- avix/configs/logger/tensorboard.yaml	Sat Aug 23 20:33:35 2025
***************
*** 4,10 ****
    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    save_dir: "${paths.output_dir}/tensorboard/"
    name: null
!   log_graph: False
    default_hp_metric: True
    prefix: ""
    # version: ""
--- 4,10 ----
    _target_: lightning.pytorch.loggers.tensorboard.TensorBoardLogger
    save_dir: "${paths.output_dir}/tensorboard/"
    name: null
!   log_graph: True   # Enable computational graph logging
    default_hp_metric: True
    prefix: ""
    # version: ""
Only in avix/configs/model: avix_cnn_11k.yaml
Only in avix/configs/model: avix_cnn_11k_jnd.yaml
Only in avix/configs/model: avix_cnn_64k.yaml
Only in avix/configs/model: avix_cnn_64k.yaml.orig
Only in avix/configs/model: avix_cnn_64k_jnd.yaml
Only in avix/configs/model: avix_cnn_64k_jnd_dg_hybrid.yaml
Only in avix/configs/model: avix_cnn_64k_jnd_dg_tuned.yaml
Only in avix/configs/model: avix_convnext_210k.yaml
Only in avix/configs/model: avix_densenet_11k_jnd.yaml
Only in avix/configs/model: avix_densenet_128k_jnd.yaml
Only in avix/configs/model: avix_densenet_64k_jnd.yaml
Only in avix/configs/model: avix_densenet_64k_jnd_dg.yaml
Only in avix/configs/model: avix_densenet_64k_jnd_dg_features.yaml
Only in avix/configs/model: avix_densenet_64k_jnd_dg_fixed.yaml
Only in avix/configs/model: avix_densenet_64k_jnd_dg_normalized.yaml
Only in avix/configs/model: avix_densenet_64k_jnd_dg_tuned.yaml
Only in avix/configs/model: avix_densenet_64k_jnd_dg_unnormalized.yaml
Only in avix/configs/model: avix_mlp_11k_jnd.yaml
Only in avix/configs/model: avix_mlp_64k_jnd.yaml
Only in avix/configs/model: avix_vit_11k_jnd.yaml
Only in avix/configs/model: avix_vit_210k.yaml
Only in avix/configs/model: avix_vit_64k_jnd.yaml
Only in avix/configs/model: direct_gradient_cnn.yaml
Only in avix/configs/model: direct_gradient_example.yaml
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/train.yaml avix/configs/train.yaml
*** lightning-hydra-template-extended/configs/train.yaml	Fri Jul  4 20:58:49 2025
--- avix/configs/train.yaml	Sat Aug 23 20:33:35 2025
***************
*** 7,13 ****
    - data: mnist
    - model: mnist_sdn_68k
    - callbacks: default
!   - logger: null # set logger here or use command line (e.g. `python train.py logger=tensorboard`)
    - trainer: default
    - paths: default
    - extras: default
--- 7,13 ----
    - data: mnist
    - model: mnist_sdn_68k
    - callbacks: default
!   - logger: tensorboard # tensorboard enabled by default for gradient tracking
    - trainer: default
    - paths: default
    - extras: default
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/configs/trainer/default.yaml avix/configs/trainer/default.yaml
*** lightning-hydra-template-extended/configs/trainer/default.yaml	Fri Jul  4 20:58:49 2025
--- avix/configs/trainer/default.yaml	Sat Aug 23 20:33:35 2025
***************
*** 14,19 ****
--- 14,22 ----
  # perform a validation loop every N training epochs
  check_val_every_n_epoch: 1

+ # log metrics every N steps for detailed tracking
+ log_every_n_steps: 10
+
  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: False
Only in avix/data: cifar-10-python.tar.gz
Only in lightning-hydra-template-extended/data: cifar-100-mh-example
Only in avix/data: cifar-100-python.tar.gz
Only in lightning-hydra-template-extended/data: resonarium_32x32
Only in lightning-hydra-template-extended/data: resonarium_32x32_binary
Only in lightning-hydra-template-extended/data: stk_32x32
Only in lightning-hydra-template-extended/data: stk_32x32_binary
Only in avix/data-vimh: avix-vimh-32x32x1_8000Hz_1p0s_72dss_simple_2p_shuffled
Only in lightning-hydra-template-extended/data-vimh: avix-vimh-32x32x3_8000Hz_1p0s_72dss_simple_2p
Only in avix/data-vimh: avix-vimh-32x32x3_8000Hz_1p0s_72dss_simple_2p_normalized_shuffled
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32_8000Hz_1p0s_256dss_resonarium_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32_8000Hz_1p0s_256dss_stk_1p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32x3_8000Hz_1p0s_16384dss_resonarium_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32x3_8000Hz_1p0s_16384dss_simple_1p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32x3_8000Hz_1p0s_16384dss_simple_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-32x32x3_8000Hz_1p0s_16384dss_stk_1p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32_8000Hz_1p0s_256dss_resonarium_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32_8000Hz_1p0s_256dss_stk_1p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32x3_8000Hz_1p0s_16384dss_resonarium_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32x3_8000Hz_1p0s_16384dss_simple_1p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32x3_8000Hz_1p0s_16384dss_simple_2p
Only in lightning-hydra-template-extended/data-vimh: vimh-el-32x32x3_8000Hz_1p0s_16384dss_stk_1p
Only in avix: dg-versus-dgnc.txt
Only in avix: diagrams
Only in avix: direct-gradient-runs.txt
Only in avix: display_vimh.py
Only in lightning-hydra-template-extended/docs: ADCxGather2025-contents.tex
Only in lightning-hydra-template-extended/docs: Abstract.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/architectures.md avix/docs/architectures.md
*** lightning-hydra-template-extended/docs/architectures.md	Mon Jul  7 23:27:09 2025
--- avix/docs/architectures.md	Sat Aug 23 20:33:35 2025
***************
*** 17,23 ****
  ## 📐 Architecture Details

  ### SimpleDenseNet (Original)
! **Type**: Fully-connected neural network
  **Best for**: Quick prototyping and baseline comparisons

  **Architecture**:
--- 17,25 ----
  ## 📐 Architecture Details

  ### SimpleDenseNet (Original)
! **Type**: Multi-Layer Perceptron (MLP) / Fully-connected neural network
! **Note**: Despite the name "DenseNet", this is NOT the famous DenseNet CNN architecture. It's a simple MLP with BatchNorm layers.
! **Also Available**: `SimpleMLP` - identical architecture without BatchNorm (for batch_size=1)
  **Best for**: Quick prototyping and baseline comparisons

  **Architecture**:
***************
*** 66,71 ****
--- 68,87 ----
  - **Primary head**: Digit classification (10 classes)
  - **Secondary heads**: Thickness (5 classes), Smoothness (3 classes)

+ **Auxiliary Feature Support**:
+ The SimpleCNN architecture supports hybrid CNN+scalar inputs via auxiliary features:
+ - **CNN branch**: Processes image spectrograms through convolutional layers
+ - **Auxiliary MLP branch**: Processes scalar features through separate neural network
+ - **Feature fusion**: Concatenates CNN and MLP features before classification heads
+ - **Use cases**: Audio synthesis parameters, sensor readings, metadata features
+
+ **Auxiliary Features Available**:
+ - `decay_time`: Extracts 4 temporal characteristics from spectrograms:
+   - `log10_decay_time`: Exponential decay time constant (log scale)
+   - `temporal_centroid`: Temporal center of mass
+   - `temporal_spread`: Temporal standard deviation
+   - `peak_to_end_ratio`: Ratio of peak to final energy
+
  **Usage**:
  ```bash
  # Single-head CNN
***************
*** 75,80 ****
--- 91,100 ----
  # Multihead CNN
  python src/train.py experiment=multihead_cnn_mnist
  make emhcm
+
+ # Hybrid CNN with auxiliary features (AVIX synthesizer parameters)
+ python src/train.py experiment=avix_cnn_72dss_jnd_dg_hybrid
+ make dgch

  # Different sizes
  python src/train.py model=mnist_cnn_8k     # 8K parameters
Only in avix/docs: avix.md
Only in avix/docs: avix_dataset.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/configuration.md avix/docs/configuration.md
*** lightning-hydra-template-extended/docs/configuration.md	Mon Jul  7 23:27:09 2025
--- avix/docs/configuration.md	Sat Aug 23 20:33:35 2025
***************
*** 95,100 ****
--- 95,142 ----
  compile: false
  ```

+ **Vision Transformer Implementation Choice**:
+ The Vision Transformer supports two implementation modes via the `use_torch_layers` parameter:
+
+ ```yaml
+ # Educational "from scratch" implementation (default)
+ net:
+   _target_: src.models.components.vision_transformer.VisionTransformer
+   use_torch_layers: false  # Custom attention, encoder blocks
+
+ # Production PyTorch optimized implementation
+ net:
+   _target_: src.models.components.vision_transformer.VisionTransformer
+   use_torch_layers: true   # nn.TransformerEncoder layers
+ ```
+
+ | Implementation | Use Case | Performance | Education Value |
+ |----------------|----------|-------------|-----------------|
+ | `false` (custom) | Learning, experimentation | Slower | High - see all components |
+ | `true` (PyTorch) | Production, benchmarks | Faster | Low - black box |
+
+ **Example Configurations**:
+ ```yaml
+ # configs/model/mnist_vit_38k.yaml (custom implementation)
+ net:
+   _target_: src.models.components.vision_transformer.VisionTransformer
+   # use_torch_layers defaults to false
+
+ # configs/model/mnist_vit_pytorch.yaml (PyTorch implementation)
+ net:
+   _target_: src.models.components.vision_transformer.VisionTransformer
+   use_torch_layers: true
+ ```
+
+ **Command-line Override**:
+ ```bash
+ # Switch to PyTorch implementation
+ python src/train.py model=mnist_vit_38k model.net.use_torch_layers=true
+
+ # Switch to custom implementation
+ python src/train.py model=mnist_vit_pytorch model.net.use_torch_layers=false
+ ```
+
  ### 3. Multihead Configuration

  **Multihead Model Pattern**:
***************
*** 333,338 ****
--- 375,423 ----
  ls logs/train/runs/ | grep "mnist.*cnn"
  ls logs/train/runs/ | grep "baseline.*v1"
  ```
+
+ ## 🎯 VIMH Advanced Features
+
+ ### Soft Target Support
+
+ The VIMH dataset format supports soft target training to reduce sensitivity to quantization boundary artifacts. This is particularly useful when quantized parameter values (0-255) are mapped to discrete class indices.
+
+ **Configuration**:
+ ```yaml
+ # configs/data/avix_16kdss.yaml
+ target_width: 1.0  # Standard deviation for soft targets (0.0 = hard targets)
+ ```
+
+ **Usage Examples**:
+ ```bash
+ # Hard targets (backward compatible)
+ python src/train.py experiment=avix_cnn_16kdss data.target_width=0.0
+
+ # Soft targets with different softness levels
+ python src/train.py experiment=avix_cnn_16kdss data.target_width=0.5  # Mild softness
+ python src/train.py experiment=avix_cnn_16kdss data.target_width=1.0  # Standard softness
+ python src/train.py experiment=avix_cnn_16kdss data.target_width=2.0  # High softness
+ ```
+
+ **How It Works**:
+ - `target_width=0.0`: Traditional hard targets (class indices)
+ - `target_width>0.0`: Gaussian distributions centered on true class
+ - **Automatic loss selection**: Uses KL divergence for soft targets, CrossEntropy for hard targets
+ - **Metrics compatibility**: Automatically converts soft targets to hard targets for accuracy calculation
+
+ **Benefits**:
+ - **Reduced boundary sensitivity**: Smoother loss landscape around quantization boundaries
+ - **Better generalization**: Reduced overfitting to arbitrary discretization choices
+ - **Research flexibility**: Easy parameter tuning for different softness levels
+ - **Backward compatibility**: No changes needed for existing experiments
+
+ **Expected Results**:
+ ```
+ Hard targets (target_width=0.0): loss ≈ 5.3
+ Soft targets (target_width=1.0): loss ≈ 2.6 (48% reduction)
+ ```
+
+ The loss reduction demonstrates that soft targets successfully reduce penalty for predictions near the correct class, leading to more robust training.

  ## 🎨 Advanced Configuration Patterns

diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/features.md avix/docs/features.md
*** lightning-hydra-template-extended/docs/features.md	Tue Jul  8 00:33:03 2025
--- avix/docs/features.md	Sat Aug 23 20:33:35 2025
***************
*** 35,40 ****
--- 35,47 ----
  | **Vision Transformer** | 38K-821K | Transformer on image patches |
  | **EfficientNet** | 22K-7M | Highly efficient CNN architecture |

+ ### 3.1 Hybrid CNN + Auxiliary Features
+ SimpleCNN supports hybrid architectures combining visual and scalar inputs:
+ - **CNN branch**: Processes image spectrograms through convolutional layers
+ - **Auxiliary MLP branch**: Processes scalar features (temporal characteristics, metadata)
+ - **Feature fusion**: Combines both modalities for improved parameter estimation
+ - **Applications**: Audio synthesis, scientific computing, multimodal learning
+
  ### 4. Multihead Classification
  Single models can predict multiple related tasks simultaneously:
  - **Primary task**: Digit classification (0-9)
***************
*** 48,58 ****
  - Single-command execution
  - Standardized baselines

! ### 6. Enhanced Make Targets
  Convenient shortcuts for common tasks:
  - **Training**: `make train`, `make trc` (CNN), `make trcn` (ConvNeXt)
  - **Quick tests**: `make tq`, `make tqc`, `make tqcn`
  - **Benchmarks**: `make cb10c` (CIFAR-10), `make cbs` (full suite)


  ## 📊 Expected Performance
--- 55,81 ----
  - Single-command execution
  - Standardized baselines

! ### 6. VIMH Dataset Format
! Variable Image MultiHead (VIMH) is a self-describing dataset format for multihead training:
! - **Variable dimensions**: Any image size (height×width×channels)
! - **MultiHead ready**: 0-255 varying parameters per sample
! - **Self-describing**: Metadata embedded in each sample
! - **Efficient**: 8-bit quantization with ~100 perceptual resolution steps
!
! ### 7. AVIX Direct Parameter Optimization
! AutoVectorIndeX (AVIX) provides systematic parameter space exploration:
! - **Complete grid coverage**: Every possible parameter combination generated exactly once
! - **Direct gradient training**: Minimizes parameter index error vs. image reconstruction
! - **JND-based optimization**: Just Noticeable Difference tolerance for perceptual accuracy
! - **Multiple architectures**: MLP, CNN, DenseNet, ViT support
!
! ### 8. Enhanced Make Targets
  Convenient shortcuts for common tasks:
  - **Training**: `make train`, `make trc` (CNN), `make trcn` (ConvNeXt)
  - **Quick tests**: `make tq`, `make tqc`, `make tqcn`
  - **Benchmarks**: `make cb10c` (CIFAR-10), `make cbs` (full suite)
+ - **AVIX training**: `make av` (CNN), `make adn` (DenseNet), `make asdn64dg` (Direct Gradient)
+ - **Hybrid models**: `make dgch` (CNN + auxiliary features)


  ## 📊 Expected Performance
***************
*** 72,77 ****
--- 95,113 ----
  | CIFAR-100 | SimpleCNN | 55-70% |
  | CIFAR-100 | ConvNeXt | 70-80% |

+ ## 🔧 Troubleshooting & Fixes
+
+ ### Progress Bar Issues
+ The template includes a fix for Rich Progress Bar console issues:
+ - **Problem**: `RichProgressBar` causing `IndexError: pop from empty list`
+ - **Solution**: Automatic fallback to standard `ProgressBar` callback
+ - **Location**: `configs/callbacks/rich_progress_bar.yaml` uses `lightning.pytorch.callbacks.ProgressBar`
+
+ ### Common Solutions
+ - **MPS compatibility**: Use `trainer=mps` for Mac Metal Performance Shaders
+ - **Multi-worker issues**: Set `data.num_workers=0` for MPS training
+ - **Memory optimization**: Use smaller batch sizes with `data.batch_size=32`
+
  ## 🔗 Integration

  All original Lightning-Hydra template features remain fully functional:
***************
*** 79,95 ****
  - Hydra configuration system enhanced, not replaced
  - Lightning module structure preserved
  - Testing framework compatible
! - Logging and callbacks unchanged

  ## 📚 Documentation

  For detailed information, see:
  - **[architectures.md](architectures.md)** - Architecture details and comparisons
  - **[benchmarks.md](benchmarks.md)** - CIFAR benchmark system
  - **[multihead.md](multihead.md)** - Multihead classification
  - **[makefile.md](makefile.md)** - Complete make targets reference
  - **[configuration.md](configuration.md)** - Configuration patterns
  - **[development.md](development.md)** - Development and extension guide

  ## 🛠️ Common Usage Patterns

--- 115,134 ----
  - Hydra configuration system enhanced, not replaced
  - Lightning module structure preserved
  - Testing framework compatible
! - Logging and callbacks enhanced with robust fallbacks

  ## 📚 Documentation

  For detailed information, see:
+ - **[vimh.md](vimh.md)** - Variable Image MultiHead dataset format
+ - **[avix_dataset.md](avix_dataset.md)** - AVIX direct parameter optimization
  - **[architectures.md](architectures.md)** - Architecture details and comparisons
  - **[benchmarks.md](benchmarks.md)** - CIFAR benchmark system
  - **[multihead.md](multihead.md)** - Multihead classification
  - **[makefile.md](makefile.md)** - Complete make targets reference
  - **[configuration.md](configuration.md)** - Configuration patterns
  - **[development.md](development.md)** - Development and extension guide
+ - **[gradient_tracking.md](gradient_tracking.md)** - Gradient statistics and monitoring

  ## 🛠️ Common Usage Patterns

***************
*** 99,104 ****
--- 138,167 ----
  python src/train.py trainer.max_epochs=10                    # SimpleDenseNet
  python src/train.py model=mnist_cnn trainer.max_epochs=10    # SimpleCNN
  python src/train.py model=mnist_convnext_68k trainer.max_epochs=10  # ConvNeXt
+ ```
+
+ ### AVIX Direct Parameter Training
+ ```bash
+ # AVIX Direct Gradient Training
+ make asdn64dg                                   # DenseNet with direct gradients
+ make asdn64dgn                                  # DenseNet with spectral normalization
+ python src/train.py experiment=avix_densenet_72dss_64k_jnd_dg
+
+ # AVIX Architecture Comparison
+ make aac64                                      # Compare all ~64K architectures
+ python src/train.py -m hparams_search=avix_architecture_comparison_64k
+ ```
+
+ ### VIMH Dataset Generation
+ ```bash
+ # Generate VIMH datasets
+ make ss                                         # Small SimpleSynth dataset (256 samples)
+ make sl                                         # Large SimpleSynth dataset (16K samples)
+ make sa                                         # Complete AVIX dataset (all combinations)
+
+ # Custom VIMH generation
+ python generate_vimh.py --avix --pickle        # Complete grid with pickle format
+ python generate_vimh.py dataset.size=1000      # 1000 random samples
  ```

  ### Custom Configuration
Only in avix/docs: gradient_tracking.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/index.md avix/docs/index.md
*** lightning-hydra-template-extended/docs/index.md	Sun Aug 24 23:08:38 2025
--- avix/docs/index.md	Sat Aug 23 20:17:32 2025
***************
*** 6,17 ****

  ### 🚀 Getting Started
  - **[features.md](features.md)** - High-level overview and key features summary
- - **[tutorial_sequence.md](tutorial_sequence.md)** - From setup to advanced experiments (using `make` targets)

  ### 🏗️ Technical Details
  - **[architectures.md](architectures.md)** - Detailed architecture documentation, parameter comparisons, and usage guides
  - **[benchmarks.md](benchmarks.md)** - CIFAR benchmark system, expected performance, and automated testing
  - **[multihead.md](multihead.md)** - Multihead classification system, synthetic label generation, and multi-task learning

  ### 🛠️ Usage and Reference
  - **[makefile.md](makefile.md)** - Complete make targets reference with abbreviations and workflows
--- 6,17 ----

  ### 🚀 Getting Started
  - **[features.md](features.md)** - High-level overview and key features summary

  ### 🏗️ Technical Details
  - **[architectures.md](architectures.md)** - Detailed architecture documentation, parameter comparisons, and usage guides
  - **[benchmarks.md](benchmarks.md)** - CIFAR benchmark system, expected performance, and automated testing
  - **[multihead.md](multihead.md)** - Multihead classification system, synthetic label generation, and multi-task learning
+ - **[avix.md](avix.md)** - AVIX (AutoVectorIndeX) framework for direct parameter optimization with exhaustive datasets

  ### 🛠️ Usage and Reference
  - **[makefile.md](makefile.md)** - Complete make targets reference with abbreviations and workflows
***************
*** 31,36 ****
--- 31,37 ----
  - Understand the available architectures → [architectures.md](architectures.md)
  - Run CIFAR benchmarks → [benchmarks.md](benchmarks.md)
  - Use multihead classification → [multihead.md](multihead.md)
+ - Use AVIX direct parameter optimization → [avix.md](avix.md)
  - Find make commands → [makefile.md](makefile.md)
  - Configure experiments → [configuration.md](configuration.md)
  - Extend the template → [development.md](development.md)
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/makefile.md avix/docs/makefile.md
*** lightning-hydra-template-extended/docs/makefile.md	Sun Aug 24 23:14:47 2025
--- avix/docs/makefile.md	Thu Aug 14 16:42:32 2025
***************
*** 2,10 ****

  ## Overview

! The Makefile provides convenient shortcuts for common development
! tasks. All targets support both full names and abbreviations for
! efficient workflow.

  ## 📋 Complete Target Reference

--- 2,8 ----

  ## Overview

! The Makefile provides convenient shortcuts for common development tasks. All targets support both full names and abbreviations for efficient workflow.

  ## 📋 Complete Target Reference

Only in lightning-hydra-template-extended/docs: png
Only in lightning-hydra-template-extended/docs: tutorial_sequence.md
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/docs/vimh.md avix/docs/vimh.md
*** lightning-hydra-template-extended/docs/vimh.md	Sun Aug 24 20:19:39 2025
--- avix/docs/vimh.md	Sat Aug 23 20:33:35 2025
***************
*** 20,66 ****

  ### Binary Layout Per Sample
  ```
! Metadata: 6 bytes (height, width, channels) - three 16-bit unsigned integers, little-endian
  Label Data: 1 + 2N bytes
!   - Byte 0: N (number of varying parameters, 0-255)
!   - Bytes 1,2: param1_id (0-255), param1_val (0-255)
!   - Bytes 3,4: param2_id (0-255), param2_val (0-255)
    - ...
!   - Bytes 2N-1,2N: paramN_id (0-255), paramN_val (0-255)
! Image Data: height*width*channels bytes (raw pixel values, typically 0-255)
! Total size: 6 + 1 + 2N + height*width*channels bytes per sample
  ```

  ### Example: 2-Parameter Dataset (32x32x3 like CIFAR-100)
  ```
! Metadata: [32, 32, 3] (height=32, width=32, channels=3) - 6 bytes
! Labels: [2, 0, 191, 1, 127] (N=2 parameters) - 5 bytes
! Image: 3072 bytes (32*32*3 RGB spectrogram/image data)
! Total: 6 + 5 + 3072 = 3083 bytes per sample
  ```

  If note_number=51.5 (range 50-52) and note_velocity=81.0 (range 80-82):
  ```
  Complete sample: [32, 32, 3, 2, 0, 191, 1, 127, <3072 image bytes>]
!   Metadata: height=32, width=32, channels=3 (6 bytes)
!   Labels: N=2 parameters (1 byte)
!   param_0 (note_number): id=0, val=191 → dequantized to 51.5 (2 bytes)
!   param_1 (note_velocity): id=1, val=127 → dequantized to 81.0 (2 bytes)
!   Image data: 3072 bytes of RGB pixel values
  ```

  ### Example: VIMH Metadata for MNIST Images (28x28x1)
  ```
! Metadata: [28, 28, 1] (height=28, width=28, channels=1) - 6 bytes
! Labels: [1, 0, 128] (N=1 parameter) - 3 bytes
  Image: 784 bytes (28*28*1 grayscale)
! Total: 6 + 3 + 784 = 793 bytes per sample
!
! Sample breakdown:
!   Metadata: height=28, width=28, channels=1 (6 bytes)
!   Labels: N=1 parameter (1 byte)
!   param_0: id=0, val=128 (2 bytes) - could represent digit thickness/style
!   Image data: 784 bytes of grayscale pixel values
  ```

  ## Dataset Structure
--- 20,58 ----

  ### Binary Layout Per Sample
  ```
! Metadata: 6 bytes (height, width, channels) - three 16-bit fields
  Label Data: 1 + 2N bytes
!   - Byte 0: N (number of varying parameters)
!   - Bytes 1,2: param1_id, param1_val
!   - Bytes 3,4: param2_id, param2_val
    - ...
!   - Bytes 2N-1,2N: paramN_id, paramN_val
! Image Data: height*width*channels bytes
  ```

  ### Example: 2-Parameter Dataset (32x32x3 like CIFAR-100)
  ```
! Metadata: [32, 32, 3] (height=32, width=32, channels=3)
! Labels: [2, 0, 191, 1, 127] (N=2 parameters)
! Image: 3072 bytes (32*32*3 RGB spectrogram)
! Total: 3 + 5 + 3072 = 3080 bytes per sample
  ```

  If note_number=51.5 (range 50-52) and note_velocity=81.0 (range 80-82):
  ```
  Complete sample: [32, 32, 3, 2, 0, 191, 1, 127, <3072 image bytes>]
!   height=32, width=32, channels=3
!   N=2 parameters
!   param_0 (note_number): 191 → 51.5
!   param_1 (note_velocity): 127 → 81.0
  ```

  ### Example: VIMH Metadata for MNIST Images (28x28x1)
  ```
! Metadata: [28, 28, 1] (height=28, width=28, channels=1)
! Labels: [1, 0, 128] (N=1 parameter)
  Image: 784 bytes (28*28*1 grayscale)
! Total: 3 + 3 + 784 = 790 bytes per sample
  ```

  ## Dataset Structure
***************
*** 68,100 ****
  Each VIMH dataset includes binary or pickle format, or both:

  ```
! data-vimh/vimh-32x32x3_8000Hz_1p0s_256dss_resonarium_2p/
! ├── train          # Binary training data
! ├── test           # Binary test data
! ├── train_batch    # Pickle training data
! ├── test_batch     # Pickle test data
! └── vimh_dataset_info.json  # Dataset information
  ```

  ### Dataset Info JSON Example
  ```json
  {
    "format": "VIMH",
!   "version": "1.0",
    "height": 32,
    "width": 32,
    "channels": 3,
    "varying_parameters": 2,
!   "parameter_names": ["note_number", "note_velocity"],
    "label_encoding": {
!     "format": "[height] [width] [channels] [N] [param1_id] [param1_val] [param2_id] [param2_val] ...",
!     "metadata_bytes": 3,
!     "N_range": [0, 255],
!     "param_id_range": [0, 255],
!     "param_val_range": [0, 255]
    },
!   "parameter_mappings": { /* full parameter info */ }
! }
  ```

  ## Benefits
--- 60,165 ----
  Each VIMH dataset includes binary or pickle format, or both:

  ```
! > tree data-vimh/
! data-vimh/
! └── vimh-32x32x3_8000Hz_1p0s_256dss_simple_2p
!     ├── test
!     ├── test_batch
!     ├── train
!     ├── train_batch
!     └── vimh_dataset_info.json
  ```

  ### Dataset Info JSON Example
+
+ In this example, the image is a _spectrogram_, and the parameters are _synthesis parameters_.
+
  ```json
  {
      "format": "VIMH",
!     "version": "2.1",
!     "dataset_name": "vimh-32x32x3_8000Hz_1p0s_256dss_simple_2p",
!     "output_format": "both",
      "height": 32,
      "width": 32,
      "channels": 3,
+     "channel_labels": ["R", "G", "B"],
      "varying_parameters": 2,
!     "parameter_names": [
!         "note_velocity",
!         "log10_decay_time"
!     ],
!     "train_samples": 204,
!     "test_samples": 52,
!     "total_samples": 256,
!     "n_samples": 256,
!     "sample_rate": 8000,
!     "duration": 1.0,
!     "synth_type": "simple",
!     "image_size": "32x32x3",
!     "parameter_mappings": {
!         "note_number": {
!             "min": 43.35,
!             "step": 0.0,
!             "max": 43.35,
!             "description": "simple parameter: note_number"
!         },
!         "note_velocity": {
!             "min": 126.0,
!             "step": 0.5,
!             "max": 127.0,
!             "description": "simple parameter: note_velocity"
!         },
!         "log10_decay_time": {
!             "min": -2.0,
!             "step": 0.1,
!             "max": 0.3,
!             "description": "simple parameter: log10_decay_time"
!         }
!     },
!     "fixed_parameters": {
!         "note_number": {
!             "value": 43.35,
!             "description": "MIDI note number (fixed at 100 Hz)"
!         }
!     },
!     "pre_emphasis_coefficient": 0.0,
!     "spectrogram_config": {
!         "sample_rate": 8000,
!         "type": "stft",
!         "n_fft": 80,
!         "n_window": 80,
!         "hop_length": 80,
!         "window_type": "rectangular",
!         "bins_per_harmonic": 1.0,
!         "n_bins": 32,
!         "method": "efficient_leaf"
!     },
!     "mel_config": {
!         "freq_min": 40.0,
!         "freq_max_ratio": 0.9
!     },
      "label_encoding": {
!         "format": "[height] [width] [channels] [spec_min] [spec_max] [N] [param1_id] [param1_val] [param2_id] [param2_val] ...",
!         "metadata_bytes": 14,
!         "scale_factors": {
!             "spec_min": "float32 - minimum dB value before normalization",
!             "spec_max": "float32 - maximum dB value before normalization"
          },
!         "N_range": [
!             0,
!             255
!         ],
!         "param_id_range": [
!             0,
!             255
!         ],
!         "param_val_range": [
!             0,
!             255
!         ]
!     }
! }
  ```

  ## Benefits
***************
*** 105,110 ****
--- 170,176 ----
  - **Efficient**: 8-bit quantization provides ~100 perceptual resolution steps
  - **Multihead CNN ready**: Enables training CNNs with multiple output heads
  - **Format flexible**: Supports spectrograms (32x32x3), MNIST (28x28x1), and custom sizes
+ - **Self-documenting**: Channel labels describe what each channel represents (v2.1+)

  ## Use Cases

***************
*** 131,140 ****

  ### Limitations
  - Maximum 255 varying parameters per sample
! - Parameter values quantized to 0-255 range (8-bit resolution)
! - Image dimensions limited to 65,535×65,535 (16-bit metadata fields)
! - Maximum 65,535 channels per image (16-bit metadata field)
! - Parameter precision: ~100 perceptual steps (8-bit quantization)

  ## Makefile Targets

--- 197,205 ----

  ### Limitations
  - Maximum 255 varying parameters per sample
! - Parameter values quantized to 0-255 range
! - Image dimensions limited to 255x255 (due to single byte metadata)
! - Maximum 255 channels per image

  ## Makefile Targets

***************
*** 166,222 ****
  actual_value = param_min + normalized_value * (param_max - param_min)
  ```

! ## Version History

! - **v2.0**: Added height, width, channels metadata for full generalization
! - **v1.0**: Initial implementation based on CIFAR-100 structure

! ## File Reading/Writing

! ### Python Usage
! ```python
! from src.data.vimh_dataset import VIMHDataset, VIMHDataModule

! # Load existing VIMH dataset
! dataset = VIMHDataset(data_dir="data-vimh/my_dataset", format="binary")

! # Access samples
! sample = dataset[0]  # Returns (image, labels) tuple
! image, labels = sample
! # image: torch.Tensor of shape (C, H, W)
! # labels: dict with parameter names and values

! # Use with DataModule for training
! datamodule = VIMHDataModule(data_dir="data-vimh/my_dataset")
! datamodule.setup()
! train_loader = datamodule.train_dataloader()
  ```

! ### Dataset Auto-Configuration
! VIMH datasets automatically configure neural network models based on their metadata:

! ```python
! # Model automatically configures from VIMH dataset info
! python src/train.py experiment=vimh_cnn_16kdss
! # Reads data-vimh/*/vimh_dataset_info.json to determine:
! # - Input image dimensions (height, width, channels)
! # - Number of output heads (equal to varying_parameters)
! # - Parameter names for head naming
! # - Loss function selection based on parameter types
  ```

! ### Performance Considerations
! - **Binary format**: Fastest loading, smallest file size
! - **Pickle format**: Python-friendly but larger files
! - **Memory usage**: ~793 bytes per MNIST sample, ~3083 bytes per CIFAR-100 sample
! - **Loading speed**: ~10x faster initialization compared to traditional formats

  ## Related Formats

  VIMH was originally inspired by CIFAR-100 but extends it significantly:
! - **CIFAR-100**: Fixed 32x32x3 images, 2 labels (coarse/fine classes)
! - **VIMH**: Variable image dimensions, 0-255 continuous parameters with self-describing metadata
! - **HDF5**: Alternative format, but VIMH is more compact and self-describing
! - **TFRecord**: Similar concept, but VIMH uses simpler binary format

! The format maintains some conceptual CIFAR-100 compatibility for 32x32x3 images while enabling much broader applications through its generalized, self-describing design.
--- 231,355 ----
  actual_value = param_min + normalized_value * (param_max - param_min)
  ```

! ## Channel Labels (v2.1+)

! VIMH v2.1 adds optional channel labels to document what each channel represents, enabling self-describing datasets and flexible channel selection in models.

! ### Basic Channel Labels

! Add a `channel_labels` array to document channel semantics:

! ```json
! {
!     "format": "VIMH",
!     "version": "2.1",
!     "channels": 3,
!     "channel_labels": ["R", "G", "B"]
! }
! ```

! ### Default Labels

! When `channel_labels` is not specified, standard defaults are used:
! - **1 channel**: `["Gray"]`
! - **3 channels**: `["R", "G", "B"]`
! - **4 channels**: `["R", "G", "B", "A"]`
! - **N channels**: `["Ch0", "Ch1", ..., "ChN-1"]`
!
! ### Advanced Channel Labels
!
! Channel labels can describe derived channels and their relationships:
!
! ```json
! {
!     "channels": 9,
!     "channel_labels": [
!         "R", "G", "B",
!         "R_temporal_envelope", "G_temporal_envelope", "B_temporal_envelope",
!         "R_spectral_envelope", "G_spectral_envelope", "B_spectral_envelope"
!     ]
! }
  ```

! **Implicit Grouping**: Labels with common prefixes form implicit groups:
! - `R*` channels: `["R", "R_temporal_envelope", "R_spectral_envelope"]`
! - `*_temporal_envelope` channels: `["R_temporal_envelope", "G_temporal_envelope", "B_temporal_envelope"]`

! ### Use Cases
!
! **Audio Spectrograms**:
! ```json
! "channel_labels": ["Left", "Right", "Mono"]
  ```

! **Scientific Data**:
! ```json
! "channel_labels": ["Visible", "Near_IR", "Far_IR", "Temperature", "Humidity"]
! ```

+ **Multi-Modal Vision**:
+ ```json
+ "channel_labels": ["RGB", "Depth", "Thermal", "Motion"]
+ ```
+
+ ### Model Integration
+
+ Models can select channels by name or pattern:
+
+ ```yaml
+ # Use specific channels
+ data:
+   channel_selection: ["R", "G", "B"]
+
+ # Use pattern matching
+ data:
+   channel_selection: ["*_temporal_envelope"]
+
+ # Use implicit groups
+ data:
+   channel_selection: ["R*"]  # All R-related channels
+ ```
+
+ ## Auxiliary Features
+
+ VIMH datasets can extract auxiliary scalar features from image data for hybrid CNN+MLP architectures:
+
+ ### Available Features
+ - **`decay_time`**: Extracts temporal decay characteristics from spectrograms:
+   - `log10_decay_time`: Exponential decay time constant (log scale)
+   - `temporal_centroid`: Temporal center of mass
+   - `temporal_spread`: Temporal standard deviation
+   - `peak_to_end_ratio`: Ratio of peak to final energy
+
+ ### Configuration
+ ```yaml
+ data:
+   auxiliary_features: ["decay_time"]  # Enable auxiliary feature extraction
+
+ model:
+   auxiliary_input_size: 4    # Number of auxiliary features (decay_time = 4)
+   auxiliary_hidden_size: 16  # Hidden layer size for auxiliary MLP
+ ```
+
+ ### Usage Example
+ ```bash
+ # Train hybrid CNN with auxiliary features
+ python src/train.py experiment=avix_cnn_72dss_jnd_dg_hybrid
+ make dgch
+ ```
+
+ The auxiliary features are extracted from the temporal envelope channel (typically channel 1) of normalized VIMH spectrograms and processed through a separate MLP branch before being fused with CNN features.
+
+ ## Version History
+
+ - **v2.1**: Added optional channel labels for self-describing datasets
+ - **v2.0**: Added height, width, channels metadata for full generalization
+ - **v1.0**: Initial implementation based on CIFAR-100 structure
+
  ## Related Formats

  VIMH was originally inspired by CIFAR-100 but extends it significantly:
! - **CIFAR-100**: Fixed 32x32x3 images, 2 labels (coarse/fine)
! - **VIMH**: Variable image dimensions, 0-255 parameters with metadata

! The format maintains some CIFAR-100 compatibility for 32x32x3 images while enabling much broader applications through its generalized design.
Only in lightning-hydra-template-extended/docs: viz
Only in avix: dv
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/examples/vimh_training.py avix/examples/vimh_training.py
*** lightning-hydra-template-extended/examples/vimh_training.py	Tue Jul 15 03:30:36 2025
--- avix/examples/vimh_training.py	Sat Aug 23 20:33:35 2025
***************
*** 252,258 ****
          logger=logger,
          accelerator="cpu",  # Use CPU to avoid MPS issues
          devices=1,
!         log_every_n_steps=10,
          val_check_interval=1.0,
          enable_progress_bar=True
      )
--- 252,258 ----
          logger=logger,
          accelerator="cpu",  # Use CPU to avoid MPS issues
          devices=1,
!         log_every_n_steps=4,
          val_check_interval=1.0,
          enable_progress_bar=True
      )
Only in avix: generate_vimh.py
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-34-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-43-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-43-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-49-11
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-49-48
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-51-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-53-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-54-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_19-55-52
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_20-00-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_20-01-02
Only in lightning-hydra-template-extended/logs/train/runs: 2025-06-30_20-06-18
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-00-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-01-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-02-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-02-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-03-52
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-04-56
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-12-09
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-13-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_04-14-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-01_16-32-28
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_02-34-12
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_02-34-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_13-43-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_13-46-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_14-15-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_14-16-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_14-18-49
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_20-23-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_20-23-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_20-23-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_20-29-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-37-49
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-37-51
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-37-53
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-37-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-44-31
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-44-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_21-46-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_22-31-48
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_22-37-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-13-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-23-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-48-31
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-48-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-49-09
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-49-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-49-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-49-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-02_23-52-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-03_00-55-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-03_02-15-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-03_02-17-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_18-13-44
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_19-14-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_19-15-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_20-16-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_20-26-31
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_20-27-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_21-57-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_21-59-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_22-25-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-04_22-25-38
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_18-50-20
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-00-38
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-01-42
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-02-02
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-27-46
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-28-02
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-29-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-32-50
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-37-52
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_19-41-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-28-18
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-35-59
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-36-19
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-36-51
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-37-12
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-47-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_20-53-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_21-09-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_21-12-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_21-15-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-05-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-10-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-23-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-25-22
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-30-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-46-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_22-59-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-31-18
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-31-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-31-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-31-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-36-55
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-05_23-52-52
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_00-05-25
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_00-37-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_00-37-37
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_00-50-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_01-03-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_01-16-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_02-09-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_02-11-35
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_02-45-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_03-02-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_03-31-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-02-20
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-03-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-04-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-04-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-06-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-49-13
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-06_17-49-48
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-07_22-56-48
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-25-16
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-25-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-26-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-28-40
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-28-56
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-30-09
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-30-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-35-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-45-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-45-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-50-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-52-10
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-08_20-57-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-10_03-48-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-10-44
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-11-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-15-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-18-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-26-38
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-35-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-41-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-43-40
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-48-05
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-49-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-55-25
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-56-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-56-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_19-58-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-13-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-14-28
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-21-41
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-21-59
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-22-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-22-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-23-21
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-25-21
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-27-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-28-49
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-29-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-29-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_20-29-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_21-48-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-11-12
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-22-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-24-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-43-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-12_22-56-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-19-55
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-20-08
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-20-22
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-20-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-20-51
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-23-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-30-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-31-34
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-37-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-38-19
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-38-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-38-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-39-02
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-39-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-41-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-44-35
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-47-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-51-13
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-52-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_00-53-49
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_01-03-11
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-14_01-05-16
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-31-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-31-32
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-31-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-32-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-32-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-32-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-33-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-57-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_00-57-39
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_02-22-54
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-13
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-41
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-05-56
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-06-11
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-08-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-11-33
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-14-56
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-18-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-19-35
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-21-00
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-27-19
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-27-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-28-10
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-28-37
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-28-57
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-32-09
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-34-15
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-47-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_03-55-08
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_04-29-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_04-30-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_05-48-41
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_05-48-59
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_05-54-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_06-00-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_09-53-28
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-13-07
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-13-17
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-13-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-14-26
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-14-36
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-18-13
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-26-27
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-29-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-29-37
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-31-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-33-47
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-35-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-36-07
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-36-23
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-36-29
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-36-41
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-37-01
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-37-18
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-41-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-42-04
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-42-53
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-48-53
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-49-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-49-58
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_10-52-03
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-46-35
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-47-43
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-49-24
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-52-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-52-46
Only in lightning-hydra-template-extended/logs/train/runs: 2025-07-15_23-54-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-45-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-46-06
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-46-22
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-46-30
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_16-46-45
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_19-26-42
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_19-33-20
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_19-42-07
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-15_22-56-44
Only in avix/logs/train/runs: 2025-08-23_18-07-19
Only in avix/logs/train/runs: 2025-08-23_18-19-46
Only in avix/logs/train/runs: 2025-08-23_18-23-09
Only in avix/logs/train/runs: 2025-08-23_18-27-13
Only in avix/logs/train/runs: 2025-08-23_18-34-41
Only in avix/logs/train/runs: 2025-08-23_18-41-05
Only in avix/logs/train/runs: 2025-08-23_18-41-46
Only in avix/logs/train/runs: 2025-08-23_18-43-06
Only in avix/logs/train/runs: 2025-08-23_18-45-59
Only in avix/logs/train/runs: 2025-08-23_18-47-56
Only in avix/logs/train/runs: 2025-08-23_19-11-10
Only in avix/logs/train/runs: 2025-08-23_19-11-40
Only in avix/logs/train/runs: 2025-08-23_19-15-16
Only in avix/logs/train/runs: 2025-08-23_19-16-46
Only in avix/logs/train/runs: 2025-08-23_19-23-27
Only in avix/logs/train/runs: 2025-08-23_19-42-00
Only in avix/logs/train/runs: 2025-08-23_19-45-10
Only in avix/logs/train/runs: 2025-08-23_19-46-57
Only in avix/logs/train/runs: 2025-08-23_19-47-38
Only in avix/logs/train/runs: 2025-08-23_19-48-27
Only in avix/logs/train/runs: 2025-08-23_19-49-03
Only in avix/logs/train/runs: 2025-08-23_19-59-55
Only in avix/logs/train/runs: 2025-08-23_20-04-14
Only in avix/logs/train/runs: 2025-08-23_21-52-14
Only in lightning-hydra-template-extended/logs/train/runs: 2025-08-24_23-12-00
Only in lightning-hydra-template-extended/logs: vimh_training
Only in avix: ls
Only in avix: make
Only in avix: outputs
Only in avix: paper
Only in avix: shuffle_vimh_dataset.py
Only in avix/src/data: auxiliary_features.py
Only in avix/src/data: avix_datamodule.py
Only in avix/src/data: avix_dataset.py
Only in avix/src/data: dataset_wrapper.py
Only in avix/src/data: flexible_transforms.py
Only in lightning-hydra-template-extended/src/data: obsolete
Only in avix/src/data: spectrogram_features.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/data/vimh_datamodule.py avix/src/data/vimh_datamodule.py
*** lightning-hydra-template-extended/src/data/vimh_datamodule.py	Thu Aug 14 16:42:32 2025
--- avix/src/data/vimh_datamodule.py	Sat Aug 23 20:33:35 2025
***************
*** 3,14 ****
  import json
  from pathlib import Path
  from lightning import LightningDataModule
! from torch.utils.data import DataLoader, Dataset
  from torchvision.transforms import transforms

  from .vimh_dataset import VIMHDataset, create_vimh_datasets


  class VIMHDataModule(LightningDataModule):
      """`LightningDataModule` for the VIMH (Variable Image MultiHead) dataset.

--- 3,17 ----
  import json
  from pathlib import Path
  from lightning import LightningDataModule
! from torch.utils.data import DataLoader, Dataset, random_split
  from torchvision.transforms import transforms

  from .vimh_dataset import VIMHDataset, create_vimh_datasets
+ from src.utils import pylogger

+ log = pylogger.RankedLogger(__name__, rank_zero_only=True)

+
  class VIMHDataModule(LightningDataModule):
      """`LightningDataModule` for the VIMH (Variable Image MultiHead) dataset.

***************
*** 68,73 ****
--- 71,83 ----
          train_transform: Optional[transforms.Compose] = None,
          val_transform: Optional[transforms.Compose] = None,
          test_transform: Optional[transforms.Compose] = None,
+         target_width: float = 0.0,
+         train_val_test_split: Tuple[float, float, float] = (0.7, 0.2, 0.1),
+         use_random_split: bool = True,
+         feature_types: Optional[List[str]] = None,
+         feature_config: Optional[Dict[str, Any]] = None,
+         auxiliary_features: Optional[List[str]] = None,
+         avix_training_mode: bool = False,
      ) -> None:
          """Initialize a `VIMHDataModule`.

***************
*** 79,84 ****
--- 89,103 ----
          :param train_transform: Optional transforms for training data.
          :param val_transform: Optional transforms for validation data.
          :param test_transform: Optional transforms for test data.
+         :param target_width: Standard deviation for soft targets (0.0 = hard targets). Defaults to `0.0`.
+         :param train_val_test_split: Train/validation/test split ratios. Defaults to `(0.7, 0.2, 0.1)`.
+         :param use_random_split: Whether to use random splitting instead of ordered train/test files. Defaults to `True`.
+         :param feature_types: List of feature types to extract and concatenate as additional channels.
+                             Options: ["log_spectral_gradient", "spectral_slope", "temporal_gradient"]
+         :param feature_config: Additional configuration for feature extraction.
+         :param auxiliary_features: List of auxiliary feature types to extract (e.g., ["decay_time"]).
+         :param avix_training_mode: If True, combine all available data into training set for complete parameter coverage.
+                                   Useful for AVIX datasets which exhaustively sample parameter space.
          """
          super().__init__()

***************
*** 317,325 ****

                      for param_name in param_names:
                          if param_name in param_mappings:
!                             # For continuous parameters, use 256 classes (0-255 quantization)
!                             heads_config[param_name] = 256

                  return heads_config

          except (FileNotFoundError, KeyError, json.JSONDecodeError):
--- 336,360 ----

                      for param_name in param_names:
                          if param_name in param_mappings:
!                             param_info = param_mappings[param_name]
!                             # Use centralized step-based calculation
!                             from ..utils.vimh_utils import calculate_logits_from_step
!                             num_logits = calculate_logits_from_step(param_info)

+                             # Log the calculation details
+                             if 'step' in param_info and param_info['step'] > 0:
+                                 min_val_raw = param_info['min']
+                                 max_val_raw = param_info['max']
+                                 step = param_info['step']
+                                 min_val = round(min_val_raw / step) * step
+                                 max_val = round(max_val_raw / step) * step
+                                 log.info(f"Parameter '{param_name}': Using step-based calculation - "
+                                          f"range[{min_val}, {max_val}], step={step} -> {num_logits} logits")
+                             else:
+                                 log.info(f"Parameter '{param_name}': Using default {num_logits} logits (no step specified)")
+
+                             heads_config[param_name] = num_logits
+
                  return heads_config

          except (FileNotFoundError, KeyError, json.JSONDecodeError):
***************
*** 388,407 ****

          return parameter_bounds

!     def _multihead_collate_fn(self, batch: List[Tuple[torch.Tensor, Dict[str, int]]]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
!         """Custom collate function for multihead labels.

!         Converts a batch of (image, labels_dict) pairs into batched tensors.

!         :param batch: List of (image_tensor, labels_dict) tuples
!         :return: Tuple of (batched_images, batched_labels_dict)
          """
          images = []
          labels_dict = {}

!         # Separate images and labels
!         for image, labels in batch:
              images.append(image)

              # Initialize label tensors on first iteration
              if not labels_dict:
--- 423,452 ----

          return parameter_bounds

!     def _multihead_collate_fn(self, batch) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], Optional[torch.Tensor]]:
!         """Custom collate function for multihead labels with auxiliary features.

!         Converts a batch of (image, labels_dict, auxiliary_features) into batched tensors.

!         :param batch: List of (image_tensor, labels_dict, auxiliary_features) tuples
!         :return: Tuple of (batched_images, batched_labels_dict, batched_auxiliary_features)
          """
          images = []
          labels_dict = {}
+         auxiliary_features = []

!         # Handle both old format (image, labels) and new format (image, labels, aux_features)
!         for item in batch:
!             if len(item) == 2:
!                 # Old format without auxiliary features
!                 image, labels = item
!                 aux_feat = None
!             else:
!                 # New format with auxiliary features
!                 image, labels, aux_feat = item
!
              images.append(image)
+             auxiliary_features.append(aux_feat)

              # Initialize label tensors on first iteration
              if not labels_dict:
***************
*** 418,427 ****
          # Convert label lists to tensors
          batched_labels = {}
          for head_name, label_list in labels_dict.items():
              batched_labels[head_name] = torch.tensor(label_list, dtype=torch.long)

!         return batched_images, batched_labels

      @property
      def num_classes(self) -> Dict[str, int]:
          """Get the number of classes for each head.
--- 463,486 ----
          # Convert label lists to tensors
          batched_labels = {}
          for head_name, label_list in labels_dict.items():
+             # Check if we have soft targets (tensors) or hard targets (integers)
+             if isinstance(label_list[0], torch.Tensor):
+                 # Soft targets - stack the probability distributions
+                 batched_labels[head_name] = torch.stack(label_list)
+             else:
+                 # Hard targets - convert integers to tensor
                  batched_labels[head_name] = torch.tensor(label_list, dtype=torch.long)

!         # Stack auxiliary features if any are present
!         batched_auxiliary_features = None
!         if any(feat is not None for feat in auxiliary_features):
!             # Filter out None values and stack
!             valid_features = [feat for feat in auxiliary_features if feat is not None]
!             if valid_features:
!                 batched_auxiliary_features = torch.stack(valid_features)

+         return batched_images, batched_labels, batched_auxiliary_features
+
      @property
      def num_classes(self) -> Dict[str, int]:
          """Get the number of classes for each head.
***************
*** 481,486 ****
--- 540,554 ----

          :param stage: The stage to setup. Either `"fit"`, `"validate"`, `"test"`, or `"predict"`. Defaults to ``None``.
          """
+         # Log dataset identification clearly
+         import os
+         dataset_name = os.path.basename(self.hparams.data_dir.rstrip('/'))
+         log.info(f"🗂️ Loading VIMH dataset: {dataset_name}")
+         log.info(f"   📁 Path: {self.hparams.data_dir}")
+         if self.hparams.avix_training_mode:
+             log.info(f"   🎯 Mode: AVIX training (combining all data for complete parameter coverage)")
+         else:
+             log.info(f"   📊 Mode: Standard train/test split")
          # Divide batch size by the number of devices.
          if self.trainer is not None:
              if self.hparams.batch_size % self.trainer.world_size != 0:
***************
*** 508,532 ****
                  # Adjust transforms based on detected image dimensions
                  self._adjust_transforms_for_image_size(height, width)

!                 # Now load datasets with correctly adjusted transforms
                  self.data_train = VIMHDataset(
                      self.hparams.data_dir,
                      train=True,
!                     transform=self.train_transform
                  )

                  self.data_test = VIMHDataset(
                      self.hparams.data_dir,
                      train=False,
!                     transform=self.test_transform
                  )

!                 # For validation, we'll use the test dataset with val transforms
!                 # In a real scenario, you might want to split the training data
                  self.data_val = VIMHDataset(
                      self.hparams.data_dir,
                      train=False,
!                     transform=self.val_transform
                  )

              except Exception as e:
--- 576,702 ----
                  # Adjust transforms based on detected image dimensions
                  self._adjust_transforms_for_image_size(height, width)

!                 # Load datasets based on splitting strategy
!                 if self.hparams.avix_training_mode:
!                     # AVIX mode: Combine all available data for training (complete parameter coverage)
!                     log.info("🎯 AVIX training mode: combining all data for complete parameter coverage")
!
!                     # Load both train and test datasets
!                     train_dataset = VIMHDataset(
!                         self.hparams.data_dir,
!                         train=True,
!                         transform=None,  # Apply transforms later
!                         target_width=self.hparams.target_width,
!                         feature_types=self.hparams.feature_types,
!                         feature_config=self.hparams.feature_config,
!                         auxiliary_features=self.hparams.auxiliary_features
!                     )
!
!                     try:
!                         test_dataset = VIMHDataset(
!                             self.hparams.data_dir,
!                             train=False,
!                             transform=None,
!                             target_width=self.hparams.target_width,
!                             feature_types=self.hparams.feature_types,
!                             feature_config=self.hparams.feature_config,
!                             auxiliary_features=self.hparams.auxiliary_features
!                         )
!
!                         # Combine train and test datasets
!                         from torch.utils.data import ConcatDataset
!                         combined_dataset = ConcatDataset([train_dataset, test_dataset])
!                         total_size = len(combined_dataset)
!                         log.info(f"Combined dataset: {len(train_dataset)} train + {len(test_dataset)} test = {total_size} total")
!
!                     except Exception as e:
!                         log.warning(f"Could not load test dataset ({e}), using train data only")
!                         combined_dataset = train_dataset
!                         total_size = len(combined_dataset)
!
!                     # Create small validation set (10%) from combined data, rest for training
!                     val_size = max(1, int(total_size * 0.1))  # At least 1 sample for validation
!                     train_size = total_size - val_size
!
!                     log.info(f"AVIX split: {total_size} samples -> train: {train_size}, val: {val_size} (using all data for training)")
!
!                     # Create split with fixed seed for reproducibility
!                     generator = torch.Generator().manual_seed(42)
!                     train_subset, val_subset = random_split(
!                         combined_dataset, [train_size, val_size], generator=generator
!                     )
!
!                     # Wrap datasets to apply different transforms
!                     from .dataset_wrapper import TransformWrapper
!                     self.data_train = TransformWrapper(train_subset, self.train_transform)
!                     self.data_val = TransformWrapper(val_subset, self.val_transform)
!                     self.data_test = None  # No test set in AVIX mode (all data used for training)
!
!                 elif self.hparams.use_random_split:
!                     # Load full dataset and split randomly
!                     full_dataset = VIMHDataset(
!                         self.hparams.data_dir,
!                         train=True,  # Load the full training dataset
!                         transform=None,  # Apply transforms later
!                         target_width=self.hparams.target_width,
!                         feature_types=self.hparams.feature_types,
!                         feature_config=self.hparams.feature_config,
!                         auxiliary_features=self.hparams.auxiliary_features
!                     )
!
!                     # Calculate split sizes
!                     total_size = len(full_dataset)
!                     train_ratio, val_ratio, test_ratio = self.hparams.train_val_test_split
!                     train_size = int(total_size * train_ratio)
!                     val_size = int(total_size * val_ratio)
!                     test_size = total_size - train_size - val_size  # Remainder to avoid rounding issues
!
!                     log.info(f"Random splitting dataset: {total_size} samples -> "
!                              f"train: {train_size}, val: {val_size}, test: {test_size}")
!
!                     # Create random splits with fixed seed for reproducibility
!                     generator = torch.Generator().manual_seed(42)
!                     train_dataset, val_dataset, test_dataset = random_split(
!                         full_dataset, [train_size, val_size, test_size], generator=generator
!                     )
!
!                     # Wrap datasets to apply different transforms
!                     from .dataset_wrapper import TransformWrapper
!                     self.data_train = TransformWrapper(train_dataset, self.train_transform)
!                     self.data_val = TransformWrapper(val_dataset, self.val_transform)
!                     self.data_test = TransformWrapper(test_dataset, self.test_transform)
!
!                 else:
!                     # Use original train/test file splitting (legacy behavior)
                      self.data_train = VIMHDataset(
                          self.hparams.data_dir,
                          train=True,
!                         transform=self.train_transform,
!                         target_width=self.hparams.target_width,
!                         feature_types=self.hparams.feature_types,
!                         feature_config=self.hparams.feature_config,
!                         auxiliary_features=self.hparams.auxiliary_features
                      )

                      self.data_test = VIMHDataset(
                          self.hparams.data_dir,
                          train=False,
!                         transform=self.test_transform,
!                         target_width=self.hparams.target_width,
!                         feature_types=self.hparams.feature_types,
!                         feature_config=self.hparams.feature_config,
!                         auxiliary_features=self.hparams.auxiliary_features
                      )

!                     # For validation, use the test dataset with val transforms
                      self.data_val = VIMHDataset(
                          self.hparams.data_dir,
                          train=False,
!                         transform=self.val_transform,
!                         target_width=self.hparams.target_width,
!                         feature_types=self.hparams.feature_types,
!                         feature_config=self.hparams.feature_config,
!                         auxiliary_features=self.hparams.auxiliary_features
                      )

              except Exception as e:
***************
*** 537,542 ****
--- 707,715 ----

          :return: The train dataloader.
          """
+         # For very small datasets, don't drop last batch
+         drop_last_batch = len(self.data_train) > self.batch_size_per_device * 2
+
          return DataLoader(
              dataset=self.data_train,
              batch_size=self.batch_size_per_device,
***************
*** 545,550 ****
--- 718,724 ----
              shuffle=True,
              persistent_workers=self.persistent_workers,
              collate_fn=self._multihead_collate_fn,
+             drop_last=drop_last_batch,  # Only drop last if dataset is large enough
          )

      def val_dataloader(self) -> DataLoader[Any]:
***************
*** 552,557 ****
--- 726,734 ----

          :return: The validation dataloader.
          """
+         # For very small datasets, don't drop last batch
+         drop_last_batch = len(self.data_val) > self.batch_size_per_device * 2
+
          return DataLoader(
              dataset=self.data_val,
              batch_size=self.batch_size_per_device,
***************
*** 560,565 ****
--- 737,743 ----
              shuffle=False,
              persistent_workers=self.persistent_workers,
              collate_fn=self._multihead_collate_fn,
+             drop_last=drop_last_batch,  # Only drop last if dataset is large enough
          )

      def test_dataloader(self) -> DataLoader[Any]:
***************
*** 567,580 ****

          :return: The test dataloader.
          """
          return DataLoader(
!             dataset=self.data_test,
              batch_size=self.batch_size_per_device,
              num_workers=self.hparams.num_workers,
              pin_memory=self.hparams.pin_memory,
              shuffle=False,
              persistent_workers=self.persistent_workers,
              collate_fn=self._multihead_collate_fn,
          )

      def predict_dataloader(self) -> DataLoader[Any]:
--- 745,766 ----

          :return: The test dataloader.
          """
+         # In AVIX mode, test set is None (all data used for training)
+         # Use validation set for testing instead
+         test_dataset = self.data_test if self.data_test is not None else self.data_val
+
+         # For very small datasets, don't drop last batch
+         drop_last_batch = len(test_dataset) > self.batch_size_per_device * 2
+
          return DataLoader(
!             dataset=test_dataset,
              batch_size=self.batch_size_per_device,
              num_workers=self.hparams.num_workers,
              pin_memory=self.hparams.pin_memory,
              shuffle=False,
              persistent_workers=self.persistent_workers,
              collate_fn=self._multihead_collate_fn,
+             drop_last=drop_last_batch,  # Only drop last if dataset is large enough
          )

      def predict_dataloader(self) -> DataLoader[Any]:
***************
*** 643,649 ****
          sys.path.insert(0, str(src_path))

      try:
!         print("Testing VIMH data module...")

          # Initialize data module
          dm = VIMHDataModule(
--- 829,835 ----
          sys.path.insert(0, str(src_path))

      try:
!         log.info("Testing VIMH data module...")

          # Initialize data module
          dm = VIMHDataModule(
***************
*** 655,678 ****
          # Setup the data module
          dm.setup()

!         print(f"✓ Data module setup successful")
!         print(f"✓ Dataset info: {dm.get_dataset_info()}")

          # Test data loaders
          train_loader = dm.train_dataloader()
          val_loader = dm.val_dataloader()
          test_loader = dm.test_dataloader()

!         print(f"✓ Created dataloaders - train: {len(train_loader)}, val: {len(val_loader)}, test: {len(test_loader)}")

          # Test a batch
          batch_images, batch_labels = next(iter(train_loader))
!         print(f"✓ Batch images shape: {batch_images.shape}")
!         print(f"✓ Batch labels: {[f'{k}: {v.shape}' for k, v in batch_labels.items()]}")

!         print("\nVIMH data module implementation successful!")

      except Exception as e:
!         print(f"✗ Error testing data module: {e}")
          import traceback
          traceback.print_exc()
--- 841,864 ----
          # Setup the data module
          dm.setup()

!         log.info(f"✓ Data module setup successful")
!         log.info(f"✓ Dataset info: {dm.get_dataset_info()}")

          # Test data loaders
          train_loader = dm.train_dataloader()
          val_loader = dm.val_dataloader()
          test_loader = dm.test_dataloader()

!         log.info(f"✓ Created dataloaders - train: {len(train_loader)}, val: {len(val_loader)}, test: {len(test_loader)}")

          # Test a batch
          batch_images, batch_labels = next(iter(train_loader))
!         log.info(f"✓ Batch images shape: {batch_images.shape}")
!         log.info(f"✓ Batch labels: {[f'{k}: {v.shape}' for k, v in batch_labels.items()]}")

!         log.info("\nVIMH data module implementation successful!")

      except Exception as e:
!         log.error(f"✗ Error testing data module: {e}")
          import traceback
          traceback.print_exc()
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/data/vimh_dataset.py avix/src/data/vimh_dataset.py
*** lightning-hydra-template-extended/src/data/vimh_dataset.py	Thu Aug 14 16:42:32 2025
--- avix/src/data/vimh_dataset.py	Sat Aug 23 20:33:35 2025
***************
*** 1,8 ****
  import json
  from pathlib import Path
! from typing import Dict, Any, Tuple, Optional
  import torch
  from .multihead_dataset_base import MultiheadDatasetBase


  class VIMHDataset(MultiheadDatasetBase):
--- 1,10 ----
  import json
  from pathlib import Path
! from typing import Dict, Any, Tuple, Optional, List
  import torch
  from .multihead_dataset_base import MultiheadDatasetBase
+ from .spectrogram_features import SpectrogramFeatureExtractor
+ from .auxiliary_features import extract_auxiliary_features


  class VIMHDataset(MultiheadDatasetBase):
***************
*** 23,29 ****
          data_path: str,
          train: bool = True,
          transform: Optional[callable] = None,
!         target_transform: Optional[callable] = None
      ):
          """Initialize VIMH dataset.

--- 25,35 ----
          data_path: str,
          train: bool = True,
          transform: Optional[callable] = None,
!         target_transform: Optional[callable] = None,
!         target_width: float = 0.0,
!         feature_types: Optional[List[str]] = None,
!         feature_config: Optional[Dict[str, Any]] = None,
!         auxiliary_features: Optional[List[str]] = None
      ):
          """Initialize VIMH dataset.

***************
*** 31,41 ****
--- 37,68 ----
          :param train: Whether to load training or test data
          :param transform: Optional transform to apply to images
          :param target_transform: Optional transform to apply to labels
+         :param target_width: Standard deviation for soft targets (0.0 = hard targets)
+         :param feature_types: List of feature types to extract and concatenate
+                               Options: ["log_spectral_gradient", "spectral_slope",
+                                        "temporal_gradient", etc.]
+         :param feature_config: Additional configuration for feature extraction
+         :param auxiliary_features: List of auxiliary feature types to extract (e.g., ["decay_time"])
          """
          self.train = train
          self.transform = transform
          self.target_transform = target_transform
+         self.target_width = target_width
+         self.auxiliary_features = auxiliary_features or []

+         # Setup feature extraction if requested
+         self.feature_extractor = None
+         # Create feature extractor if we have features OR normalization enabled
+         feature_config = feature_config or {}
+         if feature_types or feature_config.get('normalize_spectrogram', False):
+             self.feature_extractor = SpectrogramFeatureExtractor(
+                 feature_types=feature_types or [],
+                 **feature_config
+             )
+             # Track how many extra channels we're adding
+             # Will be computed during first sample access since input channels are unknown at init
+             self.num_feature_channels = None
+
          # Determine the correct file path
          data_path = Path(data_path)
          if data_path.is_dir():
***************
*** 101,109 ****
                  if param_name in param_mappings:
                      param_info = param_mappings[param_name]
                      if 'min' in param_info and 'max' in param_info:
!                         # For continuous parameters, use 256 classes (0-255 quantization)
!                         self.heads_config[param_name] = 256

      def _validate_dataset(self) -> None:
          """Validate dataset integrity and format compliance."""
          # Check file existence
--- 128,151 ----
                  if param_name in param_mappings:
                      param_info = param_mappings[param_name]
                      if 'min' in param_info and 'max' in param_info:
!                         # Use centralized step-based calculation
!                         from ..utils.vimh_utils import calculate_logits_from_step
!                         num_logits = calculate_logits_from_step(param_info)

+                         # Log the calculation details
+                         if 'step' in param_info and param_info['step'] > 0:
+                             min_val_raw = param_info['min']
+                             max_val_raw = param_info['max']
+                             step = param_info['step']
+                             min_val = round(min_val_raw / step) * step
+                             max_val = round(max_val_raw / step) * step
+                             print(f"Parameter '{param_name}': Using step-based calculation - "
+                                   f"range[{min_val}, {max_val}], step={step} -> {num_logits} logits")
+                         else:
+                             print(f"Parameter '{param_name}': Using default {num_logits} logits (no step specified)")
+
+                         self.heads_config[param_name] = num_logits
+
      def _validate_dataset(self) -> None:
          """Validate dataset integrity and format compliance."""
          # Check file existence
***************
*** 171,184 ****

          return metadata

!     def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, int]]:
          """Get a sample from the dataset.

          :param idx: Sample index
!         :return: Tuple of (image_tensor, labels_dict)
          """
          image, labels = super().__getitem__(idx)

          # Apply transforms if specified
          if self.transform is not None:
              image = self.transform(image)
--- 213,262 ----

          return metadata

!     def _create_soft_targets(self, class_index: int, num_classes: int, target_width: float) -> torch.Tensor:
!         """Create soft targets as Gaussian distribution around true class.
!
!         :param class_index: True class index
!         :param num_classes: Total number of classes
!         :param target_width: Standard deviation for soft targets
!         :return: Soft target distribution or hard target index
!         """
!         if target_width == 0.0:
!             return class_index  # Hard targets (backward compatible)
!
!         # Soft targets - Gaussian distribution
!         class_indices = torch.arange(num_classes, dtype=torch.float32)
!         distances = (class_indices - class_index) ** 2
!         weights = torch.exp(-distances / (2 * target_width ** 2))
!         return weights / weights.sum()  # Normalize to probability distribution
!
!     def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, int], Optional[torch.Tensor]]:
          """Get a sample from the dataset.

          :param idx: Sample index
!         :return: Tuple of (image_tensor, labels_dict, auxiliary_features)
          """
          image, labels = super().__getitem__(idx)

+         # Store the original image for auxiliary feature extraction (before transforms)
+         original_image = image.clone()
+
+         # Extract features and concatenate if enabled
+         if self.feature_extractor is not None:
+             # Compute feature channel count on first access if not already done
+             if self.num_feature_channels is None:
+                 n_input_channels = image.shape[0]  # C in [C, H, W]
+                 self.num_feature_channels = self.feature_extractor.get_num_output_channels(n_input_channels)
+
+             # Extract features (may return normalized spectrogram + features)
+             if self.feature_extractor.normalize_spectrogram:
+                 # Normalization mode: returns normalized_spec + envelope features concatenated
+                 image = self.feature_extractor.extract_features(image)
+             else:
+                 # Regular mode: extract features and concatenate
+                 features = self.feature_extractor.extract_features(image)
+                 image = torch.cat([image, features], dim=0)
+
          # Apply transforms if specified
          if self.transform is not None:
              image = self.transform(image)
***************
*** 186,193 ****
          if self.target_transform is not None:
              labels = self.target_transform(labels)

!         return image, labels

      def get_parameter_info(self, param_name: str) -> Dict[str, Any]:
          """Get detailed information about a specific parameter.

--- 264,362 ----
          if self.target_transform is not None:
              labels = self.target_transform(labels)

!         # Map quantized 0-255 labels to step-based class indices and apply soft targets
!         labels = self._map_labels_to_step_indices(labels)

+         # Extract auxiliary features if requested
+         auxiliary_features = None
+         if self.auxiliary_features:
+             # Create data dict for auxiliary feature extraction
+             data_dict = {'image': original_image}
+
+             # For decay_time features, we need temporal_envelope
+             # The temporal envelope is in the second channel of the normalized dataset
+             if 'decay_time' in self.auxiliary_features and original_image.shape[0] >= 2:
+                 # Extract temporal envelope (2nd channel) and format as [1, 1, H, W] (batch size 1)
+                 temporal_envelope = original_image[1:2].unsqueeze(0)  # [1, 1, H, W]
+                 data_dict['temporal_envelope'] = temporal_envelope
+
+                 # Extract auxiliary features and squeeze batch dimension since we're processing single samples
+                 batch_features = extract_auxiliary_features(data_dict, self.auxiliary_features)
+                 auxiliary_features = batch_features.squeeze(0)  # [num_features] instead of [1, num_features]
+             else:
+                 # Extract auxiliary features normally
+                 auxiliary_features = extract_auxiliary_features(data_dict, self.auxiliary_features)
+                 if auxiliary_features.dim() > 1 and auxiliary_features.shape[0] == 1:
+                     auxiliary_features = auxiliary_features.squeeze(0)
+
+         return image, labels, auxiliary_features
+
+     def _map_labels_to_step_indices(self, labels: Dict[str, int]) -> Dict[str, torch.Tensor]:
+         """Map 0-255 quantized labels to step-based class indices and apply soft targets.
+
+         :param labels: Dictionary of parameter names to 0-255 quantized values
+         :return: Dictionary of parameter names to step-based class indices or soft target distributions
+         """
+         mapped_labels = {}
+
+         if 'parameter_mappings' not in self.metadata_format:
+             # No step-based mapping available, apply soft targets to original labels if enabled
+             for param_name, quantized_value in labels.items():
+                 num_classes = self.heads_config.get(param_name, 256)  # Default to 256 classes
+                 mapped_labels[param_name] = self._create_soft_targets(quantized_value, num_classes, self.target_width)
+             return mapped_labels
+
+         param_mappings = self.metadata_format['parameter_mappings']
+
+         for param_name, quantized_value in labels.items():
+             if param_name in param_mappings:
+                 param_info = param_mappings[param_name]
+
+                 # Check if step-based calculation is available
+                 if 'min' in param_info and 'max' in param_info and 'step' in param_info:
+                     param_min = param_info['min']
+                     param_max = param_info['max']
+                     step = param_info['step']
+
+                     # Convert 0-255 quantized value back to actual parameter value
+                     actual_value = param_min + (quantized_value / 255.0) * (param_max - param_min)
+
+                     # Map actual value to step-based class index
+                     # Round min/max to nearest step multiples to avoid floating-point issues
+                     min_val = round(param_min / step) * step
+                     max_val = round(param_max / step) * step
+
+                     # Clamp actual value to valid range
+                     actual_value = max(min_val, min(actual_value, max_val))
+
+                     # Calculate step-based class index
+                     class_index = round((actual_value - min_val) / step)
+
+                     # Calculate total number of classes for bounds checking
+                     num_classes = int(round((max_val - min_val) / step)) + 1
+                     class_index = max(0, min(class_index, num_classes - 1))
+
+                     # Apply soft targets
+                     mapped_labels[param_name] = self._create_soft_targets(class_index, num_classes, self.target_width)
+                 else:
+                     # No step information, use original quantized value with soft targets if enabled
+                     num_classes = self.heads_config.get(param_name, 256)  # Default to 256 classes
+                     mapped_labels[param_name] = self._create_soft_targets(quantized_value, num_classes, self.target_width)
+             else:
+                 # Parameter not in mappings, use original value with soft targets if enabled
+                 num_classes = self.heads_config.get(param_name, 256)  # Default to 256 classes
+                 mapped_labels[param_name] = self._create_soft_targets(quantized_value, num_classes, self.target_width)
+
+         return mapped_labels
+
+     @property
+     def parameter_mappings(self) -> Optional[Dict[str, Any]]:
+         """Get parameter mappings from metadata format.
+
+         :return: Parameter mappings dictionary or None if not available
+         """
+         return self.metadata_format.get('parameter_mappings', None)
+
      def get_parameter_info(self, param_name: str) -> Dict[str, Any]:
          """Get detailed information about a specific parameter.

***************
*** 263,289 ****
  def create_vimh_datasets(
      data_dir: str,
      transform: Optional[callable] = None,
!     target_transform: Optional[callable] = None
  ) -> Tuple[VIMHDataset, VIMHDataset]:
      """Create train and test VIMH datasets.

      :param data_dir: Directory containing the dataset files
      :param transform: Optional transform to apply to images
      :param target_transform: Optional transform to apply to labels
      :return: Tuple of (train_dataset, test_dataset)
      """
      train_dataset = VIMHDataset(
          data_dir,
          train=True,
          transform=transform,
!         target_transform=target_transform
      )

      test_dataset = VIMHDataset(
          data_dir,
          train=False,
          transform=transform,
!         target_transform=target_transform
      )

      return train_dataset, test_dataset
--- 432,462 ----
  def create_vimh_datasets(
      data_dir: str,
      transform: Optional[callable] = None,
!     target_transform: Optional[callable] = None,
!     target_width: float = 0.0
  ) -> Tuple[VIMHDataset, VIMHDataset]:
      """Create train and test VIMH datasets.

      :param data_dir: Directory containing the dataset files
      :param transform: Optional transform to apply to images
      :param target_transform: Optional transform to apply to labels
+     :param target_width: Standard deviation for soft targets (0.0 = hard targets)
      :return: Tuple of (train_dataset, test_dataset)
      """
      train_dataset = VIMHDataset(
          data_dir,
          train=True,
          transform=transform,
!         target_transform=target_transform,
!         target_width=target_width
      )

      test_dataset = VIMHDataset(
          data_dir,
          train=False,
          transform=transform,
!         target_transform=target_transform,
!         target_width=target_width
      )

      return train_dataset, test_dataset
Only in avix/src/models: avix_module.py
Only in avix/src/models: avix_module.py.orig
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/components/simple_cnn.py avix/src/models/components/simple_cnn.py
*** lightning-hydra-template-extended/src/models/components/simple_cnn.py	Sun Aug 24 20:09:08 2025
--- avix/src/models/components/simple_cnn.py	Tue Aug 26 13:25:17 2025
***************
*** 18,23 ****
--- 18,25 ----
          output_mode: str = "classification",
          parameter_names: Optional[List[str]] = None,
          parameter_ranges: Optional[Dict[str, Tuple[float, float]]] = None,
+         auxiliary_input_size: int = 0,
+         auxiliary_hidden_size: int = 32,
      ) -> None:
          """Initialize a SimpleCNN module.

***************
*** 32,37 ****
--- 34,41 ----
          :param output_mode: Output mode - "classification" or "regression".
          :param parameter_names: List of parameter names for regression mode.
          :param parameter_ranges: Dict mapping parameter names to (min, max) ranges.
+         :param auxiliary_input_size: Size of auxiliary scalar input vector (0 = no aux input).
+         :param auxiliary_hidden_size: Hidden size for auxiliary input processing.
          """
          super().__init__()

***************
*** 39,55 ****
          self.output_mode = output_mode
          self.parameter_names = parameter_names or []
          self.parameter_ranges = parameter_ranges or {}

          # Handle configuration based on output mode
          if output_mode == "regression":
!             # For regression, we need parameter names
!             if not parameter_names:
!                 raise ValueError("parameter_names must be provided for regression mode")
              # Create heads_config for regression (each parameter gets 1 output)
              heads_config = {name: 1 for name in parameter_names}
          else:
              # Backward compatibility: convert old single-head config to multihead
!             if heads_config is None:
                  if output_size is not None:
                      heads_config = {'digit': output_size}
                  else:
--- 43,64 ----
          self.output_mode = output_mode
          self.parameter_names = parameter_names or []
          self.parameter_ranges = parameter_ranges or {}
+         self.auxiliary_input_size = auxiliary_input_size
+         self.auxiliary_hidden_size = auxiliary_hidden_size
+         self.fc_hidden = fc_hidden

          # Handle configuration based on output mode
          if output_mode == "regression":
!             # For regression, we need parameter names (can be empty if auto-configured later)
!             if parameter_names:
                  # Create heads_config for regression (each parameter gets 1 output)
                  heads_config = {name: 1 for name in parameter_names}
              else:
+                 # Will be auto-configured later from dataset
+                 heads_config = {}
+         else:
              # Backward compatibility: convert old single-head config to multihead
!             if heads_config is None or len(heads_config) == 0:
                  if output_size is not None:
                      heads_config = {'digit': output_size}
                  else:
***************
*** 97,109 ****
              nn.Dropout(dropout),
          )

          # Multiple heads or single head for backward compatibility
          if self.is_multihead:
              if output_mode == "regression":
                  # For regression, create heads with sigmoid activation
                  self.heads = nn.ModuleDict({
                      head_name: nn.Sequential(
!                         nn.Linear(fc_hidden, 1),
                          nn.Sigmoid()
                      )
                      for head_name in heads_config.keys()
--- 106,133 ----
              nn.Dropout(dropout),
          )

+         # Auxiliary input processing (if enabled)
+         if auxiliary_input_size > 0:
+             self.auxiliary_net = nn.Sequential(
+                 nn.Linear(auxiliary_input_size, auxiliary_hidden_size),
+                 nn.ReLU(),
+                 nn.Dropout(dropout / 2),  # Less dropout for auxiliary features
+                 nn.Linear(auxiliary_hidden_size, auxiliary_hidden_size),
+                 nn.ReLU(),
+             )
+             # Combined feature size includes auxiliary features
+             combined_feature_size = fc_hidden + auxiliary_hidden_size
+         else:
+             self.auxiliary_net = None
+             combined_feature_size = fc_hidden
+
          # Multiple heads or single head for backward compatibility
          if self.is_multihead:
              if output_mode == "regression":
                  # For regression, create heads with sigmoid activation
                  self.heads = nn.ModuleDict({
                      head_name: nn.Sequential(
!                         nn.Linear(combined_feature_size, 1),
                          nn.Sigmoid()
                      )
                      for head_name in heads_config.keys()
***************
*** 111,147 ****
              else:
                  # Classification heads
                  self.heads = nn.ModuleDict({
!                     head_name: nn.Linear(fc_hidden, num_classes)
                      for head_name, num_classes in heads_config.items()
                  })
          else:
              # Single head (backward compatibility)
              head_name, num_classes = next(iter(heads_config.items()))
              if output_mode == "regression":
                  self.classifier = nn.Sequential(
!                     nn.Linear(fc_hidden, 1),
                      nn.Sigmoid()
                  )
              else:
!                 self.classifier = nn.Linear(fc_hidden, num_classes)

!     def forward(self, x: torch.Tensor):
          """Perform a single forward pass through the network.

          :param x: Input tensor of shape (batch_size, channels, height, width).
          :return: A tensor of logits (single head) or dict of logits (multihead).
          """
          x = self.conv_layers(x)
          shared_features = self.shared_features(x)

          if self.is_multihead:
              return {
!                 head_name: head(shared_features)
                  for head_name, head in self.heads.items()
              }
          else:
              # Single head output (backward compatibility)
!             return self.classifier(shared_features)


  if __name__ == "__main__":
--- 135,207 ----
              else:
                  # Classification heads
                  self.heads = nn.ModuleDict({
!                     head_name: nn.Linear(combined_feature_size, num_classes)
                      for head_name, num_classes in heads_config.items()
                  })
          else:
              # Single head (backward compatibility)
+             if heads_config:
                  head_name, num_classes = next(iter(heads_config.items()))
+             else:
+                 # Default single head if heads_config is empty
+                 head_name, num_classes = 'default', 1
              if output_mode == "regression":
                  self.classifier = nn.Sequential(
!                     nn.Linear(combined_feature_size, 1),
                      nn.Sigmoid()
                  )
              else:
!                 self.classifier = nn.Linear(combined_feature_size, num_classes)

!     def _build_heads(self, heads_config: Dict[str, int]) -> None:
!         """Rebuild heads for auto-configuration (regression mode only)."""
!         if self.output_mode != "regression":
!             raise ValueError("_build_heads only supports regression mode")
!
!         # Calculate combined feature size (same as in __init__)
!         if self.auxiliary_input_size > 0:
!             combined_feature_size = self.fc_hidden + self.auxiliary_hidden_size
!         else:
!             combined_feature_size = self.fc_hidden
!
!         # Create regression heads with sigmoid activation
!         self.heads = nn.ModuleDict({
!             head_name: nn.Sequential(
!                 nn.Linear(combined_feature_size, 1),
!                 nn.Sigmoid()
!             )
!             for head_name in heads_config.keys()
!         })
!
!         # Update multihead status
!         self.is_multihead = len(heads_config) > 1
!
!     def forward(self, x: torch.Tensor, auxiliary: Optional[torch.Tensor] = None):
          """Perform a single forward pass through the network.

          :param x: Input tensor of shape (batch_size, channels, height, width).
+         :param auxiliary: Optional auxiliary input tensor of shape (batch_size, auxiliary_input_size).
          :return: A tensor of logits (single head) or dict of logits (multihead).
          """
+         # Process main input through CNN
          x = self.conv_layers(x)
          shared_features = self.shared_features(x)

+         # Combine with auxiliary features if provided
+         if self.auxiliary_net is not None and auxiliary is not None:
+             auxiliary_features = self.auxiliary_net(auxiliary)
+             combined_features = torch.cat([shared_features, auxiliary_features], dim=1)
+         else:
+             combined_features = shared_features
+
          if self.is_multihead:
              return {
!                 head_name: head(combined_features)
                  for head_name, head in self.heads.items()
              }
          else:
              # Single head output (backward compatibility)
!             return self.classifier(combined_features)


  if __name__ == "__main__":
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/components/simple_dense_net.py avix/src/models/components/simple_dense_net.py
*** lightning-hydra-template-extended/src/models/components/simple_dense_net.py	Thu Aug 14 16:42:32 2025
--- avix/src/models/components/simple_dense_net.py	Sat Aug 23 20:33:35 2025
***************
*** 14,28 ****
          lin3_size: int = 256,
          output_size: Optional[int] = None,
          heads_config: Optional[Dict[str, int]] = None,
      ) -> None:
          """Initialize a `SimpleDenseNet` module.

!         :param input_size: The number of input features.
          :param lin1_size: The number of output features of the first linear layer.
          :param lin2_size: The number of output features of the second linear layer.
          :param lin3_size: The number of output features of the third linear layer.
          :param output_size: The number of output features of the final linear layer (backward compatibility).
          :param heads_config: Dict mapping head names to number of classes for multihead.
          """
          super().__init__()

--- 14,30 ----
          lin3_size: int = 256,
          output_size: Optional[int] = None,
          heads_config: Optional[Dict[str, int]] = None,
+         dropout: float = 0.0,
      ) -> None:
          """Initialize a `SimpleDenseNet` module.

!         :param input_size: The number of input features.  E.g., 3072 for VIMH 32x32x3 (CIFAR), 784 for MNIST 28x28x1.
          :param lin1_size: The number of output features of the first linear layer.
          :param lin2_size: The number of output features of the second linear layer.
          :param lin3_size: The number of output features of the third linear layer.
          :param output_size: The number of output features of the final linear layer (backward compatibility).
          :param heads_config: Dict mapping head names to number of classes for multihead.
+         :param dropout: Dropout probability for regularization (0.0 = no dropout).
          """
          super().__init__()

***************
*** 37,53 ****
          self.is_multihead = len(heads_config) > 1

          # Shared feature extraction layers
!         self.feature_extractor = nn.Sequential(
              nn.Linear(input_size, lin1_size),
              nn.BatchNorm1d(lin1_size),
              nn.ReLU(),
              nn.Linear(lin1_size, lin2_size),
              nn.BatchNorm1d(lin2_size),
              nn.ReLU(),
              nn.Linear(lin2_size, lin3_size),
              nn.BatchNorm1d(lin3_size),
              nn.ReLU(),
!         )

          # Classification heads
          self.heads = nn.ModuleDict()
--- 39,69 ----
          self.is_multihead = len(heads_config) > 1

          # Shared feature extraction layers
!         layers = [
              nn.Linear(input_size, lin1_size),
              nn.BatchNorm1d(lin1_size),
              nn.ReLU(),
+         ]
+         if dropout > 0.0:
+             layers.append(nn.Dropout(dropout))
+
+         layers.extend([
              nn.Linear(lin1_size, lin2_size),
              nn.BatchNorm1d(lin2_size),
              nn.ReLU(),
+         ])
+         if dropout > 0.0:
+             layers.append(nn.Dropout(dropout))
+
+         layers.extend([
              nn.Linear(lin2_size, lin3_size),
              nn.BatchNorm1d(lin3_size),
              nn.ReLU(),
!         ])
!         if dropout > 0.0:
!             layers.append(nn.Dropout(dropout))
!
!         self.feature_extractor = nn.Sequential(*layers)

          # Classification heads
          self.heads = nn.ModuleDict()
Only in avix/src/models/components: simple_mlp.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/components/vision_transformer.py avix/src/models/components/vision_transformer.py
*** lightning-hydra-template-extended/src/models/components/vision_transformer.py	Thu Aug 14 16:42:32 2025
--- avix/src/models/components/vision_transformer.py	Sat Aug 23 20:33:35 2025
***************
*** 1,6 ****
  import torch
  import torch.nn as nn
! from typing import Optional


  class EmbedLayer(nn.Module):
--- 1,6 ----
  import torch
  import torch.nn as nn
! from typing import Optional, Dict


  class EmbedLayer(nn.Module):
***************
*** 169,176 ****
          n_layers (int)          : Number of encoder blocks to use
          n_attention_heads (int) : Number of attention heads to use for performing MultiHeadAttention
          forward_mul (int)       : Used to calculate dimension of the hidden fc layer = embed_dim * forward_mul
-         output_size (int)       : Number of classes
          dropout (float)         : dropout value
          use_torch_layers (bool) : Whether to use PyTorch's built-in transformer layers

      Input:
--- 169,178 ----
          n_layers (int)          : Number of encoder blocks to use
          n_attention_heads (int) : Number of attention heads to use for performing MultiHeadAttention
          forward_mul (int)       : Used to calculate dimension of the hidden fc layer = embed_dim * forward_mul
          dropout (float)         : dropout value
+         heads_config (dict)     : Dict mapping head names to number of classes for multihead
+         output_mode (str)       : "classification" or "regression"
+         parameter_names (list)  : List of parameter names for auto-configuration
          use_torch_layers (bool) : Whether to use PyTorch's built-in transformer layers

      Input:
***************
*** 182,187 ****
--- 184,190 ----
      def __init__(
          self,
          input_size: int = 784,  # For backward compatibility with template, will be ignored
+         output_size: Optional[int] = None,  # For backward compatibility with tests
          n_channels: int = 1,
          image_size: int = 28,
          patch_size: int = 4,
***************
*** 189,202 ****
          n_layers: int = 6,
          n_attention_heads: int = 4,
          forward_mul: int = 2,
-         output_size: int = 10,
          dropout: float = 0.1,
          use_torch_layers: bool = False,
      ):
          super().__init__()

          self.use_torch_layers = use_torch_layers

          # Always use custom embedding layer
          self.embedding = EmbedLayer(n_channels, embed_dim, image_size, patch_size, dropout=dropout)

--- 192,226 ----
          n_layers: int = 6,
          n_attention_heads: int = 4,
          forward_mul: int = 2,
          dropout: float = 0.1,
          use_torch_layers: bool = False,
+         heads_config: Optional[Dict[str, int]] = None,
+         output_mode: str = "classification",
+         parameter_names: Optional[list] = None,
      ):
          super().__init__()

          self.use_torch_layers = use_torch_layers

+         # Handle backward compatibility: convert output_size to heads_config
+         if output_size is not None and heads_config is None:
+             heads_config = {'digit': output_size}
+
+         # Handle multihead configuration similar to SimpleCNN
+         if heads_config is None:
+             if parameter_names is not None:
+                 # Create heads_config for regression (each parameter gets 1 output)
+                 heads_config = {name: 1 for name in parameter_names}
+             else:
+                 if output_mode == "regression":
+                     heads_config = {'regression': 1}  # Default single regression output
+                 else:
+                     heads_config = {'digit': 10}  # Default classification
+
+         self.heads_config = heads_config
+         self.is_multihead = len(heads_config) > 1
+         self.output_mode = output_mode
+
          # Always use custom embedding layer
          self.embedding = EmbedLayer(n_channels, embed_dim, image_size, patch_size, dropout=dropout)

***************
*** 220,226 ****
              ])
              self.norm = nn.LayerNorm(embed_dim)

!         self.classifier = Classifier(embed_dim, output_size)

          self.apply(self._init_weights)

--- 244,276 ----
              ])
              self.norm = nn.LayerNorm(embed_dim)

!         # Create multihead or single head classification layers
!         if self.is_multihead:
!             if output_mode == "regression":
!                 # Regression heads (each outputs 1 value)
!                 self.heads = nn.ModuleDict({
!                     head_name: nn.Sequential(
!                         nn.Linear(embed_dim, 1),
!                         nn.Sigmoid()
!                     )
!                     for head_name in heads_config.keys()
!                 })
!             else:
!                 # Classification heads
!                 self.heads = nn.ModuleDict({
!                     head_name: nn.Linear(embed_dim, num_classes)
!                     for head_name, num_classes in heads_config.items()
!                 })
!         else:
!             # Single head (backward compatibility)
!             head_name, num_classes = next(iter(heads_config.items()))
!             if output_mode == "regression":
!                 self.classifier = nn.Sequential(
!                     nn.Linear(embed_dim, 1),
!                     nn.Sigmoid()
!                 )
!             else:
!                 self.classifier = nn.Linear(embed_dim, num_classes)

          self.apply(self._init_weights)

***************
*** 260,267 ****
                  x = block(x)
              x = self.norm(x)

!         x = self.classifier(x)
!         return x


  if __name__ == "__main__":
--- 310,326 ----
                  x = block(x)
              x = self.norm(x)

!         # Get CLS token for classification
!         cls_token = x[:, 0, :]  # B, S, E --> B, E
!
!         if self.is_multihead:
!             return {
!                 head_name: head(cls_token)
!                 for head_name, head in self.heads.items()
!             }
!         else:
!             # Single head output (backward compatibility)
!             return self.classifier(cls_token)


  if __name__ == "__main__":
Only in avix/src/models: jnd_accuracy.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/losses.py avix/src/models/losses.py
*** lightning-hydra-template-extended/src/models/losses.py	Fri Aug 15 20:35:24 2025
--- avix/src/models/losses.py	Tue Aug 26 13:25:17 2025
***************
*** 6,11 ****
--- 6,154 ----
  from typing import Optional, Tuple


+ # OLD SLOW IMPLEMENTATION REMOVED
+ # The DirectGradientFunction class had a triple nested loop (O(batch_size * num_classes^2))
+ # which made it 1000x slower than necessary. Replaced with fast MSE-based approach.
+
+
+ class DirectParameterGradientLoss(nn.Module):
+     """
+     FAST direct gradient loss for parameter estimation.
+
+     Uses a simple insight: MSE loss gives gradient = 2*(predicted - target)
+     So negative MSE gives gradient = -2*(predicted - target) = 2*(target - predicted)
+     We just scale by 0.5 to get exactly: gradient = target - predicted = -(predicted - target)
+
+     This is 1000x faster than the previous implementation because it uses PyTorch's
+     optimized autograd instead of manual triple loops.
+
+     Args:
+         scale_factor: Scaling factor for gradients (default 1.0)
+         param_range: Parameter range for normalization (optional)
+     """
+
+     def __init__(
+         self,
+         scale_factor: float = 1.0,
+         param_range: Optional[dict] = None,
+     ):
+         super().__init__()
+         self.scale_factor = scale_factor
+         self.param_range = param_range
+
+     def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
+         """
+         Fast direct gradient using negative MSE.
+
+         The gradient of -0.5 * MSE(predicted, target) is exactly:
+         gradient = target - predicted = -(predicted - target)
+
+         This gives direct pressure toward target parameters.
+         """
+         # Handle both continuous outputs and logits
+         if logits.dim() > 1 and logits.shape[1] > 1:
+             # This shouldn't happen with regression mode - network should output continuous values
+             raise ValueError("DirectParameterGradientLoss expects continuous outputs, not logits. "
+                            "Use regression mode with sigmoid outputs in the network.")
+         else:
+             # Already continuous from sigmoid outputs - use directly
+             predictions = logits.squeeze(-1) if logits.dim() > 1 else logits
+
+         # Handle target conversion and denormalization
+         if targets.dim() > 1:
+             # If targets are one-hot or multi-dimensional, convert to indices
+             if targets.shape[1] > 1:
+                 targets = torch.argmax(targets, dim=1)
+             else:
+                 targets = targets.squeeze(-1)
+
+         targets_float = targets.float()
+
+         # Denormalize both predictions and targets to parameter space
+         if self.param_range is not None:
+             # Get parameter range info
+             param_min, param_max = self.param_range['min'], self.param_range['max']
+             param_span = param_max - param_min
+
+             # Denormalize predictions from [0, 1] to [param_min, param_max]
+             predictions_denorm = predictions * param_span + param_min
+
+             # Denormalize targets from class indices to parameter values
+             # Assume targets are quantized indices that need to be mapped to parameter space
+             num_classes = self.param_range.get('num_classes', int(targets_float.max().item()) + 1)
+             step_size = param_span / (num_classes - 1) if num_classes > 1 else param_span
+             targets_denorm = targets_float * step_size + param_min
+
+             # Apply MSE in parameter space
+             mse_loss = F.mse_loss(predictions_denorm, targets_denorm)
+         else:
+             # Fallback: direct MSE without denormalization (predictions and targets assumed in same space)
+             mse_loss = F.mse_loss(predictions, targets_float)
+
+         direct_gradient_loss = 0.5 * self.scale_factor * mse_loss
+
+         return direct_gradient_loss
+
+
+ class DirectRegressionLoss(nn.Module):
+     """
+     Simple direct regression loss without softmax - closest to L1 without quantization.
+
+     This treats the network output as direct continuous parameter predictions
+     and applies L1 loss directly. This gives you gradients that are essentially
+     sign(predicted - target), which is very close to your desired
+     (predicted - target) gradient.
+
+     Args:
+         loss_type: Type of regression loss ('l1', 'l2', 'huber')
+         huber_delta: Delta for Huber loss
+         clamp_range: Optional range to clamp predictions (min, max)
+     """
+
+     def __init__(
+         self,
+         loss_type: str = 'l1',
+         huber_delta: float = 1.0,
+         clamp_range: Optional[Tuple[float, float]] = None,
+     ):
+         super().__init__()
+         self.loss_type = loss_type
+         self.huber_delta = huber_delta
+         self.clamp_range = clamp_range
+
+     def forward(self, output: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
+         """
+         Direct regression loss on continuous outputs.
+
+         Args:
+             output: Network output [batch_size, 1] or [batch_size]
+             targets: Target parameter values [batch_size]
+
+         Returns:
+             Regression loss
+         """
+         # Ensure output is proper shape
+         if output.dim() > 1:
+             output = output.squeeze(-1)
+
+         # Optional clamping
+         if self.clamp_range is not None:
+             output = torch.clamp(output, self.clamp_range[0], self.clamp_range[1])
+
+         # Convert targets to float
+         targets = targets.float()
+
+         # Apply loss function
+         if self.loss_type == 'l1':
+             return F.l1_loss(output, targets)
+         elif self.loss_type == 'l2':
+             return F.mse_loss(output, targets)
+         elif self.loss_type == 'huber':
+             return F.huber_loss(output, targets, delta=self.huber_delta)
+         else:
+             raise ValueError(f"Unknown loss type: {self.loss_type}")
+
+
  class OrdinalRegressionLoss(nn.Module):
      """
      Ordinal regression loss for quantized continuous parameters in perceptual units.
***************
*** 304,306 ****
--- 447,545 ----
          total_loss = cross_entropy_loss + distance_penalty

          return total_loss.mean()
+
+
+ class IndexLoss(nn.Module):
+     """
+     Direct index error loss for AVIX training.
+
+     This loss function computes the direct parameter space error between
+     predicted and target normalized parameter vectors, avoiding any image
+     synthesis during training. It's designed for exhaustive pre-computed
+     datasets where we can optimize directly in parameter space.
+
+     Args:
+         reduction: Reduction method ('mean', 'sum', 'none')
+         parameter_weights: Optional weights for each parameter (if None, uses equal weights)
+         loss_type: Type of loss ('l1', 'l2', 'huber')
+         huber_delta: Delta parameter for Huber loss
+     """
+
+     def __init__(
+         self,
+         reduction: str = 'mean',
+         parameter_weights: Optional[torch.Tensor] = None,
+         loss_type: str = 'l1',
+         huber_delta: float = 0.1
+     ):
+         super().__init__()
+         self.reduction = reduction
+         self.loss_type = loss_type
+         self.huber_delta = huber_delta
+
+         if parameter_weights is not None:
+             self.register_buffer('parameter_weights', parameter_weights)
+         else:
+             self.parameter_weights = None
+
+     def forward(
+         self,
+         predicted_params_normalized: torch.Tensor,
+         target_params_normalized: torch.Tensor
+     ) -> torch.Tensor:
+         """
+         Compute direct index error loss.
+
+         Args:
+             predicted_params_normalized: Predicted normalized parameters [batch_size, num_params]
+             target_params_normalized: Target normalized parameters [batch_size, num_params]
+
+         Returns:
+             Index error loss
+         """
+         # Ensure inputs are properly shaped
+         if predicted_params_normalized.dim() != 2:
+             raise ValueError(f"Expected 2D predicted parameters, got {predicted_params_normalized.dim()}D")
+         if target_params_normalized.dim() != 2:
+             raise ValueError(f"Expected 2D target parameters, got {target_params_normalized.dim()}D")
+
+         # Compute parameter-wise errors
+         if self.loss_type == 'l1':
+             param_errors = torch.abs(predicted_params_normalized - target_params_normalized)
+         elif self.loss_type == 'l2':
+             param_errors = (predicted_params_normalized - target_params_normalized) ** 2
+         elif self.loss_type == 'huber':
+             param_errors = F.huber_loss(
+                 predicted_params_normalized,
+                 target_params_normalized,
+                 delta=self.huber_delta,
+                 reduction='none'
+             )
+         else:
+             raise ValueError(f"Unknown loss type: {self.loss_type}")
+
+         # Apply parameter weights if provided
+         if self.parameter_weights is not None:
+             if self.parameter_weights.shape[0] != param_errors.shape[1]:
+                 raise ValueError(
+                     f"Parameter weights shape {self.parameter_weights.shape} doesn't match "
+                     f"number of parameters {param_errors.shape[1]}"
+                 )
+             param_errors = param_errors * self.parameter_weights.unsqueeze(0)
+
+         # Aggregate across parameters (sum over parameter dimension)
+         sample_errors = torch.sum(param_errors, dim=1)
+
+         # Apply reduction
+         if self.reduction == 'mean':
+             return torch.mean(sample_errors)
+         elif self.reduction == 'sum':
+             return torch.sum(sample_errors)
+         elif self.reduction == 'none':
+             return sample_errors
+         else:
+             raise ValueError(f"Unknown reduction: {self.reduction}")
+
+     def set_parameter_weights(self, weights: torch.Tensor) -> None:
+         """Set parameter weights after initialization."""
+         self.register_buffer('parameter_weights', weights)
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/models/multihead_module.py avix/src/models/multihead_module.py
*** lightning-hydra-template-extended/src/models/multihead_module.py	Fri Aug 15 20:35:24 2025
--- avix/src/models/multihead_module.py	Tue Aug 26 13:25:17 2025
***************
*** 153,160 ****
          if hasattr(self.net, 'heads_config'):
              head_configs = self.net.heads_config
          else:
!             # Fallback for backward compatibility
!             head_configs = {'head_0': 10}

          # Initialize criteria if not already set
          if not self.criteria:
--- 153,160 ----
          if hasattr(self.net, 'heads_config'):
              head_configs = self.net.heads_config
          else:
!             # Fallback for backward compatibility (original template was for MNIST only)
!             head_configs = {'head_0': 10} # 10 digits 0-9

          # Initialize criteria if not already set
          if not self.criteria:
***************
*** 254,259 ****
--- 254,300 ----
          )
          return isinstance(criterion, regression_losses)

+     def load_parameter_metadata_from_datamodule(self, datamodule) -> None:
+         """Load parameter metadata from VIMH datamodule for auto-configuration.
+
+         This method provides a consistent interface for early configuration
+         that matches the AVIX model approach.
+
+         :param datamodule: The datamodule containing VIMH dataset metadata
+         """
+         if self.auto_configure_from_dataset:
+             # Try to access the dataset for metadata
+             dataset = None
+             if hasattr(datamodule, 'data_train') and datamodule.data_train is not None:
+                 dataset = datamodule.data_train
+             elif hasattr(datamodule, 'hparams') and hasattr(datamodule.hparams, 'data_dir'):
+                 # For VIMH datasets, we can configure directly from metadata
+                 try:
+                     from src.utils.vimh_utils import get_parameter_names_from_metadata, get_heads_config_from_metadata
+                     parameter_names = get_parameter_names_from_metadata(datamodule.hparams.data_dir)
+                     if parameter_names and hasattr(self.net, 'heads_config'):
+                         if self.output_mode == 'regression':
+                             # For regression mode, use parameter_names
+                             if hasattr(self.net, 'parameter_names'):
+                                 self.net.parameter_names = parameter_names
+                         else:
+                             # For classification/ordinal mode, use heads_config
+                             heads_config = get_heads_config_from_metadata(datamodule.hparams.data_dir)
+                             self.net.heads_config = heads_config
+
+                         # Auto-configure loss_weights (equal weight for all parameters by default)
+                         if not self.loss_weights or len(self.loss_weights) == 0:
+                             self.loss_weights = {name: 1.0 for name in parameter_names}
+
+                         print(f"Loaded parameter metadata for VIMH model: {parameter_names}")
+                         return
+                 except Exception as e:
+                     print(f"Warning: Failed to load parameter metadata from VIMH dataset: {e}")
+
+             # Fallback to dataset-based configuration if available
+             if dataset and isinstance(dataset, MultiheadDatasetBase):
+                 self._auto_configure_from_dataset(dataset)
+
      def _compute_predictions(self, logits: torch.Tensor, criterion, head_name: str) -> torch.Tensor:
          """Compute predictions based on loss function type."""
          if self.output_mode == "regression":
***************
*** 324,330 ****
--- 365,376 ----
              - A dict of predictions per head.
              - A dict of target labels per head.
          """
+         # Handle both 2-value and 3-value batch formats
+         if len(batch) == 3:
+             x, y, auxiliary_features = batch
+         else:
              x, y = batch
+             auxiliary_features = None
          logits = self.forward(x)

          if self.is_multihead:
***************
*** 332,342 ****
              losses = {}
              for head_name in self.criteria.keys():
                  if head_name in y and head_name in logits:
!                     losses[head_name] = self.criteria[head_name](logits[head_name], y[head_name])
                  else:
                      print(f"Warning: Head {head_name} not found in targets or logits")

              total_loss = sum(self.loss_weights[name] * loss for name, loss in losses.items())

              preds = {
                  head_name: self._compute_predictions(logits_head, self.criteria[head_name], head_name)
--- 378,406 ----
              losses = {}
              for head_name in self.criteria.keys():
                  if head_name in y and head_name in logits:
!                     # Auto-detect soft vs hard targets and use appropriate loss
!                     target_tensor = y[head_name]
!                     if target_tensor.dim() > 1 and target_tensor.shape[1] > 1:
!                         # Soft targets (probability distributions)
!                         loss_fn = torch.nn.KLDivLoss(reduction='batchmean')
!                         # Apply log_softmax to logits for KL divergence
!                         log_probs = F.log_softmax(logits[head_name], dim=1)
!                         losses[head_name] = loss_fn(log_probs, target_tensor)
                      else:
+                         # Hard targets (class indices)
+                         if target_tensor.dim() > 1:
+                             target_tensor = target_tensor.squeeze()
+                         losses[head_name] = self.criteria[head_name](logits[head_name], target_tensor)
+                 else:
                      print(f"Warning: Head {head_name} not found in targets or logits")

+             if losses:
                  total_loss = sum(self.loss_weights[name] * loss for name, loss in losses.items())
+             else:
+                 # If no losses computed, something is wrong - return a meaningful error
+                 raise RuntimeError(f"No losses computed! Available heads in criteria: {list(self.criteria.keys())}, "
+                                  f"Available heads in targets: {list(y.keys()) if isinstance(y, dict) else 'not_dict'}, "
+                                  f"Available heads in logits: {list(logits.keys()) if isinstance(logits, dict) else 'not_dict'}")

              preds = {
                  head_name: self._compute_predictions(logits_head, self.criteria[head_name], head_name)
***************
*** 359,365 ****
--- 423,441 ----
              else:
                  logits_single = logits

+             # Auto-detect soft vs hard targets and use appropriate loss
+             if y_single.dim() > 1 and y_single.shape[1] > 1:
+                 # Soft targets (probability distributions)
+                 loss_fn = torch.nn.KLDivLoss(reduction='batchmean')
+                 # Apply log_softmax to logits for KL divergence
+                 log_probs = F.log_softmax(logits_single, dim=1)
+                 loss = loss_fn(log_probs, y_single)
+             else:
+                 # Hard targets (class indices)
+                 if y_single.dim() > 1:
+                     y_single = y_single.squeeze()
                  loss = self.criteria[head_name](logits_single, y_single)
+
              preds = self._compute_predictions(logits_single, self.criteria[head_name], head_name)

              return loss, {head_name: preds}, {head_name: y_single}
***************
*** 385,391 ****
                      self.train_metrics[f"{head_name}_mae"](preds_dict[head_name], targets_dict[head_name])
              else:
                  if f"{head_name}_acc" in self.train_metrics:
!                     self.train_metrics[f"{head_name}_acc"](preds_dict[head_name], targets_dict[head_name])

          # Log metrics
          self.log("train/loss", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)
--- 461,475 ----
                      self.train_metrics[f"{head_name}_mae"](preds_dict[head_name], targets_dict[head_name])
              else:
                  if f"{head_name}_acc" in self.train_metrics:
!                     # For accuracy calculation, convert soft targets to hard targets if needed
!                     target_tensor = targets_dict[head_name]
!                     if target_tensor.dim() > 1 and target_tensor.shape[1] > 1:
!                         # Soft targets - convert to hard targets by taking argmax
!                         hard_targets = torch.argmax(target_tensor, dim=1)
!                     else:
!                         # Hard targets - squeeze if necessary
!                         hard_targets = target_tensor.squeeze() if target_tensor.dim() > 1 else target_tensor
!                     self.train_metrics[f"{head_name}_acc"](preds_dict[head_name], hard_targets)

          # Log metrics
          self.log("train/loss", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)
***************
*** 427,433 ****
                      self.val_metrics[f"{head_name}_mae"](preds_dict[head_name], targets_dict[head_name])
              else:
                  if f"{head_name}_acc" in self.val_metrics:
!                     self.val_metrics[f"{head_name}_acc"](preds_dict[head_name], targets_dict[head_name])

          # Log metrics
          self.log("val/loss", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)
--- 511,525 ----
                      self.val_metrics[f"{head_name}_mae"](preds_dict[head_name], targets_dict[head_name])
              else:
                  if f"{head_name}_acc" in self.val_metrics:
!                     # For accuracy calculation, convert soft targets to hard targets if needed
!                     target_tensor = targets_dict[head_name]
!                     if target_tensor.dim() > 1 and target_tensor.shape[1] > 1:
!                         # Soft targets - convert to hard targets by taking argmax
!                         hard_targets = torch.argmax(target_tensor, dim=1)
!                     else:
!                         # Hard targets - squeeze if necessary
!                         hard_targets = target_tensor.squeeze() if target_tensor.dim() > 1 else target_tensor
!                     self.val_metrics[f"{head_name}_acc"](preds_dict[head_name], hard_targets)

          # Log metrics
          self.log("val/loss", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)
***************
*** 489,495 ****
                      self.test_metrics[f"{head_name}_mae"](preds_dict[head_name], targets_dict[head_name])
              else:
                  if f"{head_name}_acc" in self.test_metrics:
!                     self.test_metrics[f"{head_name}_acc"](preds_dict[head_name], targets_dict[head_name])

          # Log metrics
          self.log("test/loss", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)
--- 581,595 ----
                      self.test_metrics[f"{head_name}_mae"](preds_dict[head_name], targets_dict[head_name])
              else:
                  if f"{head_name}_acc" in self.test_metrics:
!                     # For accuracy calculation, convert soft targets to hard targets if needed
!                     target_tensor = targets_dict[head_name]
!                     if target_tensor.dim() > 1 and target_tensor.shape[1] > 1:
!                         # Soft targets - convert to hard targets by taking argmax
!                         hard_targets = torch.argmax(target_tensor, dim=1)
!                     else:
!                         # Hard targets - squeeze if necessary
!                         hard_targets = target_tensor.squeeze() if target_tensor.dim() > 1 else target_tensor
!                     self.test_metrics[f"{head_name}_acc"](preds_dict[head_name], hard_targets)

          # Log metrics
          self.log("test/loss", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)
Only in avix/src/models: multihead_module.py.orig
Only in avix/src/models: multihead_module_jnd.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/train.py avix/src/train.py
*** lightning-hydra-template-extended/src/train.py	Fri Aug 15 20:35:24 2025
--- avix/src/train.py	Tue Aug 26 13:25:17 2025
***************
*** 95,103 ****
                      heads_config = get_heads_config_from_metadata(cfg.data.data_dir)
                      cfg.model.net.heads_config = heads_config

!                 # Auto-configure loss_weights (equal weight for all parameters)
                  if not hasattr(cfg.model, 'loss_weights') or not cfg.model.loss_weights or len(cfg.model.loss_weights) == 0:
                      cfg.model.loss_weights = {name: 1.0 for name in parameter_names}
                      log.info(f"Auto-configured loss_weights: {cfg.model.loss_weights}")
          except Exception as e:
              log.warning(f"Failed to auto-configure model from dataset metadata: {e}")
--- 95,130 ----
                      heads_config = get_heads_config_from_metadata(cfg.data.data_dir)
                      cfg.model.net.heads_config = heads_config

!                 # Auto-configure loss_weights proportional to JND range (max-min)/step
                  if not hasattr(cfg.model, 'loss_weights') or not cfg.model.loss_weights or len(cfg.model.loss_weights) == 0:
+                     from src.utils.vimh_utils import load_vimh_metadata
+                     try:
+                         metadata = load_vimh_metadata(cfg.data.data_dir)
+                         param_mappings = metadata.get('parameter_mappings', {})
+                         loss_weights = {}
+
+                         for param_name in parameter_names:
+                             if param_name in param_mappings:
+                                 mapping = param_mappings[param_name]
+                                 param_range = mapping['max'] - mapping['min']
+                                 step = mapping.get('step', 1.0)
+                                 # Weight proportional to number of JND steps
+                                 jnd_steps = param_range / step
+                                 loss_weights[param_name] = float(jnd_steps)
+                             else:
+                                 loss_weights[param_name] = 1.0
+
+                         # Normalize weights so maximum weight is 1.0
+                         if loss_weights:
+                             max_weight = max(loss_weights.values())
+                             loss_weights = {name: weight/max_weight for name, weight in loss_weights.items()}
+
+                         cfg.model.loss_weights = loss_weights
+                         log.info(f"Auto-configured loss_weights based on JND ranges (normalized): {cfg.model.loss_weights}")
+                     except Exception as e:
+                         # Fallback to equal weights
                          cfg.model.loss_weights = {name: 1.0 for name in parameter_names}
+                         log.warning(f"Failed to calculate JND-based weights, using equal weights: {e}")
                          log.info(f"Auto-configured loss_weights: {cfg.model.loss_weights}")
          except Exception as e:
              log.warning(f"Failed to auto-configure model from dataset metadata: {e}")
***************
*** 105,110 ****
--- 132,148 ----
      log.info(f"Instantiating model <{cfg.model._target_}>")
      model: LightningModule = hydra.utils.instantiate(cfg.model)

+     # Load parameter metadata for models that support it - THIS DIRECTION WAS ABANDONED IN BRANCH lhta WHICH WAS SINCE MERGED on 2025-08-23
+     if hasattr(model, 'load_parameter_metadata_from_datamodule'):
+         log.info("DEPRECATED load_parameter_metadata_from_datamodule() method for for dynamic loading parameter metadata from datamodule...")
+         # model.load_parameter_metadata_from_datamodule(datamodule)
+         # Supporting models (AVIX, VIMH with auto-configure) - should be config'd out, but maybe remove the code as well?  Or use some day?:
+         # ./models/multihead_module.py
+         # ./models/avix_module.py
+         # Should not see the above warning because we're currently using instead
+         # ./models/multihead_module_jnd.py
+         sys.exit(1) # fail fast
+
      log.info("Instantiating callbacks...")
      callbacks: List[Callback] = instantiate_callbacks(cfg.get("callbacks"))

***************
*** 122,127 ****
--- 160,166 ----
          "logger": logger,
          "trainer": trainer,
      }
+

      if logger:
          log.info("Logging hyperparameters!")
Only in avix/src: train.py.orig
Only in avix/src/utils: custom_progress_bar.py
Only in avix/src/utils: gradient_stats_callback.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/utils/synth_utils.py avix/src/utils/synth_utils.py
*** lightning-hydra-template-extended/src/utils/synth_utils.py	Thu Aug 14 18:50:17 2025
--- avix/src/utils/synth_utils.py	Sat Aug 23 20:33:35 2025
***************
*** 334,340 ****
              logger.warning(f"Parameter '{param_name}' not found in params")


! class SimpleSynth:
      """Simple exponentially decaying sawtooth synthesizer."""

      def __init__(self, sample_rate: int = 8000):
--- 334,340 ----
              logger.warning(f"Parameter '{param_name}' not found in params")


! class SimpleSawSynth:
      """Simple exponentially decaying sawtooth synthesizer."""

      def __init__(self, sample_rate: int = 8000):
***************
*** 371,384 ****
              if num_samples <= 0:
                  raise ValueError(f"Duration too short for sample rate, got {num_samples} samples")

              # Use arange for exact sample timing instead of linspace
              t = np.arange(num_samples, dtype=np.float64) / self.sample_rate

              # Generate sawtooth wave with proper 1/f spectrum
              # Use fractional part of phase for cleaner sawtooth
              phase = freq * t
!             phase -= np.floor(phase)
!             sawtooth = 2.0 * phase - 1.0

              # Apply exponential decay
              decay_envelope = np.exp(-t / decay_time)
--- 371,386 ----
              if num_samples <= 0:
                  raise ValueError(f"Duration too short for sample rate, got {num_samples} samples")

+
              # Use arange for exact sample timing instead of linspace
              t = np.arange(num_samples, dtype=np.float64) / self.sample_rate

              # Generate sawtooth wave with proper 1/f spectrum
              # Use fractional part of phase for cleaner sawtooth
              phase = freq * t
!             phase -= 0.5 # Makes waveform start at zero with positive slope
!             phase -= np.floor(phase) # -0.5 - (-1.0) = 0.5 for sample 0, and rising
!             sawtooth = 2.0 * phase - 1.0  # 0 for sample 0, and rising

              # Apply exponential decay
              decay_envelope = np.exp(-t / decay_time)
***************
*** 418,430 ****


  def prepare_channels(spectrogram: np.ndarray, channels: int) -> np.ndarray:
!     """Prepare spectrogram with correct number of channels."""
      height, width = spectrogram.shape

      if channels == 1:
          return spectrogram
      else:
!         # For spectrograms, replicate the single-channel data across all channels
          # This maintains compatibility with multi-channel image processing pipelines
          final_spectrogram = np.zeros((height, width, channels), dtype=spectrogram.dtype)
          for c in range(channels):
--- 420,447 ----


  def prepare_channels(spectrogram: np.ndarray, channels: int) -> np.ndarray:
!     """Prepare spectrogram with correct number of channels.
!
!     Args:
!         spectrogram: 2D spectrogram array (height, width)
!         channels: Number of output channels
!
!     Returns:
!         Spectrogram with appropriate channel structure
!
!     Future: Will support derived channels like modulation spectra, envelopes, etc.
!     """
      height, width = spectrogram.shape

      if channels == 1:
          return spectrogram
      else:
!         # TODO: Future derived channel support
!         # - Channel 0: Original spectrogram
!         # - Channel 1: Modulation spectrum
!         # - Channel 2: Envelope or other derived features
!
!         # For now, replicate the single-channel data for backward compatibility
          # This maintains compatibility with multi-channel image processing pipelines
          final_spectrogram = np.zeros((height, width, channels), dtype=spectrogram.dtype)
          for c in range(channels):
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/utils/utils.py avix/src/utils/utils.py
*** lightning-hydra-template-extended/src/utils/utils.py	Thu Aug 14 16:42:32 2025
--- avix/src/utils/utils.py	Sat Aug 23 20:33:35 2025
***************
*** 29,34 ****
--- 29,41 ----
          log.info("Disabling python warnings! <cfg.extras.ignore_warnings=True>")
          warnings.filterwarnings("ignore")

+     # suppress specific DataLoader num_workers warning for MPS compatibility
+     if cfg.extras.get("ignore_dataloader_warnings", True):
+         warnings.filterwarnings("ignore",
+                               message=".*does not have many workers.*may be a bottleneck.*",
+                               category=UserWarning,
+                               module="lightning.pytorch.trainer.connectors.data_connector")
+
      # prompt user to input tags from command line if none are provided in the config
      if cfg.extras.get("enforce_tags"):
          log.info("Enforcing tags! <cfg.extras.enforce_tags=True>")
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/src/utils/vimh_utils.py avix/src/utils/vimh_utils.py
*** lightning-hydra-template-extended/src/utils/vimh_utils.py	Thu Aug 14 16:57:27 2025
--- avix/src/utils/vimh_utils.py	Sat Aug 23 20:33:35 2025
***************
*** 54,59 ****
--- 54,84 ----
      return parameter_ranges


+ def calculate_logits_from_step(param_info: Dict) -> int:
+     """Calculate number of logits from parameter step configuration.
+
+     :param param_info: Parameter info dictionary with 'min', 'max', and 'step' keys
+     :return: Number of logits needed for this parameter
+     """
+     if 'min' in param_info and 'max' in param_info:
+         # Use step size to calculate number of logits if available
+         if 'step' in param_info and param_info['step'] > 0:
+             min_val_raw = param_info['min']
+             max_val_raw = param_info['max']
+             step = param_info['step']
+             # Round min/max to nearest step multiples to avoid floating-point issues
+             min_val = round(min_val_raw / step) * step
+             max_val = round(max_val_raw / step) * step
+             num_steps = int(round((max_val - min_val) / step)) + 1
+             return num_steps
+         else:
+             # Fallback: For continuous parameters without step, use 256 classes (0-255 quantization)
+             return 256
+     else:
+         # Default fallback
+         return 256
+
+
  def get_heads_config_from_metadata(data_dir: str) -> Dict[str, int]:
      """Get heads configuration from VIMH dataset metadata.

***************
*** 71,78 ****
              param_mappings = metadata['parameter_mappings']
              for param_name in param_names:
                  if param_name in param_mappings:
!                     # For continuous parameters, use 256 classes (0-255 quantization)
!                     heads_config[param_name] = 256
          else:
              # Default configuration when no parameter_mappings are available
              # Use the label_encoding range for quantization levels
--- 96,103 ----
              param_mappings = metadata['parameter_mappings']
              for param_name in param_names:
                  if param_name in param_mappings:
!                     param_info = param_mappings[param_name]
!                     heads_config[param_name] = calculate_logits_from_step(param_info)
          else:
              # Default configuration when no parameter_mappings are available
              # Use the label_encoding range for quantization levels
Only in avix: test_direct_gradient.py
Only in avix/tests: test_avix.py
Only in avix/tests: test_losses.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/tests/test_regression_mode.py avix/tests/test_regression_mode.py
*** lightning-hydra-template-extended/tests/test_regression_mode.py	Tue Jul 15 23:50:08 2025
--- avix/tests/test_regression_mode.py	Tue Aug 26 13:17:45 2025
***************
*** 65,78 ****
              assert torch.all(param_output >= 0.0), f"Output should be >= 0 for {param_name}"
              assert torch.all(param_output <= 1.0), f"Output should be <= 1 for {param_name}"

!     def test_regression_network_parameter_names_required(self):
!         """Test that parameter_names is required for regression mode."""
!         with pytest.raises(ValueError, match="parameter_names must be provided"):
!             SimpleCNN(
                  input_channels=3,
                  output_mode="regression",
                  input_size=32
              )

      def test_regression_network_backward_compatibility(self):
          """Test that classification mode still works (backward compatibility)."""
--- 65,82 ----
              assert torch.all(param_output >= 0.0), f"Output should be >= 0 for {param_name}"
              assert torch.all(param_output <= 1.0), f"Output should be <= 1 for {param_name}"

!     def test_regression_network_parameter_names_optional(self):
!         """Test that parameter_names can be empty for auto-configuration in regression mode."""
!         # Should not raise an error - parameter_names can be auto-configured later
!         net = SimpleCNN(
              input_channels=3,
              output_mode="regression",
              input_size=32
          )
+
+         # Should have empty heads_config initially
+         assert net.heads_config == {}
+         assert net.output_mode == "regression"

      def test_regression_network_backward_compatibility(self):
          """Test that classification mode still works (backward compatibility)."""
Only in avix/tests: test_soft_targets_integration.py
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/tests/test_train.py avix/tests/test_train.py
*** lightning-hydra-template-extended/tests/test_train.py	Sat Jul 12 21:46:18 2025
--- avix/tests/test_train.py	Sat Aug 23 20:33:35 2025
***************
*** 18,23 ****
--- 18,26 ----
      with open_dict(cfg_train):
          cfg_train.trainer.fast_dev_run = True
          cfg_train.trainer.accelerator = "cpu"
+         # Remove rich progress bar callback to avoid the live stack issue
+         if 'rich_progress_bar' in cfg_train.callbacks:
+             del cfg_train.callbacks.rich_progress_bar
      train(cfg_train)


diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/tests/test_vimh_datasets.py avix/tests/test_vimh_datasets.py
*** lightning-hydra-template-extended/tests/test_vimh_datasets.py	Tue Jul 15 02:28:35 2025
--- avix/tests/test_vimh_datasets.py	Tue Aug 26 13:17:45 2025
***************
*** 156,162 ****

          dataset = VIMHDataset(str(temp_dir), train=True)

!         image, labels = dataset[0]

          assert isinstance(image, torch.Tensor)
          assert image.shape == (3, 32, 32)
--- 156,162 ----

          dataset = VIMHDataset(str(temp_dir), train=True)

!         image, labels, auxiliary_features = dataset[0]

          assert isinstance(image, torch.Tensor)
          assert image.shape == (3, 32, 32)
***************
*** 179,185 ****
              target_transform=target_transform
          )

!         image, labels = dataset[0]

          # Check that transforms were applied
          assert torch.max(image) > 1.0  # Should be > 1 due to doubling
--- 179,185 ----
              target_transform=target_transform
          )

!         image, labels, auxiliary_features = dataset[0]

          # Check that transforms were applied
          assert torch.max(image) > 1.0  # Should be > 1 due to doubling
***************
*** 330,336 ****
          assert len(dataset) == 5

          # Test sample
!         image, labels = dataset[0]
          assert image.shape == (1, 28, 28)
          assert 'digit_class' in labels

--- 330,336 ----
          assert len(dataset) == 5

          # Test sample
!         image, labels, auxiliary_features = dataset[0]
          assert image.shape == (1, 28, 28)
          assert 'digit_class' in labels

***************
*** 391,397 ****
          assert test_loader is not None

          # Test batch loading
!         batch_images, batch_labels = next(iter(train_loader))
          assert isinstance(batch_images, torch.Tensor)
          assert isinstance(batch_labels, dict)
          assert 'note_number' in batch_labels
--- 391,397 ----
          assert test_loader is not None

          # Test batch loading
!         batch_images, batch_labels, batch_auxiliary_features = next(iter(train_loader))
          assert isinstance(batch_images, torch.Tensor)
          assert isinstance(batch_labels, dict)
          assert 'note_number' in batch_labels
***************
*** 415,421 ****
              (torch.randn(3, 32, 32), {'note_number': 15, 'note_velocity': 25})
          ]

!         batched_images, batched_labels = dm._multihead_collate_fn(batch)

          assert batched_images.shape == (2, 3, 32, 32)
          assert batched_labels['note_number'].shape == (2,)
--- 415,421 ----
              (torch.randn(3, 32, 32), {'note_number': 15, 'note_velocity': 25})
          ]

!         batched_images, batched_labels, batched_auxiliary_features = dm._multihead_collate_fn(batch)

          assert batched_images.shape == (2, 3, 32, 32)
          assert batched_labels['note_number'].shape == (2,)
***************
*** 430,436 ****
          dm = VIMHDataModule(
              data_dir=str(temp_dir),
              batch_size=4,
!             num_workers=0
          )

          dm.setup()
--- 430,437 ----
          dm = VIMHDataModule(
              data_dir=str(temp_dir),
              batch_size=4,
!             num_workers=0,
!             use_random_split=False  # Use original train/test files
          )

          dm.setup()
***************
*** 617,622 ****
--- 618,934 ----
          assert heads_config['note_velocity'] == 256


+ class TestVIMHSoftTargets:
+     """Test cases for VIMH soft target functionality."""
+
+     def test_hard_targets_default(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test hard targets with default target_width=0.0."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         dataset = VIMHDataset(str(temp_dir), train=True, target_width=0.0)
+         image, labels, auxiliary_features = dataset[0]
+
+         # Hard targets should be integers
+         for head_name, label_value in labels.items():
+             assert isinstance(label_value, int) or (isinstance(label_value, torch.Tensor) and label_value.dim() == 0)
+
+     def test_soft_targets_basic(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test soft targets with target_width > 0."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         dataset = VIMHDataset(str(temp_dir), train=True, target_width=1.0)
+         image, labels, auxiliary_features = dataset[0]
+
+         # Soft targets should be probability distributions
+         for head_name, label_value in labels.items():
+             assert isinstance(label_value, torch.Tensor)
+             assert label_value.dim() == 1  # Should be 1D distribution
+             assert label_value.shape[0] > 1  # Should have multiple classes
+             assert torch.all(label_value >= 0)  # All probabilities should be non-negative
+             assert torch.abs(label_value.sum() - 1.0) < 1e-6  # Should sum to 1
+
+     def test_soft_target_gaussian_shape(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test that soft targets form proper Gaussian distributions."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         dataset = VIMHDataset(str(temp_dir), train=True, target_width=1.0)
+
+         # Test the _create_soft_targets method directly
+         class_index = 5
+         num_classes = 10
+         target_width = 1.0
+
+         soft_targets = dataset._create_soft_targets(class_index, num_classes, target_width)
+
+         assert soft_targets.shape == (num_classes,)
+         assert torch.abs(soft_targets.sum() - 1.0) < 1e-6
+
+         # Maximum should be at the true class index
+         assert torch.argmax(soft_targets) == class_index
+
+         # Should decrease with distance from true class
+         assert soft_targets[class_index] > soft_targets[class_index + 1]
+         assert soft_targets[class_index] > soft_targets[class_index - 1]
+
+     def test_soft_target_width_effect(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test effect of different target_width values."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         class_index = 5
+         num_classes = 10
+
+         # Test different widths
+         for target_width in [0.5, 1.0, 2.0]:
+             dataset = VIMHDataset(str(temp_dir), train=True, target_width=target_width)
+             soft_targets = dataset._create_soft_targets(class_index, num_classes, target_width)
+
+             # Wider distributions should have lower peak at true class
+             peak_value = soft_targets[class_index].item()
+
+             if target_width == 0.5:
+                 narrow_peak = peak_value
+             elif target_width == 2.0:
+                 wide_peak = peak_value
+                 # Wider distribution should have lower peak
+                 assert wide_peak < narrow_peak
+
+     def test_soft_target_edge_cases(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test soft targets at class boundaries."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         dataset = VIMHDataset(str(temp_dir), train=True, target_width=1.0)
+
+         # Test edge cases
+         num_classes = 10
+
+         # First class
+         soft_targets_0 = dataset._create_soft_targets(0, num_classes, 1.0)
+         assert torch.argmax(soft_targets_0) == 0
+         assert torch.abs(soft_targets_0.sum() - 1.0) < 1e-6
+
+         # Last class
+         soft_targets_last = dataset._create_soft_targets(num_classes - 1, num_classes, 1.0)
+         assert torch.argmax(soft_targets_last) == num_classes - 1
+         assert torch.abs(soft_targets_last.sum() - 1.0) < 1e-6
+
+     def test_step_based_soft_targets(self, temp_dir, mock_vimh_metadata):
+         """Test soft targets with step-based class mapping."""
+         # Create metadata with step-based parameters
+         step_metadata = mock_vimh_metadata.copy()
+         step_metadata['parameter_names'] = ['note_velocity']
+         step_metadata['varying_parameters'] = 1
+         step_metadata['parameter_mappings'] = {
+             'note_velocity': {
+                 'min': 126.0,
+                 'max': 127.0,
+                 'step': 0.5,
+                 'description': 'Note velocity with step mapping'
+             }
+         }
+
+         # Create simple test data with correct parameter count
+         mock_data = {
+             'data': [list(range(3072))],  # 32x32x3 image
+             'vimh_labels': [[1, 0, 128]],  # 1 parameter: note_velocity
+             'height': 32,
+             'width': 32,
+             'channels': 3
+         }
+
+         create_test_vimh_files(temp_dir, mock_data, step_metadata)
+
+         dataset = VIMHDataset(str(temp_dir), train=True, target_width=1.0)
+         image, labels, auxiliary_features = dataset[0]
+
+         # Should have soft targets for step-based parameter
+         assert 'note_velocity' in labels
+         label_value = labels['note_velocity']
+         assert isinstance(label_value, torch.Tensor)
+         assert label_value.dim() == 1
+         assert label_value.shape[0] == 3  # Should have 3 classes from step calculation
+
+     def test_datamodule_soft_target_collation(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test that data module properly collates soft targets."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         dm = VIMHDataModule(
+             data_dir=str(temp_dir),
+             batch_size=2,
+             num_workers=0,
+             target_width=1.0  # Enable soft targets
+         )
+         dm.setup()
+
+         # Test batch collation
+         train_loader = dm.train_dataloader()
+         batch_images, batch_labels, batch_auxiliary_features = next(iter(train_loader))
+
+         assert isinstance(batch_images, torch.Tensor)
+         assert isinstance(batch_labels, dict)
+
+         # Check that soft targets are properly batched
+         for head_name, label_tensor in batch_labels.items():
+             assert isinstance(label_tensor, torch.Tensor)
+             assert label_tensor.dim() == 2  # (batch_size, num_classes)
+             assert label_tensor.shape[0] == 2  # Batch size
+
+             # Each sample should be a valid probability distribution
+             for i in range(label_tensor.shape[0]):
+                 sample_probs = label_tensor[i]
+                 assert torch.all(sample_probs >= 0)
+                 assert torch.abs(sample_probs.sum() - 1.0) < 1e-6
+
+     def test_mixed_hard_soft_targets_collation(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test collation function with mixed hard/soft targets."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         dm = VIMHDataModule(
+             data_dir=str(temp_dir),
+             batch_size=2,
+             num_workers=0
+         )
+
+         # Create a mixed batch manually
+         hard_target_batch = [
+             (torch.randn(3, 32, 32), {'head1': 5, 'head2': 10}),
+             (torch.randn(3, 32, 32), {'head1': 3, 'head2': 8})
+         ]
+
+         soft_target_batch = [
+             (torch.randn(3, 32, 32), {'head1': torch.tensor([0.1, 0.3, 0.6]), 'head2': torch.tensor([0.2, 0.8])}),
+             (torch.randn(3, 32, 32), {'head1': torch.tensor([0.4, 0.4, 0.2]), 'head2': torch.tensor([0.7, 0.3])})
+         ]
+
+         # Test hard targets
+         batch_images, batch_labels, batch_auxiliary_features = dm._multihead_collate_fn(hard_target_batch)
+         assert batch_labels['head1'].dtype == torch.long
+         assert batch_labels['head1'].shape == (2,)
+
+         # Test soft targets
+         batch_images, batch_labels, batch_auxiliary_features = dm._multihead_collate_fn(soft_target_batch)
+         assert batch_labels['head1'].dtype == torch.float32
+         assert batch_labels['head1'].shape == (2, 3)
+         assert batch_labels['head2'].shape == (2, 2)
+
+     def test_backward_compatibility(self, temp_dir, mock_vimh_data, mock_vimh_metadata):
+         """Test that target_width=0.0 maintains backward compatibility."""
+         create_test_vimh_files(temp_dir, mock_vimh_data, mock_vimh_metadata)
+
+         # Test without target_width parameter (should default to 0.0)
+         dataset_default = VIMHDataset(str(temp_dir), train=True)
+         image_default, labels_default, aux_features_default = dataset_default[0]
+
+         # Test with explicit target_width=0.0
+         dataset_hard = VIMHDataset(str(temp_dir), train=True, target_width=0.0)
+         image_hard, labels_hard, aux_features_hard = dataset_hard[0]
+
+         # Should produce identical results
+         assert torch.equal(image_default, image_hard)
+         for head_name in labels_default.keys():
+             if isinstance(labels_default[head_name], torch.Tensor):
+                 assert torch.equal(labels_default[head_name], labels_hard[head_name])
+             else:
+                 assert labels_default[head_name] == labels_hard[head_name]
+
+     def test_single_head_soft_targets(self, temp_dir, mock_vimh_metadata):
+         """Test soft targets with single-head configuration."""
+         # Create single-head metadata
+         single_head_metadata = mock_vimh_metadata.copy()
+         single_head_metadata['parameter_names'] = ['digit_class']
+         single_head_metadata['parameter_mappings'] = {
+             'digit_class': {
+                 'min': 0.0,
+                 'max': 9.0,
+                 'step': 1.0,
+                 'description': 'Single digit classification'
+             }
+         }
+
+         # Create single-head test data
+         mock_data = {
+             'data': [list(range(3072))],  # 32x32x3 image
+             'vimh_labels': [[1, 0, 128]],  # 1 parameter only
+             'height': 32,
+             'width': 32,
+             'channels': 3
+         }
+
+         create_test_vimh_files(temp_dir, mock_data, single_head_metadata)
+
+         # Test with soft targets
+         dataset = VIMHDataset(str(temp_dir), train=True, target_width=1.0)
+         image, labels, auxiliary_features = dataset[0]
+
+         # Should have only one head
+         assert len(labels) == 1
+         assert 'digit_class' in labels
+
+         # Should be a soft target distribution
+         label_value = labels['digit_class']
+         assert isinstance(label_value, torch.Tensor)
+         assert label_value.dim() == 1
+         assert label_value.shape[0] == 10  # 10 classes (0-9)
+         assert torch.abs(label_value.sum() - 1.0) < 1e-6
+
+     def test_single_head_datamodule_integration(self, temp_dir, mock_vimh_metadata):
+         """Test single-head integration with data module and model."""
+         # Create single-head metadata
+         single_head_metadata = mock_vimh_metadata.copy()
+         single_head_metadata['parameter_names'] = ['digit_class']
+         single_head_metadata['parameter_mappings'] = {
+             'digit_class': {
+                 'min': 0.0,
+                 'max': 9.0,
+                 'step': 1.0,
+                 'description': 'Single digit classification'
+             }
+         }
+
+         # Create test data with multiple samples - must match metadata expectations
+         mock_data = {
+             'data': [list(range(3072)) for _ in range(10)],  # 10 samples total
+             'vimh_labels': [[1, 0, i * 25] for i in range(10)],  # Different labels
+             'height': 32,
+             'width': 32,
+             'channels': 3
+         }
+
+         # Update metadata to match our test data
+         single_head_metadata['train_samples'] = 8
+         single_head_metadata['test_samples'] = 2
+
+         create_test_vimh_files(temp_dir, mock_data, single_head_metadata)
+
+         # Test with data module
+         dm = VIMHDataModule(
+             data_dir=str(temp_dir),
+             batch_size=2,
+             num_workers=0,
+             target_width=1.0  # Enable soft targets
+         )
+         dm.setup()
+
+         # Should detect single head correctly
+         assert len(dm.heads_config) == 1
+         assert 'digit_class' in dm.heads_config
+         assert dm.heads_config['digit_class'] == 10
+
+         # Test batch loading
+         train_loader = dm.train_dataloader()
+         batch_images, batch_labels, batch_auxiliary_features = next(iter(train_loader))
+
+         assert isinstance(batch_images, torch.Tensor)
+         assert isinstance(batch_labels, dict)
+         assert len(batch_labels) == 1  # Single head
+         assert 'digit_class' in batch_labels
+
+         # Check soft target batching
+         label_tensor = batch_labels['digit_class']
+         assert label_tensor.shape[0] == 2  # Batch size
+         assert label_tensor.shape[1] == 10  # Number of classes
+         assert label_tensor.dim() == 2  # (batch_size, num_classes)
+
+
  class TestVIMHIntegration:
      """Integration tests for VIMH dataset functionality."""

***************
*** 638,644 ****

          # Test dataloader creation
          train_loader = dm.train_dataloader()
!         batch_images, batch_labels = next(iter(train_loader))

          assert isinstance(batch_images, torch.Tensor)
          assert isinstance(batch_labels, dict)
--- 950,956 ----

          # Test dataloader creation
          train_loader = dm.train_dataloader()
!         batch_images, batch_labels, batch_auxiliary_features = next(iter(train_loader))

          assert isinstance(batch_images, torch.Tensor)
          assert isinstance(batch_labels, dict)
***************
*** 662,668 ****
              images = []
              labels_dict = {}

!             for image, labels in batch:
                  images.append(image)
                  if not labels_dict:
                      for head_name in labels.keys():
--- 974,980 ----
              images = []
              labels_dict = {}

!             for image, labels, aux_features in batch:
                  images.append(image)
                  if not labels_dict:
                      for head_name in labels.keys():
***************
*** 675,687 ****
              for head_name, label_list in labels_dict.items():
                  batched_labels[head_name] = torch.tensor(label_list, dtype=torch.long)

!             return batched_images, batched_labels

          # Test DataLoader creation
          dataloader = DataLoader(dataset, batch_size=3, shuffle=False, collate_fn=custom_collate)

          # Test batch loading
!         batch_images, batch_labels = next(iter(dataloader))

          assert isinstance(batch_images, torch.Tensor)
          assert batch_images.shape[0] == 3  # Batch size
--- 987,999 ----
              for head_name, label_list in labels_dict.items():
                  batched_labels[head_name] = torch.tensor(label_list, dtype=torch.long)

!             return batched_images, batched_labels, None  # No auxiliary features for this test

          # Test DataLoader creation
          dataloader = DataLoader(dataset, batch_size=3, shuffle=False, collate_fn=custom_collate)

          # Test batch loading
!         batch_images, batch_labels, batch_auxiliary_features = next(iter(dataloader))

          assert isinstance(batch_images, torch.Tensor)
          assert batch_images.shape[0] == 3  # Batch size
Only in avix: tests-log-2025-08-15.txt
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/viz/enhanced_model_diagrams.py avix/viz/enhanced_model_diagrams.py
*** lightning-hydra-template-extended/viz/enhanced_model_diagrams.py	Fri Jul  4 22:32:20 2025
--- avix/viz/enhanced_model_diagrams.py	Sat Aug 23 20:33:35 2025
***************
*** 179,199 ****

      except Exception as e:
          print(f"Error loading config {config_name}: {e}")
!         print("Falling back to hardcoded SimpleCNN...")
!
!         # Fallback to hardcoded model
!         model = SimpleCNN(
!             input_channels=1,
!             conv1_channels=3,
!             conv2_channels=6,
!             fc_hidden=25,
!             output_size=10,
!             dropout=0.25
!         )
!
!         create_text_summary(model, model_name=f"SimpleCNN (fallback)")
!         create_ascii_diagram_cnn()
!         create_graphical_diagram(model, model_name="SimpleCNN", output_dir=output_dir)

  def main():
      parser = argparse.ArgumentParser(description="Generate model architecture diagrams")
--- 179,185 ----

      except Exception as e:
          print(f"Error loading config {config_name}: {e}")
!         sys.exit(1)

  def main():
      parser = argparse.ArgumentParser(description="Generate model architecture diagrams")
diff --exclude .venv --exclude __pycache__ --exclude .svn --exclude .git* --exclude .claude* --exclude .pytest* --exclude CVS --exclude .dropbox --exclude .hg --exclude .xcbkptlist --exclude .xcworkspace --exclude .xcuserdatad -rcb lightning-hydra-template-extended/viz/simple_model_diagram.py avix/viz/simple_model_diagram.py
*** lightning-hydra-template-extended/viz/simple_model_diagram.py	Mon Jul  7 18:58:40 2025
--- avix/viz/simple_model_diagram.py	Sat Aug 23 20:33:35 2025
***************
*** 38,47 ****

      # Test forward pass and show shapes
      print(f"\nForward Pass Shape Analysis:")
-     print(f"Input shape: {input_shape}")

      with torch.no_grad():
!         x = torch.randn(*input_shape)

          # Track intermediate shapes
          print(f"Input: {x.shape}")
--- 38,51 ----

      # Test forward pass and show shapes
      print(f"\nForward Pass Shape Analysis:")

+     # Use batch size of 2 to avoid BatchNorm issues, and set model to eval mode
+     batch_shape = (2,) + input_shape[1:]
+     print(f"Input shape: {batch_shape}")
+
+     model.eval()  # Set to eval mode to avoid BatchNorm issues
      with torch.no_grad():
!         x = torch.randn(*batch_shape)

          # Track intermediate shapes
          print(f"Input: {x.shape}")
***************
*** 56,62 ****
              shared_out = model.shared_features(conv_out)
              print(f"After shared features: {shared_out.shape}")

!         # Final output
          final_out = model(x)
          if isinstance(final_out, dict):
              print(f"Final output (multihead):")
--- 60,67 ----
              shared_out = model.shared_features(conv_out)
              print(f"After shared features: {shared_out.shape}")

!         # Final output - wrap in try/except for models that need dataset metadata
!         try:
              final_out = model(x)
              if isinstance(final_out, dict):
                  print(f"Final output (multihead):")
***************
*** 64,77 ****
                  print(f"  {head_name}: {logits.shape}")
          else:
              print(f"Final output: {final_out.shape}")

! def create_ascii_diagram():
      """Create an ASCII diagram of the model architecture."""
      print("\n" + "="*80)
      print("ASCII Architecture Diagram")
      print("="*80)

      print("""
      Input (1x28x28)
             │
             ▼
--- 69,122 ----
                      print(f"  {head_name}: {logits.shape}")
              else:
                  print(f"Final output: {final_out.shape}")
+         except (KeyError, AttributeError) as e:
+             print(f"Note: Model requires dataset metadata for full initialization (heads auto-configured)")
+             print(f"       The model structure above shows the feature extractor architecture")

! def create_ascii_diagram(config_name=""):
      """Create an ASCII diagram of the model architecture."""
      print("\n" + "="*80)
      print("ASCII Architecture Diagram")
      print("="*80)

+     # Check if it's a DenseNet model
+     if 'densenet' in config_name.lower():
          print("""
+     Input (3x32x32) → Flatten → (3072,)
+            │
+            ▼
+     ┌─────────────────┐
+     │ Linear(3072→32) │  Feature extraction layer 1
+     │   BatchNorm1d   │
+     │      ReLU       │
+     └─────────────────┘
+            │ (32,)
+            ▼
+     ┌─────────────────┐
+     │  Linear(32→64)  │  Feature extraction layer 2
+     │   BatchNorm1d   │
+     │      ReLU       │
+     └─────────────────┘
+            │ (64,)
+            ▼
+     ┌─────────────────┐
+     │  Linear(64→32)  │  Feature extraction layer 3
+     │   BatchNorm1d   │
+     │      ReLU       │
+     └─────────────────┘
+            │ (32,)
+            ▼
+     ┌─────────────────┐
+     │   Multi-Head    │  Auto-configured heads
+     │    Outputs      │  (from dataset metadata)
+     └─────────────────┘
+            │
+            ▼
+       Output Dict
+     """)
+     else:
+         # Default CNN diagram
+         print("""
      Input (1x28x28)
             │
             ▼
***************
*** 131,157 ****
              input_shape = (1, 1, 28, 28)  # Default for MNIST
              if 'cifar' in config_name.lower():
                  input_shape = (1, 3, 32, 32)  # CIFAR input shape

              # Generate summary and diagram
              create_model_summary(model, input_shape=input_shape, model_name=f"Model: {config_name}")
!             create_ascii_diagram()

      except Exception as e:
          print(f"Error loading config {config_name}: {e}")
!         print("Falling back to hardcoded SimpleCNN...")
!
!         # Fallback to hardcoded model
!         model = SimpleCNN(
!             input_channels=1,
!             conv1_channels=3,
!             conv2_channels=6,
!             fc_hidden=25,
!             output_size=10,
!             dropout=0.25
!         )
!
!         create_model_summary(model, model_name=f"SimpleCNN (fallback for {config_name})")
!         create_ascii_diagram()

  def main():
      parser = argparse.ArgumentParser(description="Generate model architecture diagrams")
--- 176,191 ----
              input_shape = (1, 1, 28, 28)  # Default for MNIST
              if 'cifar' in config_name.lower():
                  input_shape = (1, 3, 32, 32)  # CIFAR input shape
+             elif 'avix' in config_name.lower() or 'vimh' in config_name.lower():
+                 input_shape = (1, 3, 32, 32)  # AVIX/VIMH input shape

              # Generate summary and diagram
              create_model_summary(model, input_shape=input_shape, model_name=f"Model: {config_name}")
!             create_ascii_diagram(config_name)

      except Exception as e:
          print(f"Error loading config {config_name}: {e}")
!         sys.exit(1)

  def main():
      parser = argparse.ArgumentParser(description="Generate model architecture diagrams")
