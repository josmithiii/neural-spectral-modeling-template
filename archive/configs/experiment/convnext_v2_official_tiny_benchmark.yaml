# @package _global_

# Official ConvNeXt V2-Tiny benchmark experiment
# Based on the canonical configuration from Facebook's ConvNeXt-V2 repository
# https://github.com/facebookresearch/ConvNeXt-V2/blob/main/TRAINING.md
#
# This serves as an "acid test" for our ConvNeXt-V2 integration correctness
# Original ImageNet-1K performance: 83.0% top-1 accuracy with 28.6M parameters
# Adapted for MNIST with proportional scaling

defaults:
  - override /data: mnist
  - override /model: mnist_convnext_210k  # Base config
  - override /callbacks: default
  - override /trainer: default

# Canonical experiment tags for benchmarking
tags: ["convnext", "benchmark", "official", "tiny", "acid-test"]

seed: 42  # Fixed seed for reproducible benchmarking

trainer:
  min_epochs: 50
  max_epochs: 100  # Scaled down from official 300 epochs for MNIST
  gradient_clip_val: 1.0
  precision: 16-mixed  # Equivalent to official --use_amp True

model:
  # Official ConvNeXt V2-Tiny training hyperparameters (adapted for MNIST)
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0008  # Official base lr: 8e-4 (blr parameter)
    weight_decay: 0.05  # Official weight_decay: 0.05
    eps: 1e-8  # Official opt_eps
    betas: [0.9, 0.999]  # Standard AdamW betas

  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    _partial_: true
    T_max: 100  # Match max_epochs
    eta_min: 1e-6  # Official min_lr: 1e-6

  criterion:
    _target_: torch.nn.CrossEntropyLoss
    label_smoothing: 0.1  # Official smoothing: 0.1

  # Use official benchmark model with exact hyperparameters
  net:
    _target_: src.models.components.convnext_v2.convnext_v2_official_tiny_benchmark
    input_size: 28
    in_chans: 1
    output_size: 10

data:
  batch_size: 128  # Scaled for single GPU (official: 32 * 8 GPUs = 256 effective)
  num_workers: 4

# Enable all official training techniques (adapted for Lightning)
callbacks:
  model_checkpoint:
    monitor: val/acc
    mode: max
    save_top_k: 3
    save_last: true
    verbose: true
    filename: "convnext_v2_official_tiny_epoch_{epoch:03d}_acc_{val/acc:.4f}"

  early_stopping:
    monitor: val/acc
    patience: 20  # Increased patience for longer training
    mode: max
    min_delta: 0.001

  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar

  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: 2

logger:
  wandb:
    project: "convnext-v2-benchmark"
    tags: ${tags}
    group: "official-tiny"
    notes: |
      Official ConvNeXt V2-Tiny benchmark configuration
      Based on Facebook's canonical training recipe
      Hyperparameters exactly match the official implementation
      Serves as acid test for integration correctness

  aim:
    experiment: "convnext_v2_official_benchmark"

# Performance expectations based on official results
# Original: 83.0% ImageNet-1K top-1 accuracy
# Expected MNIST performance: >95% accuracy (preliminary estimate)
