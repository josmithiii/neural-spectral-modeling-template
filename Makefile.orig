h help:  ## Show help
	@grep -E '^[.a-zA-Z0-9_ -]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-30s\033[0m %s\n", $$1, $$2}'

c clean: ## Clean autogenerated files
	rm -rf dist
	find . -type f -name "*.DS_Store" -ls -delete
	find . | grep -E "(__pycache__|\.pyc|\.pyo)" | xargs rm -rf
	find . | grep -E ".pytest_cache" | xargs rm -rf
	find . | grep -E ".ipynb_checkpoints" | xargs rm -rf
	rm -f .coverage
	rm -rf ./viz/diagrams
	rm -rf ./outputs/

dc dclean: ## Clean data files
	rm -rf data/*

cl clean-logs: ## Clean logs
	rm -rf logs/**

f format: ## Run pre-commit hooks
	pre-commit run -a

s sync: ## Merge changes from main branch to your current branch
	git pull
	git pull origin main

tb tensorboard: ## Launch TensorBoard on port 6006
	@lsof -i :6006 >/dev/null 2>&1 && echo "TensorBoard already running on port 6006" || \
		(echo "Starting TensorBoard on port 6006..." && tensorboard --logdir logs/train/runs/ --port 6006 &)
	@echo "Open http://localhost:6006/"

a activate: ## Activate the uv environment
	@echo "Add to ~/.tcshrc: alias a 'echo \"source .venv/bin/activate.csh\" && source .venv/bin/activate.csh'"
	@echo "Then just type: a"

d deactivate: ## Deactivate the uv environment
	@echo "Add to ~/.tcshrc: alias d 'echo deactivate && deactivate'"
	@echo "Then just type: d"

# TRAINING TARGETS "tr"

tr train train-sdn: ## Train the default model (a small SimpleDenseNet) 
	time python src/train.py


trc trcnn train-cnn: ## Train with CNN architecture
	time python src/train.py model=mnist_cnn_small

tg train-with-gradients: ## Train with gradient statistics tracking enabled
	python src/train.py callbacks=grouped_progress_bar_with_gradients logger=tensorboard
	@echo "Check gradient stats at http://localhost:6006/#scalars&tagFilter=grad_stats"

trvs train-vit-small: ## Train small ViT (~38K params)
	time python src/train.py model=mnist_vit_38k

trvm train-vit-medium: ## Train with ViT architecture (~210K params)
	time python src/train.py model=mnist_vit_210k

trvl train-vit-large: ## Train large ViT (~821K params)
	time python src/train.py model=mnist_vit_821k


trvp train-vit-pytorch: ## Train ViT using PyTorch layers
	time python src/train.py model=mnist_vit_pytorch

trcns train-convnext-small: ## Train ConvNeXt-V2 small (~68K params)
	time python src/train.py model=mnist_convnext_68k

trcnm train-convnext-medium: ## Train ConvNeXt-V2 medium (~210K params)
	time python src/train.py model=mnist_convnext_210k

trcnl train-convnext-large: ## Train ConvNeXt-V2 large (~821K params)
	time python src/train.py model=mnist_convnext_821k



# TRAIN-QUICKLY TARGETS "tq"

tq train-quick: ## Train quickly SimpleDenseNet, 1 epoch
	python src/train.py trainer.max_epochs=1 +trainer.limit_train_batches=10 +trainer.limit_val_batches=5

tqc train-quick-cnn: ## Train quickly SimpleCNN, 1 epoch
	python src/train.py model=mnist_cnn_8k trainer.max_epochs=1 +trainer.limit_train_batches=10 +trainer.limit_val_batches=5

tqv train-quick-vit: ## Train quickly ViT, 1 epoch
	python src/train.py model=mnist_vit_38k trainer.max_epochs=1 +trainer.limit_train_batches=10 +trainer.limit_val_batches=5

tqcn train-quick-convnext: ## Train quickly ConvNeXt-V2, 1 epoch
	python src/train.py model=mnist_convnext_68k trainer.max_epochs=1 +trainer.limit_train_batches=10 +trainer.limit_val_batches=5

tqa train-quick-all: tq tqc tqv tqcn ## Train quickly all architectures supported

# TESTING TARGETS "t"

t test: ## Run fast pytest tests
	pytest -k "not slow"

ta test-all: ## Run all pytest tests
	pytest

td test-diagram: ## Generate model architecture diagrams (text + graphical)
	python viz/enhanced_model_diagrams.py

tda test-diagram-all: ## Generate diagrams for all model architectures
	python viz/enhanced_model_diagrams.py -c mnist_cnn_8k
	python viz/enhanced_model_diagrams.py -c mnist_vit_38k
	python viz/enhanced_model_diagrams.py -c mnist_convnext_68k
	python viz/enhanced_model_diagrams.py -c mnist_sdn_8k

tdl test-diagram-list: ## List available model configs for diagrams
	python viz/enhanced_model_diagrams.py --list-configs

tds test-diagram-simple: ## Generate simple text-only diagrams (default mnist_cnn_8k)
	python viz/simple_model_diagram.py

tdsc test-diagram-simple-config: ## Generate simple diagram for specific config (usage: make tdsc CONFIG=mnist_vit_38k)
	python viz/simple_model_diagram.py --config $(CONFIG)

tdsl test-diagram-simple-list: ## List available configs for simple diagrams
	python viz/simple_model_diagram.py --list-configs

tdss test-diagram-simple-samples: ## Generate simple diagrams for sample architectures
	@echo "=== MNIST CNN (8K params) ==="
	python viz/simple_model_diagram.py --config mnist_cnn_8k
	@echo "\n=== MNIST ViT (38K params) ==="
	python viz/simple_model_diagram.py --config mnist_vit_38k
	@echo "\n=== MNIST ConvNeXt (68K params) ==="
	python viz/simple_model_diagram.py --config mnist_convnext_68k
	@echo "\n=== CIFAR-10 CNN (64K params) ==="
	python viz/simple_model_diagram.py --config cifar10_cnn_64k
	@echo "\n=== MNIST Multihead CNN (422K params) ==="
	python viz/simple_model_diagram.py --config mnist_mh_cnn_422k

tdsm test-diagram-simple-mnist: ## Generate simple diagrams for all MNIST architectures
	@echo "=== MNIST Architectures ==="
	python viz/simple_model_diagram.py --config mnist_cnn_8k
	python viz/simple_model_diagram.py --config mnist_sdn_8k
	python viz/simple_model_diagram.py --config mnist_vit_38k
	python viz/simple_model_diagram.py --config mnist_convnext_68k
	python viz/simple_model_diagram.py --config mnist_mh_cnn_422k

tdsc10 test-diagram-simple-cifar10: ## Generate simple diagrams for CIFAR-10 architectures
	@echo "=== CIFAR-10 Architectures ==="
	python viz/simple_model_diagram.py --config cifar10_cnn_64k
	python viz/simple_model_diagram.py --config cifar10_convnext_64k
	python viz/simple_model_diagram.py --config cifar10_mh_cnn_64k
	python viz/simple_model_diagram.py --config cifar10_vit_210k

ca compare-arch: ## Compare medium sized architectures on three epochs
	@echo "=== Training SimpleDenseNet ==="
	python src/train.py trainer.max_epochs=3 tags="[arch_comparison,dense]"
	@echo "=== Training SimpleCNN ==="
	python src/train.py model=mnist_cnn_68k trainer.max_epochs=3 tags="[arch_comparison,cnn]"
	@echo "=== Training ViT ==="
	python src/train.py model=mnist_vit_38k trainer.max_epochs=3 tags="[arch_comparison,vit]"
	@echo "=== Training ConvNeXt-V2 ==="
	python src/train.py model=mnist_convnext_68k trainer.max_epochs=3 tags="[arch_comparison,convnext]"
	@echo "=== Check logs/ directory for results comparison ==="

# EXPERIMENTS "e" - Reproducible Configuration Examples

e esdn exp-sdn: ## Run original example experiment (reproducible baseline)
	time python src/train.py experiment=example

evit exp-vit: ## Run ViT experiment
	time python src/train.py experiment=vit_mnist


ev995 exp-vit-995: ## Run ViT experiment achieving SOTA 99.5% validation accuracy
	time python src/train.py experiment=mnist_vit_995
	# == python src/train.py model=mnist_vit_995 data=mnist_vit_995 trainer.max_epochs=200 trainer.min_epochs=10 trainer.gradient_clip_val=1.0 data.batch_size=128 seed=12345 tags="[mnist,vit,995,optimized]"

ecm exp-cnn-mnist: ## Run single-head CNN MNIST classification experiment - accuracy ~99.1%
	time python src/train.py experiment=cnn_mnist

emhcm exp-multihead-cnn-mnist: ## Run MultiHead CNN MNIST classification experiment - accuracies ~99.1%, 99.2%, 99.2%
	time python src/train.py experiment=multihead_cnn_mnist

emhcc10 exp-multihead-cnn-cifar10: ## Run MultiHead CNN CIFAR-10 classification experiment
	time python src/train.py experiment=multihead_cnn_cifar10

evimh exp-vimh-16kdss: ## Run VIMH CNN training with 16K dataset samples (SimpleSynth)
	time python src/train.py experiment=vimh_cnn_16kdss # ./configs/experiment/vimh_cnn_16kdss.yaml
evimho exp-vimh-16kdss-ordinal: ## Run VIMH CNN training with ordinal regression loss (distance-aware)
	time python src/train.py experiment=vimh_cnn_16kdss_ordinal # ./configs/experiment/vimh_cnn_16kdss.yaml
evimhr exp-vimh-16kdss-regression: ## Run VIMH CNN training with pure regression heads (sigmoid + parameter mapping)
	time python src/train.py experiment=vimh_cnn_16kdss_regression # ./configs/experiment/vimh_cnn_16kdss_regression.yaml

excn exp-convnext: ## Run ConvNeXt-V2 experiment
	time python src/train.py experiment=convnext_mnist

ecnb exp-convnext-benchmark: ## Run official ConvNeXt V2-Tiny benchmark (acid test)
	time python src/train.py experiment=convnext_v2_official_tiny_benchmark

# CIFAR BENCHMARKS "cb" - Computer Vision Dataset Experiments

cb10c cifar10-cnn: ## Run CIFAR-10 CNN benchmark (85-92% expected accuracy)
	time python src/train.py experiment=cifar10_benchmark_cnn

cb10cn cifar10-convnext: ## Run CIFAR-10 ConvNeXt benchmark (90-95% expected accuracy)
	time python src/train.py experiment=cifar10_benchmark_convnext

cb10cn64 cifar10-convnext-64k-optimized: ## Run CIFAR-10 ConvNeXt 64K optimized for small images
	time python src/train.py experiment=cifar10_convnext_64k_optimized

cb10cn128 cifar10-convnext-128k-optimized: ## Run CIFAR-10 ConvNeXt 128K optimized for small images
	time python src/train.py experiment=cifar10_convnext_128k_optimized

cb10v cifar10-vit: ## Run CIFAR-10 Vision Transformer benchmark (88-93% expected accuracy)
	time python src/train.py experiment=cifar10_benchmark_vit

cb10e cifar10-efficientnet: ## Run CIFAR-10 EfficientNet benchmark (89-94% expected accuracy)
	time python src/train.py experiment=cifar10_benchmark_efficientnet

cb100c cifar100-cnn: ## Run CIFAR-100 CNN benchmark (55-70% expected accuracy)
	time python src/train.py experiment=cifar100_benchmark_cnn

cb100cn cifar100-convnext: ## Run CIFAR-100 ConvNeXt benchmark (70-80% expected accuracy)
	time python src/train.py experiment=cifar100_benchmark_convnext

cb100v cifar100-vit: ## Run CIFAR-100 Vision Transformer benchmark (65-75% expected accuracy)
	time python src/train.py experiment=cifar100_vit_210k

cb100e cifar100-efficientnet: ## Run CIFAR-100 EfficientNet benchmark (68-78% expected accuracy)
	time python src/train.py experiment=cifar100_efficientnet_210k

cb100sdn cifar100-sdn: ## Run CIFAR-100 SimpleDenseNet benchmark (~1M params, 50-65% expected accuracy)
	time python src/train.py model=cifar100_sdn_1m data=cifar100

cb100cnn1m cifar100-cnn-1m: ## Run CIFAR-100 CNN benchmark (~1M params, 55-70% expected accuracy)
	time python src/train.py model=cifar100_cnn_1m data=cifar100

cb100cn1m cifar100-convnext-1m: ## Run CIFAR-100 ConvNeXt benchmark (~1M params, 70-80% expected accuracy)
	time python src/train.py model=cifar100_convnext_1m data=cifar100

cb100cn10m cifar100-convnext-10m: ## Run CIFAR-100 ConvNeXt 10M benchmark (~10M params, 65-75% expected accuracy)
	time python src/train.py model=cifar100_convnext_10m data=cifar100

cb100cc cifar100-coarse-cnn: ## Run CIFAR-100 coarse (20-class) CNN benchmark (75-85% expected accuracy) - CPU required due to MPS pooling limitation (and no GPU here)
	time python src/train.py experiment=cifar100_coarse_cnn trainer=cpu

cb100ccn cifar100-coarse-convnext: ## Run CIFAR-100 coarse ConvNeXt benchmark (80-90% expected accuracy)
	time python src/train.py experiment=cifar100_coarse_convnext

# CIFAR QUICK BENCHMARKS "cbq" - Fast Validation Runs

cbq10c cifar10-quick-cnn: ## Quick CIFAR-10 CNN validation (5 epochs)
	python src/train.py experiment=cifar10_benchmark_cnn trainer.max_epochs=5 trainer.min_epochs=1

cbq10cn cifar10-quick-convnext: ## Quick CIFAR-10 ConvNeXt validation (5 epochs)
	python src/train.py experiment=cifar10_benchmark_convnext trainer.max_epochs=5 trainer.min_epochs=1

cbq10cn64 cifar10-quick-convnext-64k: ## Quick CIFAR-10 ConvNeXt 64K optimized validation (5 epochs)
	python src/train.py experiment=cifar10_convnext_64k_optimized trainer.max_epochs=5 trainer.min_epochs=1

cbq10cn128 cifar10-quick-convnext-128k: ## Quick CIFAR-10 ConvNeXt 128K optimized validation (5 epochs)
	python src/train.py experiment=cifar10_convnext_128k_optimized trainer.max_epochs=5 trainer.min_epochs=1

cbq100c cifar100-quick-cnn: ## Quick CIFAR-100 CNN validation (5 epochs)
	python src/train.py experiment=cifar100_benchmark_cnn trainer.max_epochs=5 trainer.min_epochs=1

cbq100sdn cifar100-quick-sdn: ## Quick CIFAR-100 SimpleDenseNet validation (5 epochs)
	python src/train.py model=cifar100_sdn_1m data=cifar100 trainer.max_epochs=5 trainer.min_epochs=1

cbq100cnn1m cifar100-quick-cnn-1m: ## Quick CIFAR-100 CNN 1M validation (5 epochs)
	python src/train.py model=cifar100_cnn_1m data=cifar100 trainer.max_epochs=5 trainer.min_epochs=1

cbq100cn1m cifar100-quick-convnext-1m: ## Quick CIFAR-100 ConvNeXt 1M validation (5 epochs)
	python src/train.py model=cifar100_convnext_1m data=cifar100 trainer.max_epochs=5 trainer.min_epochs=1

cbq100cn10m cifar100-quick-convnext-10m: ## Quick CIFAR-100 ConvNeXt 10M validation (10 epochs)
	python src/train.py model=cifar100_convnext_10m data=cifar100 trainer.max_epochs=10 trainer.min_epochs=5

cbq100cc cifar100-quick-coarse: ## Quick CIFAR-100 coarse validation (5 epochs)
	python src/train.py experiment=cifar100_coarse_cnn trainer.max_epochs=5 trainer.min_epochs=1

cbqa cifar-quick-all: cbq10c cbq10cn cbq10cn64 cbq10cn128 cbq100sdn cbq100cnn1m cbq100cn10m cbq100c cbq100cc  ## Run all quick CIFAR validations

# CIFAR BENCHMARK SUITES "cbs" - Systematic Comparisons

cbs benchmark-suite: ## Run automated CIFAR benchmark suite
	python benchmarks/scripts/benchmark_cifar.py

cbs10 benchmark-cifar10: cb10c cb10cn cb10v cb10e ## Run all CIFAR-10 benchmarks
	@echo "=== CIFAR-10 benchmark suite complete ==="

cbs100 benchmark-cifar100: cb100c cb100cn cb100v cb100e cb100sdn cb100cnn1m cb100cn1m cb100cn10m cb100cc cb100ccn ## Run all CIFAR-100 benchmarks
	@echo "=== CIFAR-100 benchmark suite complete ==="

cbsa benchmark-all: cbs10 cbs100 ## Run complete CIFAR benchmark suite
	@echo "=== Complete CIFAR benchmark suite finished ==="

allqt all-quick-tests: tqa cbqa ## All quick tests

<<<<<<< HEAD
# AVIX TRAINING TARGETS "avix" - Direct Parameter Optimization

avix avix-cnn-16kdss: ## Train AVIX CNN on 16K dataset
	time python src/train.py experiment=avix_cnn_16kdss
=======
# UTILITY TARGETS

lc list-configs: ## List available model configurations
	@echo "Available model configs:"
	@find configs/model -name "*.yaml" | sed 's|configs/model/||' | sed 's|\.yaml||' | sort
	@echo "\nAvailable data configs:"  
	@find configs/data -name "*.yaml" | sed 's|configs/data/||' | sed 's|\.yaml||' | sort
	@echo "\nAvailable experiment configs:"
	@find configs/experiment -name "*.yaml" | sed 's|configs/experiment/||' | sed 's|\.yaml||' | sort
>>>>>>> main
